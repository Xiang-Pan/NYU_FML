{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee2c8f4-2b20-4604-8650-e1d4ae27dda1",
   "metadata": {},
   "source": [
    "# C1 Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9652fcd0",
   "metadata": {},
   "source": [
    "abalone dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0afc2472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libsvm.svmutil import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1c399afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x = svm_read_problem('./cached_datasets/abalone', return_scipy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "86405141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4177,)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4473d86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4177, 8)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "bf75b437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3133 + 1044 == 4177"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004aa0b7",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "022e8ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import dump_svmlight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a41b9e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 3133\n",
    "test_size = 1044\n",
    "train_x = x[:train_size]\n",
    "train_y = y[:train_size]\n",
    "test_x = x[-test_size:]\n",
    "test_y = y[-test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "510565ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_svmlight_file(X=train_x, y=train_y, f='./cached_datasets/abalone_train', zero_based=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9b1b7f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_svmlight_file(X=test_x, y=test_y, f='./cached_datasets/abalone_test', zero_based=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5a2da0",
   "metadata": {},
   "source": [
    "# C2 Scale the Data\n",
    "Use the libsvm scaling tool to scale the features of all the data. Use the first 3133 examples for training, the last 1044 for testing. The scaling parameters should be computed only on the training data and then applied to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "53294626",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/zsh: /home/xiangpan/.conda/envs/39/lib/libncursesw.so.6: no version information available (required by /bin/zsh)\n",
      "WARNING: minimal feature index is 0, but indices should start from 1\n",
      "WARNING: scaling factors with indices smaller than 1 are not stored to the file ./cached_datasets/train_range.\n",
      "15 0:-1 1:0.027027 2:0.0420168 3:-0.831858 4:-0.63733 5:-0.699395 6:-0.735352 7:-0.704036 \n",
      "7 0:-1 1:-0.256757 2:-0.294118 3:-0.840708 4:-0.841686 5:-0.867518 6:-0.873601 7:-0.863478 \n",
      "9 1:0.22973 2:0.226891 3:-0.761062 4:-0.52187 5:-0.656355 6:-0.628703 7:-0.584454 \n",
      "10 0:-1 1:-0.0135135 2:0.0420168 3:-0.778761 4:-0.635913 5:-0.7115 6:-0.701119 7:-0.694071 \n",
      "7 0:1 1:-0.310811 2:-0.327731 3:-0.858407 4:-0.856207 5:-0.880968 6:-0.897301 7:-0.893373 \n",
      "8 0:1 1:-0.0540541 2:-0.176471 3:-0.831858 4:-0.752435 5:-0.811701 6:-0.797235 7:-0.763827 \n",
      "20 1:0.22973 2:0.210084 3:-0.734513 4:-0.450682 5:-0.682582 6:-0.628703 7:-0.345291 \n",
      "16 1:0.27027 2:0.243697 3:-0.778761 4:-0.457411 5:-0.605918 6:-0.607637 7:-0.484803 \n",
      "9 0:-1 1:0.0810811 2:0.0588235 3:-0.778761 4:-0.640517 5:-0.710155 6:-0.705069 7:-0.674141 \n",
      "19 1:0.283784 2:0.294118 3:-0.734513 4:-0.367806 5:-0.578346 6:-0.603687 7:-0.365222 \n",
      "14 1:0.216216 2:0.092437 3:-0.752212 4:-0.571808 5:-0.740417 6:-0.612903 7:-0.584454 \n",
      "10 0:-1 1:-0.0405405 2:-0.00840336 3:-0.80531 4:-0.71383 5:-0.776059 6:-0.788018 7:-0.733931 \n",
      "11 0:-1 1:0.121622 2:0.092437 3:-0.761062 4:-0.61785 5:-0.70881 6:-0.751152 7:-0.624315 \n",
      "10 1:0.243243 2:0.176471 3:-0.743363 4:-0.516557 5:-0.634835 6:-0.55102 7:-0.59442 \n",
      "10 1:0.0675676 2:0.00840336 3:-0.823009 4:-0.664601 5:-0.776059 6:-0.789335 7:-0.63428 \n",
      "12 0:-1 1:0.148649 2:0.159664 3:-0.769912 4:-0.530724 5:-0.654338 6:-0.651086 7:-0.524664 \n",
      "7 0:1 1:-0.243243 2:-0.243697 3:-0.849558 4:-0.795644 5:-0.873571 6:-0.897301 7:-0.773792 \n",
      "10 1:-0.0135135 2:-0.0420168 3:-0.823009 4:-0.681955 5:-0.748487 6:-0.772219 7:-0.743896 \n",
      "7 0:-1 1:-0.216216 2:-0.193277 3:-0.858407 4:-0.820436 5:-0.870881 6:-0.888084 7:-0.803687 \n",
      "9 0:-1 1:0.0135135 2:-0.109244 3:-0.823009 4:-0.731539 5:-0.772024 6:-0.803818 7:-0.773792 \n",
      "11 0:-1 1:-0.243243 2:-0.243697 3:-0.831858 4:-0.827519 5:-0.872898 6:-0.838051 7:-0.853513 \n",
      "10 0:1 1:-0.175676 2:-0.260504 3:-0.823009 4:-0.841686 5:-0.893746 6:-0.872284 7:-0.833582 \n",
      "12 1:0.324324 2:0.294118 3:-0.725664 4:-0.335931 5:-0.426362 6:-0.437788 7:-0.464873 \n",
      "9 1:0.283784 2:0.210084 3:-0.761062 4:-0.460599 5:-0.573638 6:-0.448321 7:-0.604385 \n",
      "10 1:0.459459 2:0.428571 3:-0.707965 4:-0.178679 5:-0.311365 6:-0.20869 7:-0.395117 \n",
      "11 1:0.310811 2:0.294118 3:-0.752212 4:-0.343722 5:-0.486886 6:-0.506254 7:-0.405082 \n",
      "11 1:0.364865 2:0.327731 3:-0.672566 4:-0.296264 5:-0.470746 6:-0.285056 7:-0.434978 \n",
      "12 0:-1 1:0.391892 2:0.310924 3:-0.752212 4:-0.341951 5:-0.522529 6:-0.385122 7:-0.444943 \n",
      "15 0:-1 1:0.432432 2:0.411765 3:-0.681416 4:-0.338056 5:-0.471419 6:-0.424621 7:-0.415047 \n",
      "11 0:-1 1:0.351351 2:0.243697 3:-0.752212 4:-0.389764 5:-0.472764 6:-0.403555 7:-0.604385 \n",
      "10 0:-1 1:0.364865 2:0.394958 3:-0.707965 4:-0.294847 5:-0.472091 6:-0.364055 7:-0.345291 \n",
      "15 1:0.635135 2:0.697479 3:-0.707965 4:0.159554 5:-0.186954 6:-0.262673 7:-0.0861983 \n",
      "18 0:-1 1:0.594595 2:0.579832 3:-0.707965 4:-0.0536568 5:-0.259583 6:-0.0599078 7:-0.305431 \n",
      "19 1:0.635135 2:0.663866 3:-0.690265 4:0.27218 5:0.0948218 6:0.0322581 7:-0.0961634 \n",
      "13 1:0.702703 2:0.663866 3:-0.646018 4:0.209492 5:-0.149966 6:0.082291 7:-0.0264076 \n",
      "8 0:-1 1:0.0540541 2:0.00840336 3:-0.814159 4:-0.661767 5:-0.696032 6:-0.674786 7:-0.753861 \n",
      "16 1:0.256757 2:0.411765 3:-0.725664 4:-0.139366 5:-0.287828 6:-0.191573 7:-0.325361 \n",
      "8 1:0.0135135 2:0.00840336 3:-0.814159 4:-0.631309 5:-0.682582 6:-0.694536 7:-0.714001 \n",
      "11 1:0.351351 2:0.310924 3:-0.761062 4:-0.375952 5:-0.488904 6:-0.465438 7:-0.484803 \n",
      "9 0:-1 1:-0.243243 2:-0.210084 3:-0.840708 4:-0.769435 5:-0.821116 6:-0.774852 7:-0.823617 \n",
      "9 1:0.0135135 2:-0.0588235 3:-0.814159 4:-0.700372 5:-0.750504 6:-0.761685 7:-0.773792 \n",
      "14 1:0.283784 2:0.243697 3:-0.761062 4:-0.398265 5:-0.514459 6:-0.485188 7:-0.464873 \n",
      "5 0:1 1:-0.554054 2:-0.596639 3:-0.920354 4:-0.951833 5:-0.958978 6:-0.939434 7:-0.963129 \n",
      "5 0:1 1:-0.648649 2:-0.680672 3:-0.902655 4:-0.971666 5:-0.967048 6:-0.961817 7:-0.979073 \n",
      "4 0:1 1:-0.635135 2:-0.680672 3:-0.911504 4:-0.971666 5:-0.977808 6:-0.9684 7:-0.973094 \n",
      "7 0:1 1:-0.148649 2:-0.193277 3:-0.831858 4:-0.857624 5:-0.883658 6:-0.882818 7:-0.853513 \n",
      "9 0:-1 1:0.0675676 2:0.0588235 3:-0.787611 4:-0.590933 5:-0.607263 6:-0.403555 7:-0.723966 \n",
      "7 1:0.0405405 2:0.0756303 3:-0.787611 4:-0.675226 5:-0.762609 6:-0.711652 7:-0.704036 \n",
      "6 0:1 1:-0.324324 2:-0.361345 3:-0.876106 4:-0.887374 5:-0.899798 6:-0.934167 7:-0.913303 \n",
      "9 1:0.216216 2:0.243697 3:-0.716814 4:-0.409598 5:-0.524546 6:-0.439105 7:-0.514699 \n",
      "8 0:1 1:0.202703 2:0.193277 3:-0.787611 4:-0.579954 5:-0.680565 6:-0.709019 7:-0.624315 \n",
      "7 0:-1 1:-0.121622 2:-0.109244 3:-0.831858 4:-0.786789 5:-0.821789 6:-0.843318 7:-0.803687 \n",
      "10 0:-1 1:0.108108 2:0.0252101 3:-0.769912 4:-0.61785 5:-0.65232 6:-0.748519 7:-0.684106 \n",
      "10 1:0.0675676 2:0.0252101 3:-0.787611 4:-0.663184 5:-0.718225 6:-0.723502 7:-0.704036 \n",
      "7 0:-1 1:-0.108108 2:-0.142857 3:-0.823009 4:-0.728706 5:-0.768662 6:-0.760369 7:-0.783757 \n",
      "8 1:0.148649 2:0.159664 3:-0.752212 4:-0.532849 5:-0.656355 6:-0.539171 7:-0.564524 \n",
      "8 0:-1 2:-0.00840336 3:-0.787611 4:-0.687976 5:-0.743107 6:-0.749835 7:-0.733931 \n",
      "8 0:-1 1:0.0675676 2:0.109244 3:-0.761062 4:-0.58385 5:-0.629455 6:-0.685319 7:-0.664175 \n",
      "4 0:1 1:-0.540541 2:-0.546218 3:-0.893805 4:-0.940499 5:-0.944855 6:-0.96445 7:-0.953164 \n",
      "7 1:0.162162 2:0.159664 3:-0.778761 4:-0.588454 5:-0.670477 6:-0.658986 7:-0.65421 \n",
      "7 0:-1 1:0.0135135 2:-0.0252101 3:-0.814159 4:-0.709934 5:-0.759247 6:-0.705069 7:-0.733931 \n",
      "9 0:-1 1:0.162162 2:0.176471 3:-0.80531 4:-0.558704 5:-0.591123 6:-0.579987 7:-0.65421 \n",
      "10 1:0.22973 2:0.193277 3:-0.769912 4:-0.508057 5:-0.595158 6:-0.491771 7:-0.604385 \n",
      "7 0:-1 1:-0.0540541 2:-0.092437 3:-0.831858 4:-0.73331 5:-0.772024 6:-0.790652 7:-0.803687 \n",
      "8 0:-1 1:0.202703 2:0.159664 3:-0.787611 4:-0.590579 5:-0.686617 6:-0.655036 7:-0.63428 \n",
      "8 0:-1 1:0.0810811 2:0.00840336 3:-0.787611 4:-0.661413 5:-0.686617 6:-0.734036 7:-0.733931 \n",
      "12 1:0.324324 2:0.294118 3:-0.716814 4:-0.353285 5:-0.525219 6:-0.491771 7:-0.365222 \n",
      "13 1:0.405405 2:0.478992 3:-0.672566 4:-0.0911989 5:-0.441829 6:-0.411455 7:-0.0363727 \n",
      "10 1:0.0810811 2:0.12605 3:-0.787611 4:-0.625642 5:-0.71419 6:-0.697169 7:-0.664175 \n",
      "6 0:1 1:-0.364865 2:-0.394958 3:-0.876106 4:-0.894457 5:-0.916611 6:-0.894668 7:-0.913303 \n",
      "13 0:-1 1:0.297297 2:0.243697 3:-0.769912 4:-0.458474 5:-0.646268 6:-0.55892 7:-0.454908 \n",
      "8 1:-0.121622 2:-0.109244 3:-0.80531 4:-0.751372 5:-0.812374 6:-0.741935 7:-0.803687 \n",
      "20 1:0.405405 2:0.411765 3:-0.699115 4:-0.118116 5:-0.35575 6:-0.408822 7:-0.155954 \n",
      "11 0:-1 1:0.337838 2:0.428571 3:-0.690265 4:-0.162033 5:-0.36382 6:-0.314022 7:-0.24564 \n",
      "13 1:0.432432 2:0.327731 3:-0.654867 4:-0.223659 5:-0.354405 6:-0.238973 7:-0.375187 \n",
      "15 1:0.418919 2:0.411765 3:-0.734513 4:-0.287763 5:-0.406187 6:-0.419355 7:-0.444943 \n",
      "9 0:-1 1:0.405405 2:0.411765 3:-0.752212 4:-0.332743 5:-0.513786 6:-0.503621 7:-0.375187 \n",
      "10 1:0.418919 2:0.394958 3:-0.734513 4:-0.348327 5:-0.513114 6:-0.490454 7:-0.395117 \n",
      "11 1:0.297297 2:0.243697 3:-0.752212 4:-0.443244 5:-0.622058 6:-0.581303 7:-0.434978 \n",
      "14 1:0.459459 2:0.411765 3:-0.699115 4:-0.220471 5:-0.369872 6:-0.381172 7:-0.315396 \n",
      "9 1:0.351351 2:0.310924 3:-0.752212 4:-0.334868 5:-0.484196 6:-0.337722 7:-0.434978 \n",
      "12 0:-1 1:0.472973 2:0.529412 3:-0.690265 4:0.142554 5:-0.314728 6:-0.495721 7:0.342302 \n",
      "16 1:0.202703 2:0.243697 3:-0.707965 4:-0.301222 5:-0.468729 6:-0.408822 7:-0.365222 \n",
      "21 0:-1 1:0.405405 2:0.411765 3:-0.716814 4:-0.0681778 5:-0.452589 6:-0.385122 7:0.152965 \n",
      "14 0:-1 1:0.364865 2:0.327731 3:-0.752212 4:-0.283868 5:-0.490249 6:-0.432521 7:-0.285501 \n",
      "12 1:0.337838 2:0.378151 3:-0.681416 4:-0.0841155 5:-0.545393 6:-0.415405 7:-0.126059 \n",
      "13 0:-1 1:0.486486 2:0.378151 3:-0.752212 4:-0.15495 5:-0.352387 6:-0.461488 7:-0.20578 \n",
      "10 0:-1 1:0.310811 2:0.294118 3:-0.716814 4:-0.389056 5:-0.556826 6:-0.454905 7:-0.484803 \n",
      "9 1:0.0405405 2:0.00840336 3:-0.769912 4:-0.635205 5:-0.704775 6:-0.701119 7:-0.674141 \n",
      "12 1:0.351351 2:0.327731 3:-0.716814 4:-0.309014 5:-0.579691 6:-0.393022 7:-0.345291 \n",
      "15 0:-1 1:0.324324 2:0.243697 3:-0.761062 4:-0.426598 5:-0.542703 6:-0.560237 7:-0.494768 \n",
      "12 0:-1 1:0.297297 2:0.294118 3:-0.734513 4:-0.466619 5:-0.588433 6:-0.599737 7:-0.484803 \n",
      "13 0:-1 1:0.405405 2:0.378151 3:-0.690265 4:-0.211617 5:-0.461332 6:-0.332456 7:-0.22571 \n",
      "10 1:0.486486 2:0.478992 3:-0.707965 4:-0.107491 5:-0.319435 6:-0.163924 7:-0.22571 \n",
      "15 0:-1 1:0.675676 2:0.697479 3:-0.663717 4:0.0568443 5:-0.210491 6:-0.0994075 7:-0.0363727 \n",
      "14 0:-1 1:0.594595 2:0.613445 3:-0.654867 4:0.136179 5:-0.227303 6:0.0204082 7:-0.0463378 \n",
      "9 0:-1 1:0.243243 2:0.277311 3:-0.734513 4:-0.48787 5:-0.639543 6:-0.636603 7:-0.504733 \n",
      "8 0:-1 1:0.0675676 2:0.0756303 3:-0.769912 4:-0.630954 5:-0.713517 6:-0.65372 7:-0.714001 \n",
      "7 0:-1 1:0.0675676 2:0.0588235 3:-0.769912 4:-0.631309 5:-0.731002 6:-0.651086 7:-0.674141 \n",
      "10 1:0.0810811 2:0.0756303 3:-0.778761 4:-0.591642 5:-0.62811 6:-0.777485 7:-0.694071 \n",
      "7 0:1 1:-0.22973 2:-0.294118 3:-0.831858 4:-0.837436 5:-0.860121 6:-0.880184 7:-0.853513 \n",
      "15 0:-1 1:0.283784 2:0.277311 3:-0.743363 4:-0.404285 5:-0.560188 6:-0.497038 7:-0.494768 \n",
      "15 0:-1 1:0.22973 2:0.277311 3:-0.716814 4:-0.375952 5:-0.576328 6:-0.569454 7:-0.335326 \n",
      "10 0:-1 1:0.22973 2:0.210084 3:-0.752212 4:-0.488578 5:-0.583726 6:-0.560237 7:-0.59442 \n",
      "12 0:-1 1:0.432432 2:0.394958 3:-0.716814 4:-0.170179 5:-0.332213 6:-0.368005 7:-0.315396 \n",
      "12 1:0.202703 2:0.193277 3:-0.725664 4:-0.486453 5:-0.609953 6:-0.518104 7:-0.534629 \n",
      "11 1:0.27027 2:0.260504 3:-0.707965 4:-0.433327 5:-0.60659 6:-0.519421 7:-0.444943 \n",
      "10 1:0.148649 2:0.159664 3:-0.778761 4:-0.528599 5:-0.650303 6:-0.655036 7:-0.564524 \n",
      "9 1:0.175676 2:0.12605 3:-0.761062 4:-0.552683 5:-0.690652 6:-0.529954 7:-0.604385 \n",
      "9 1:-0.027027 2:0.142857 3:-0.814159 4:-0.743935 5:-0.818426 6:-0.743252 7:-0.743896 \n",
      "9 0:-1 1:0.135135 2:0.142857 3:-0.778761 4:-0.61785 5:-0.68191 6:-0.647136 7:-0.694071 \n",
      "9 0:-1 1:0.0540541 2:0.0252101 3:-0.814159 4:-0.696122 5:-0.770007 6:-0.719552 7:-0.65421 \n",
      "9 0:1 1:-0.027027 2:-0.109244 3:-0.858407 4:-0.765893 5:-0.801614 6:-0.834101 7:-0.793722 \n",
      "9 0:-1 1:-0.0540541 2:-0.00840336 3:-0.814159 4:-0.723039 5:-0.826496 6:-0.835418 7:-0.674141 \n",
      "11 1:0.27027 2:0.193277 3:-0.778761 4:-0.510182 5:-0.60121 6:-0.616853 7:-0.584454 \n",
      "11 1:0.22973 2:0.210084 3:-0.79646 4:-0.582433 5:-0.687962 6:-0.583937 7:-0.644245 \n",
      "11 1:0.121622 2:0.0756303 3:-0.761062 4:-0.567558 5:-0.6577 6:-0.732719 7:-0.564524 \n",
      "10 0:-1 1:-0.0135135 2:-0.0420168 3:-0.814159 4:-0.716664 5:-0.825824 6:-0.749835 7:-0.674141 \n",
      "9 1:0.310811 2:0.260504 3:-0.734513 4:-0.376306 5:-0.535306 6:-0.548387 7:-0.385152 \n",
      "8 0:-1 1:-0.108108 2:-0.159664 3:-0.849558 4:-0.816894 5:-0.847344 6:-0.844635 7:-0.833582 \n",
      "9 1:0.0675676 2:0.0420168 3:-0.814159 4:-0.703559 5:-0.782112 6:-0.728769 7:-0.723966 \n",
      "7 0:1 1:-0.162162 2:-0.193277 3:-0.849558 4:-0.821852 5:-0.862811 6:-0.849901 7:-0.833582 \n",
      "14 1:0.189189 2:0.243697 3:-0.752212 4:-0.458828 5:-0.592468 6:-0.54707 7:-0.494768 \n",
      "6 0:-1 1:-0.202703 2:-0.294118 3:-0.867257 4:-0.849832 5:-0.880296 6:-0.867018 7:-0.863478 \n",
      "6 0:1 1:-0.22973 2:-0.243697 3:-0.858407 4:-0.877103 5:-0.892401 6:-0.868334 7:-0.863478 \n",
      "5 0:1 1:-0.472973 2:-0.529412 3:-0.893805 4:-0.949708 5:-0.963013 6:-0.939434 7:-0.943199 \n",
      "6 0:1 1:-0.189189 2:-0.260504 3:-0.840708 4:-0.832832 5:-0.856759 6:-0.857801 7:-0.863478 \n",
      "8 0:1 1:-0.162162 2:-0.210084 3:-0.849558 4:-0.823977 5:-0.850706 6:-0.840685 7:-0.843548 \n",
      "19 0:-1 1:0.689189 2:0.613445 3:-0.716814 4:0.220825 5:-0.154001 6:-0.307439 7:0.0732436 \n",
      "18 0:-1 1:0.716216 2:0.630252 3:-0.707965 4:0.386223 5:0.0295898 6:-0.314022 7:0.55157 \n",
      "17 0:-1 1:0.405405 2:0.428571 3:-0.707965 4:-0.107491 5:-0.351042 6:-0.25609 7:-0.18585 \n",
      "9 1:-0.0135135 2:-0.00840336 3:-0.778761 4:-0.715601 5:-0.765972 6:-0.835418 7:-0.745889 \n",
      "7 1:-0.324324 2:-0.310924 3:-0.840708 4:-0.865769 5:-0.887021 6:-0.906517 7:-0.879422 \n",
      "7 0:1 1:-0.256757 2:-0.310924 3:-0.831858 4:-0.851957 5:-0.885676 6:-0.853851 7:-0.867464 \n",
      "7 0:1 1:-0.486486 2:-0.512605 3:-0.884956 4:-0.932353 5:-0.947545 6:-0.947334 7:-0.947185 \n",
      "8 1:-0.0540541 2:-0.0756303 3:-0.79646 4:-0.71383 5:-0.781439 6:-0.788018 7:-0.732935 \n",
      "7 1:-0.378378 2:-0.411765 3:-0.858407 4:-0.890916 5:-0.910558 6:-0.910467 7:-0.907324 \n",
      "9 0:-1 1:-0.27027 2:-0.327731 3:-0.840708 4:-0.859394 5:-0.874916 6:-0.923634 7:-0.877429 \n",
      "9 1:-0.108108 2:-0.092437 3:-0.80531 4:-0.749602 5:-0.798252 6:-0.835418 7:-0.769806 \n",
      "9 0:-1 1:-0.189189 2:-0.226891 3:-0.831858 4:-0.822206 5:-0.872226 6:-0.849901 7:-0.818635 \n",
      "10 1:0.324324 2:0.310924 3:-0.725664 4:-0.416327 5:-0.542703 6:-0.460171 7:-0.509716 \n",
      "10 1:0.283784 2:0.327731 3:-0.743363 4:-0.476536 5:-0.604573 6:-0.623436 7:-0.471849 \n",
      "16 0:-1 1:0.554054 2:0.563025 3:-0.663717 4:-0.0490526 5:-0.303295 6:-0.195523 7:-0.113104 \n",
      "11 0:-1 1:0.310811 2:0.344538 3:-0.725664 4:-0.436869 5:-0.544048 6:-0.500987 7:-0.519681 \n",
      "10 0:-1 1:0.0810811 2:0.0756303 3:-0.769912 4:-0.63485 5:-0.72226 6:-0.694536 7:-0.664175 \n",
      "10 1:0.121622 2:0.092437 3:-0.778761 4:-0.612538 5:-0.671822 6:-0.718236 7:-0.656203 \n",
      "10 0:-1 1:0.0405405 2:-0.00840336 3:-0.787611 4:-0.636621 5:-0.700067 6:-0.716919 7:-0.691081 \n",
      "9 0:1 1:-0.445946 2:-0.495798 3:-0.858407 4:-0.911457 5:-0.931406 6:-0.898618 7:-0.919283 \n",
      "5 0:1 1:-0.72973 2:-0.747899 3:-0.902655 4:-0.979104 5:-0.987223 6:-0.9842 7:-0.978077 \n",
      "4 0:1 1:-0.743243 2:-0.747899 3:-0.831858 4:-0.980166 5:-0.98386 6:-0.98025 7:-0.983059 \n",
      "15 0:-1 1:0.391892 2:0.411765 3:-0.743363 4:-0.255534 5:-0.407532 6:-0.311389 7:-0.355257 \n",
      "9 1:0.432432 2:0.495798 3:-0.672566 4:-0.209138 5:-0.370545 6:-0.320606 7:-0.335326 \n",
      "10 1:0.513514 2:0.546218 3:-0.663717 4:-0.0299274 5:-0.320108 6:-0.198157 7:-0.106129 \n",
      "10 1:0.432432 2:0.445378 3:-0.716814 4:-0.253055 5:-0.503699 6:-0.381172 7:-0.295466 \n",
      "12 1:0.324324 2:0.327731 3:-0.761062 4:-0.301222 5:-0.480834 6:-0.607637 7:-0.385152 \n",
      "10 0:-1 1:0.189189 2:0.176471 3:-0.769912 4:-0.489995 5:-0.570948 6:-0.656353 7:-0.584454 \n",
      "13 1:0.351351 2:0.361345 3:-0.663717 4:-0.297326 5:-0.474109 6:-0.362739 7:-0.325361 \n",
      "16 0:-1 1:0.540541 2:0.445378 3:-0.619469 4:0.0710112 5:-0.26698 6:-0.312706 7:0.262581 \n",
      "13 1:0.364865 2:0.344538 3:-0.699115 4:-0.358597 5:-0.498319 6:-0.439105 7:-0.434978 \n",
      "13 1:0.351351 2:0.361345 3:-0.707965 4:-0.205242 5:-0.599866 6:-0.531271 7:-0.126059 \n",
      "13 0:-1 1:0.432432 2:0.378151 3:-0.707965 4:-0.253409 5:-0.434432 6:-0.349572 7:-0.325361 \n",
      "13 1:0.432432 2:0.445378 3:-0.716814 4:-0.135824 5:-0.2885 6:-0.323239 7:-0.444943 \n",
      "12 0:-1 1:0.445946 2:0.445378 3:-0.690265 4:-0.119887 5:-0.26967 6:-0.219223 7:-0.315396 \n",
      "18 1:0.756757 2:0.697479 3:-0.628319 4:0.515141 5:-0.127102 6:0.0467413 7:1 \n",
      "16 1:0.554054 2:0.647059 3:-0.59292 4:0.239596 5:-0.247478 6:-0.238973 7:0.621325 \n",
      "14 0:-1 1:0.756757 2:0.731092 3:-0.663717 4:0.804852 5:0.438467 6:0.270573 7:0.441953 \n",
      "20 1:0.756757 2:0.747899 3:-0.690265 4:0.503099 5:0.0275723 6:0.187623 7:0.691081 \n",
      "20 1:0.635135 2:0.731092 3:-0.637168 4:0.303347 5:-0.160726 6:0.0730744 7:0.292476 \n",
      "14 0:-1 1:0.702703 2:0.697479 3:-0.610619 4:0.401806 5:0.0981843 6:-0.18894 7:0.511709 \n",
      "12 1:0.635135 2:0.546218 3:-0.690265 4:0.145033 5:-0.312038 6:0.0757077 7:0.232686 \n",
      "14 0:-1 1:0.675676 2:0.663866 3:-0.619469 4:0.384452 5:-0.0430397 6:0.423305 7:0.172895 \n",
      "7 1:0.22973 2:0.142857 3:-0.743363 4:-0.452453 5:-0.587088 6:-0.556287 7:-0.494768 \n",
      "8 0:-1 1:0.216216 2:0.277311 3:-0.725664 4:-0.247034 5:-0.34768 6:-0.387755 7:-0.434978 \n",
      "8 1:0.202703 2:0.176471 3:-0.79646 4:-0.451744 5:-0.570948 6:-0.515471 7:-0.564524 \n",
      "5 0:1 1:-0.567568 2:-0.647059 3:-0.929204 4:-0.967416 5:-0.976463 6:-0.953917 7:-0.973094 \n",
      "7 0:1 1:-0.22973 2:-0.310924 3:-0.840708 4:-0.874978 5:-0.914593 6:-0.903884 7:-0.853513 \n",
      "5 0:1 1:-0.351351 2:-0.478992 3:-0.893805 4:-0.912874 5:-0.920646 6:-0.902567 7:-0.933234 \n",
      "8 0:1 1:-0.351351 2:-0.361345 3:-0.849558 4:-0.89977 5:-0.930061 6:-0.876234 7:-0.903338 \n",
      "4 0:1 1:-0.594595 2:-0.647059 3:-0.920354 4:-0.968479 5:-0.96772 6:-0.961817 7:-0.973094 \n",
      "11 0:-1 1:0.364865 2:0.411765 3:-0.734513 4:-0.314326 5:-0.483524 6:-0.431205 7:-0.305431 \n",
      "14 0:-1 1:0.337838 2:0.428571 3:-0.681416 4:-0.335931 5:-0.464694 6:-0.474654 7:-0.415047 \n",
      "21 0:-1 1:0.527027 2:0.529412 3:-0.690265 4:-0.0324066 5:-0.308675 6:-0.300856 7:0.133034 \n",
      "10 1:0.310811 2:0.327731 3:-0.716814 4:-0.27643 5:-0.424344 6:-0.295589 7:-0.405082 \n",
      "10 1:0.472973 2:0.411765 3:-0.690265 4:-0.281388 5:-0.415602 6:-0.437788 7:-0.355257 \n",
      "12 1:0.540541 2:0.529412 3:-0.646018 4:0.108907 5:-0.166106 6:-0.0348914 7:-0.0861983 \n",
      "13 0:-1 1:0.472973 2:0.462185 3:-0.663717 4:-0.138658 5:-0.267653 6:-0.22054 7:-0.295466 \n",
      "12 1:0.5 2:0.428571 3:-0.734513 4:-0.255888 5:-0.474109 6:-0.116524 7:-0.434978 \n",
      "10 1:0.5 2:0.495798 3:-0.672566 4:-0.0217815 5:-0.27505 6:-0.128374 7:-0.24564 \n",
      "11 1:0.5 2:0.428571 3:-0.716814 4:-0.152116 5:-0.293208 6:-0.119157 7:-0.375187 \n",
      "9 1:0.378378 2:0.361345 3:-0.699115 4:-0.340889 5:-0.510424 6:-0.287689 7:-0.425012 \n",
      "13 0:-1 1:0.459459 2:0.428571 3:-0.681416 4:-0.180096 5:-0.349697 6:-0.431205 7:-0.355257 \n",
      "12 0:-1 1:0.445946 2:0.445378 3:-0.699115 4:-0.277138 5:-0.437794 6:-0.368005 7:-0.285501 \n",
      "14 0:-1 1:0.364865 2:0.327731 3:-0.734513 4:-0.344785 5:-0.630128 6:-0.523371 7:-0.285501 \n",
      "8 0:1 1:-0.243243 2:-0.260504 3:-0.849558 4:-0.845582 5:-0.877606 6:-0.843318 7:-0.704036 \n",
      "10 1:0.175676 2:0.159664 3:-0.752212 4:-0.424473 5:-0.383995 6:-0.483871 7:-0.61435 \n",
      "12 0:-1 1:0.148649 2:0.176471 3:-0.725664 4:-0.454578 5:-0.535978 6:-0.597103 7:-0.514699 \n",
      "11 1:0.162162 2:0.193277 3:-0.734513 4:-0.545245 5:-0.618023 6:-0.619487 7:-0.584454 \n",
      "16 0:-1 1:0.527027 2:0.495798 3:-0.672566 4:-0.0780946 5:-0.403497 6:-0.307439 7:-0.0762332 \n",
      "15 0:-1 1:0.310811 2:0.327731 3:-0.716814 4:-0.348327 5:-0.420309 6:-0.532587 7:-0.484803 \n",
      "10 0:-1 1:0.378378 2:0.361345 3:-0.672566 4:-0.348327 5:-0.512441 6:-0.440421 7:-0.434978 \n",
      "9 1:0.0135135 2:-0.0252101 3:-0.787611 4:-0.706393 5:-0.778749 6:-0.751152 7:-0.733931 \n",
      "13 0:-1 1:0.148649 2:0.159664 3:-0.707965 4:-0.417036 5:-0.659718 6:-0.461488 7:-0.434978 \n",
      "12 1:0.148649 2:0.159664 3:-0.743363 4:-0.555162 5:-0.686617 6:-0.615537 7:-0.544594 \n",
      "13 1:0.22973 2:0.277311 3:-0.699115 4:-0.423765 5:-0.599866 6:-0.593153 7:-0.454908 \n",
      "8 0:-1 1:-0.0675676 2:-0.0588235 3:-0.79646 4:-0.740039 5:-0.771352 6:-0.814352 7:-0.763827 \n",
      "9 1:-0.0135135 2:-0.0420168 3:-0.752212 4:-0.659996 5:-0.751177 6:-0.715602 7:-0.684106 \n",
      "9 0:1 1:-0.121622 2:-0.176471 3:-0.80531 4:-0.778289 5:-0.854741 6:-0.824885 7:-0.763827 \n",
      "8 0:1 1:-0.027027 2:-0.0420168 3:-0.80531 4:-0.732601 5:-0.800269 6:-0.777485 7:-0.763827 \n",
      "13 1:0.216216 2:0.210084 3:-0.699115 4:-0.411723 5:-0.6308 6:-0.557604 7:-0.385152 \n",
      "7 0:1 1:-0.202703 2:-0.243697 3:-0.831858 4:-0.813352 5:-0.837256 6:-0.864384 7:-0.843548 \n",
      "10 1:0.121622 2:0.0420168 3:-0.743363 4:-0.551974 5:-0.73302 6:-0.573404 7:-0.564524 \n",
      "7 0:-1 1:-0.297297 2:-0.344538 3:-0.840708 4:-0.873207 5:-0.899798 6:-0.892034 7:-0.883408 \n",
      "12 1:-0.0810811 2:-0.092437 3:-0.814159 4:-0.732247 5:-0.786819 6:-0.794602 7:-0.763827 \n",
      "9 0:-1 1:0.148649 2:0.176471 3:-0.752212 4:-0.565433 5:-0.677202 6:-0.644503 7:-0.59442 \n",
      "14 1:0.108108 2:0.142857 3:-0.716814 4:-0.533912 5:-0.66846 6:-0.664253 7:-0.534629 \n",
      "10 0:-1 1:0.283784 2:0.176471 3:-0.752212 4:-0.432973 5:-0.673167 6:-0.57077 7:-0.494768 \n",
      "8 0:-1 1:0.0135135 2:-0.00840336 3:-0.769912 4:-0.67558 5:-0.767317 6:-0.709019 7:-0.733931 \n",
      "7 0:1 1:-0.108108 2:-0.176471 3:-0.787611 4:-0.771914 5:-0.831204 6:-0.816985 7:-0.783757 \n",
      "10 0:-1 1:0.0675676 2:0.0252101 3:-0.761062 4:-0.646538 5:-0.777404 6:-0.698486 7:-0.674141 \n",
      "8 1:-0.0810811 2:-0.159664 3:-0.769912 4:-0.774748 5:-0.825824 6:-0.802502 7:-0.793722 \n",
      "9 2:-0.092437 3:-0.778761 4:-0.679122 5:-0.761264 6:-0.705069 7:-0.723966 \n",
      "11 1:0.0675676 2:-0.00840336 3:-0.743363 4:-0.63485 5:-0.749832 6:-0.676103 7:-0.644245 \n",
      "9 1:0.121622 2:0.0756303 3:-0.734513 4:-0.593767 5:-0.705447 6:-0.62212 7:-0.624315 \n",
      "11 2:0.00840336 3:-0.734513 4:-0.657871 5:-0.757902 6:-0.672153 7:-0.694071 \n",
      "10 0:1 1:-0.0540541 2:0.092437 3:-0.814159 4:-0.770143 5:-0.828514 6:-0.794602 7:-0.803687 \n",
      "9 1:0.148649 2:0.0588235 3:-0.761062 4:-0.682663 5:-0.770679 6:-0.723502 7:-0.694071 \n",
      "7 1:-0.148649 2:-0.210084 3:-0.778761 4:-0.785019 5:-0.838601 6:-0.785385 7:-0.823617 \n",
      "7 0:1 1:-0.216216 2:-0.277311 3:-0.849558 4:-0.856207 5:-0.896436 6:-0.873601 7:-0.863478 \n",
      "11 1:0.364865 2:0.378151 3:-0.707965 4:-0.221179 5:-0.457969 6:-0.449638 7:-0.305431 \n",
      "15 1:0.22973 2:0.210084 3:-0.716814 4:-0.446786 5:-0.60659 6:-0.585253 7:-0.514699 \n",
      "13 0:-1 1:0.297297 2:0.310924 3:-0.761062 4:-0.409244 5:-0.549428 6:-0.573404 7:-0.454908 \n",
      "14 0:-1 1:0.324324 2:0.294118 3:-0.690265 4:-0.362139 5:-0.584398 6:-0.493088 7:-0.355257 \n",
      "22 0:-1 1:0.486486 2:0.512605 3:-0.619469 4:0.0224898 5:-0.33423 6:-0.245556 7:-0.136024 \n",
      "7 0:1 1:-0.459459 2:-0.462185 3:-0.867257 4:-0.919603 5:-0.936113 6:-0.924951 7:-0.933234 \n",
      "12 0:1 1:-0.0135135 2:-0.00840336 3:-0.761062 4:-0.693288 5:-0.757229 6:-0.782752 7:-0.753861 \n",
      "9 0:1 1:-0.405405 2:-0.428571 3:-0.858407 4:-0.913582 5:-0.936113 6:-0.917051 7:-0.923269 \n",
      "1 0:1 1:-1 2:-1 3:-0.982301 4:-1 5:-1 6:-1 7:-1 \n",
      "3 0:1 1:-0.851351 2:-0.848739 3:-0.946903 4:-0.992208 5:-0.995293 6:-0.993417 7:-0.995017 \n",
      "3 0:1 1:-0.905405 2:-0.882353 3:-0.946903 4:-0.99575 5:-0.997983 6:-0.99605 7:-0.99701 \n",
      "5 0:1 1:-0.77027 2:-0.781513 3:-0.938053 4:-0.986542 5:-0.991258 6:-0.989467 7:-0.993024 \n",
      "17 0:-1 1:0.324324 2:0.243697 3:-0.716814 4:-0.333806 5:-0.531271 6:-0.425938 7:-0.454908 \n",
      "5 0:1 1:-0.472973 2:-0.512605 3:-0.876106 4:-0.930583 5:-0.955615 6:-0.936801 7:-0.933234 \n",
      "5 0:1 1:-0.581081 2:-0.596639 3:-0.884956 4:-0.955729 5:-0.966375 6:-0.973667 7:-0.963129 \n",
      "8 0:1 1:-0.391892 2:-0.411765 3:-0.858407 4:-0.911103 5:-0.942838 6:-0.931534 7:-0.923269 \n",
      "8 0:1 1:-0.310811 2:-0.327731 3:-0.849558 4:-0.884186 5:-0.916611 6:-0.898618 7:-0.883408 \n",
      "10 0:1 1:-0.256757 2:-0.310924 3:-0.849558 4:-0.878165 5:-0.906523 6:-0.910467 7:-0.883408 \n",
      "13 0:1 1:-0.337838 2:-0.361345 3:-0.858407 4:-0.889145 5:-0.915938 6:-0.915734 7:-0.903338 \n",
      "9 0:1 1:-0.22973 2:-0.260504 3:-0.849558 4:-0.861519 5:-0.901143 6:-0.892034 7:-0.863478 \n",
      "7 0:1 1:-0.378378 2:-0.361345 3:-0.867257 4:-0.890916 5:-0.910558 6:-0.901251 7:-0.913303 \n",
      "7 0:1 1:-0.27027 2:-0.277311 3:-0.80531 4:-0.850186 5:-0.891056 6:-0.857801 7:-0.863478 \n",
      "7 0:1 1:-0.310811 2:-0.344538 3:-0.814159 4:-0.879936 5:-0.913248 6:-0.909151 7:-0.883408 \n",
      "13 0:-1 1:0.391892 2:0.394958 3:-0.681416 4:-0.205596 5:-0.435777 6:-0.262673 7:-0.285501 \n",
      "12 1:0.405405 2:0.344538 3:-0.725664 4:-0.250221 5:-0.310693 6:-0.431205 7:-0.405082 \n",
      "15 1:0.351351 2:0.361345 3:-0.672566 4:-0.226492 5:-0.398117 6:-0.429888 7:-0.315396 \n",
      "15 0:-1 1:0.418919 2:0.478992 3:-0.707965 4:-0.122012 5:-0.349025 6:-0.270573 7:-0.325361 \n",
      "15 0:-1 1:0.310811 2:0.327731 3:-0.690265 4:-0.285284 5:-0.485541 6:-0.457538 7:-0.265571 \n",
      "19 0:-1 1:0.310811 2:0.327731 3:-0.672566 4:-0.243492 5:-0.489576 6:-0.540487 7:-0.18585 \n",
      "10 0:-1 1:0.27027 2:0.361345 3:-0.716814 4:-0.365681 5:-0.542703 6:-0.565504 7:-0.315396 \n",
      "15 1:0.513514 2:0.512605 3:-0.699115 4:0.000885426 5:-0.187626 6:-0.219223 7:-0.275536 \n",
      "13 1:0.391892 2:0.411765 3:-0.716814 4:-0.221179 5:-0.359112 6:-0.328506 7:-0.415047 \n",
      "11 1:0.256757 2:0.411765 3:-0.725664 4:-0.344077 5:-0.471419 6:-0.490454 7:-0.484803 \n",
      "12 1:0.337838 2:0.294118 3:-0.778761 4:-0.388702 5:-0.507061 6:-0.54707 7:-0.464873 \n",
      "11 0:-1 1:0.22973 2:0.226891 3:-0.707965 4:-0.367806 5:-0.572293 6:-0.371955 7:-0.514699 \n",
      "4 0:1 1:-0.540541 2:-0.529412 3:-0.893805 4:-0.934124 5:-0.941493 6:-0.936801 7:-0.951171 \n",
      "6 0:-1 1:-0.472973 2:-0.512605 3:-0.858407 4:-0.916062 5:-0.938803 6:-0.927584 7:-0.923269 \n",
      "11 1:0.0405405 2:0.092437 3:-0.769912 4:-0.548787 5:-0.597848 6:-0.599737 7:-0.684106 \n",
      "14 0:-1 1:0.202703 2:0.327731 3:-0.734513 4:-0.367452 5:-0.515131 6:-0.511521 7:-0.534629 \n",
      "8 0:-1 1:-0.256757 2:-0.260504 3:-0.80531 4:-0.794227 5:-0.836584 6:-0.834101 7:-0.822621 \n",
      "9 0:-1 1:0.0675676 2:0.12605 3:-0.734513 4:-0.551266 5:-0.707465 6:-0.768269 7:-0.494768 \n",
      "13 1:0.0135135 2:0.0252101 3:-0.778761 4:-0.6476 5:-0.72764 6:-0.737986 7:-0.664175 \n",
      "22 1:0.527027 2:0.579832 3:-0.619469 4:0.258721 5:-0.391392 6:-0.249506 7:0.0931739 \n",
      "16 0:-1 1:0.391892 2:0.495798 3:-0.646018 4:-0.160616 5:-0.447209 6:-0.289006 7:-0.265571 \n",
      "14 0:-1 1:0.472973 2:0.445378 3:-0.637168 4:-0.137949 5:-0.480161 6:-0.341672 7:-0.235675 \n",
      "15 0:-1 1:0.5 2:0.512605 3:-0.60177 4:0.0788029 5:-0.248151 6:-0.123107 7:-0.106129 \n",
      "13 0:-1 1:0.5 2:0.546218 3:-0.725664 4:-0.109616 5:-0.449227 6:-0.482554 7:-0.18585 \n",
      "22 0:-1 1:0.567568 2:0.630252 3:-0.619469 4:0.304764 5:-0.00268998 6:-0.140224 7:0.16293 \n",
      "12 1:0.581081 2:0.596639 3:-0.672566 4:-0.0462192 5:-0.338265 6:-0.356155 7:-0.0264076 \n",
      "18 0:-1 1:0.445946 2:0.495798 3:-0.575221 4:0.161679 5:-0.28581 6:-0.120474 7:0.372197 \n",
      "20 0:-1 1:0.513514 2:0.579832 3:-0.637168 4:0.0497609 5:-0.261601 6:-0.18104 7:-0.145989 \n",
      "11 1:0.189189 2:0.243697 3:-0.761062 4:-0.497078 5:-0.642905 6:-0.57867 7:-0.504733 \n",
      "15 1:0.243243 2:0.210084 3:-0.672566 4:-0.405348 5:-0.579018 6:-0.583937 7:-0.405082 \n",
      "7 0:1 1:-0.22973 2:-0.226891 3:-0.814159 4:-0.830352 5:-0.878278 6:-0.851218 7:-0.853513 \n",
      "9 1:0.027027 2:0.00840336 3:-0.787611 4:-0.683018 5:-0.763282 6:-0.727452 7:-0.704036 \n",
      "14 0:-1 1:0.108108 2:0.142857 3:-0.752212 4:-0.555516 5:-0.694015 6:-0.666886 7:-0.554559 \n",
      "14 0:-1 1:0.189189 2:0.092437 3:-0.690265 4:-0.323889 5:-0.564223 6:-0.585253 7:-0.385152 \n",
      "10 1:0.243243 2:0.210084 3:-0.699115 4:-0.378785 5:-0.604573 6:-0.483871 7:-0.434978 \n",
      "10 0:-1 1:0.22973 2:0.277311 3:-0.725664 4:-0.506287 5:-0.613988 6:-0.581303 7:-0.59442 \n",
      "17 1:0.135135 2:0.159664 3:-0.725664 4:-0.544891 5:-0.675857 6:-0.652403 7:-0.59442 \n",
      "9 0:-1 1:-0.0135135 2:0.00840336 3:-0.778761 4:-0.663184 5:-0.823806 6:-0.786702 7:-0.624315 \n",
      "10 1:0.243243 2:0.277311 3:-0.716814 4:-0.427307 5:-0.577001 6:-0.528637 7:-0.524664 \n",
      "17 0:-1 1:0.256757 2:0.277311 3:-0.681416 4:-0.295909 5:-0.485541 6:-0.406188 7:-0.355257 \n",
      "12 1:0.324324 2:0.512605 3:-0.628319 4:-0.0972198 5:-0.327505 6:-0.266623 7:-0.295466 \n",
      "15 0:-1 1:0.445946 2:0.411765 3:-0.707965 4:-0.210908 5:-0.425689 6:-0.420671 7:-0.375187 \n",
      "19 1:0.324324 2:0.344538 3:-0.690265 4:-0.283868 5:-0.541358 6:-0.456221 7:-0.305431 \n",
      "26 0:-1 1:0.418919 2:0.478992 3:-0.654867 4:-0.252346 5:-0.484869 6:-0.500987 7:-0.255605 \n",
      "6 0:1 1:-0.405405 2:-0.462185 3:-0.849558 4:-0.910749 5:-0.93544 6:-0.911784 7:-0.923269 \n",
      "6 0:1 1:-0.459459 2:-0.495798 3:-0.867257 4:-0.923145 5:-0.94082 6:-0.926267 7:-0.933234 \n",
      "4 0:1 1:-0.445946 2:-0.478992 3:-0.849558 4:-0.925978 5:-0.94889 6:-0.923634 7:-0.943199 \n",
      "11 0:-1 1:0.121622 2:0.142857 3:-0.752212 4:-0.612538 5:-0.70343 6:-0.665569 7:-0.704036 \n",
      "9 0:-1 1:-0.202703 2:-0.243697 3:-0.814159 4:-0.835665 5:-0.879623 6:-0.847268 7:-0.853513 \n",
      "9 1:-0.108108 2:-0.159664 3:-0.831858 4:-0.75456 5:-0.805649 6:-0.765635 7:-0.803687 \n",
      "13 1:0.256757 2:0.277311 3:-0.690265 4:-0.369577 5:-0.568258 6:-0.54312 7:-0.335326 \n",
      "8 0:-1 1:-0.202703 2:-0.243697 3:-0.823009 4:-0.822915 5:-0.858104 6:-0.844635 7:-0.855506 \n",
      "6 0:-1 1:-0.22973 2:-0.277311 3:-0.823009 4:-0.847707 5:-0.882313 6:-0.870968 7:-0.860488 \n",
      "10 1:0.0675676 2:0.0252101 3:-0.769912 4:-0.66708 5:-0.756557 6:-0.701119 7:-0.704036 \n",
      "4 0:1 1:-0.662162 2:-0.697479 3:-0.893805 4:-0.975208 5:-0.984533 6:-0.9763 7:-0.981066 \n",
      "3 0:1 1:-0.756757 2:-0.781513 3:-0.946903 4:-0.986187 5:-0.99193 6:-0.98815 7:-0.993024 \n",
      "13 0:-1 1:0.540541 2:0.546218 3:-0.575221 4:0.0904905 5:-0.367855 6:-0.0296248 7:0.0632785 \n",
      "14 0:-1 1:0.283784 2:0.193277 3:-0.778761 4:-0.462724 5:-0.664425 6:-0.57077 7:-0.61435 \n",
      "10 0:-1 1:0.337838 2:0.277311 3:-0.743363 4:-0.360014 5:-0.473436 6:-0.381172 7:-0.454908 \n",
      "21 1:0.5 2:0.445378 3:-0.663717 4:-0.120595 5:-0.377942 6:-0.19684 7:-0.22571 \n",
      "14 0:-1 1:0.310811 2:0.294118 3:-0.752212 4:-0.313618 5:-0.405514 6:-0.462804 7:-0.474838 \n",
      "19 0:-1 1:0.405405 2:0.344538 3:-0.654867 4:-0.0589694 5:-0.383322 6:-0.14944 7:-0.315396 \n",
      "23 1:0.472973 2:0.394958 3:-0.646018 4:-0.133345 5:-0.488904 6:-0.290323 7:-0.136024 \n",
      "23 0:-1 1:0.5 2:0.445378 3:-0.690265 4:-0.0805738 5:-0.418292 6:-0.225806 7:-0.0861983 \n",
      "8 0:1 1:0.0135135 2:0.00840336 3:-0.80531 4:-0.676642 5:-0.740417 6:-0.824885 7:-0.723966 \n",
      "14 1:0.513514 2:0.613445 3:-0.663717 4:-0.121658 5:-0.226631 6:-0.349572 7:-0.22571 \n",
      "10 0:-1 1:0.0135135 2:-0.00840336 3:-0.823009 4:-0.741101 5:-0.804304 6:-0.734036 7:-0.763827 \n",
      "18 1:0.364865 2:0.344538 3:-0.725664 4:-0.40889 5:-0.577673 6:-0.636603 7:-0.365222 \n",
      "6 0:1 1:-0.310811 2:-0.327731 3:-0.831858 4:-0.879582 5:-0.912576 6:-0.934167 7:-0.883408 \n",
      "5 0:1 1:-0.486486 2:-0.478992 3:-0.893805 4:-0.933062 5:-0.944183 6:-0.943384 7:-0.943199 \n",
      "4 0:1 1:-0.689189 2:-0.697479 3:-0.929204 4:-0.9745 5:-0.979153 6:-0.9842 7:-0.973094 \n",
      "11 0:-1 1:-0.162162 2:-0.142857 3:-0.823009 4:-0.799894 5:-0.858104 6:-0.803818 7:-0.803687 \n",
      "5 0:1 1:-0.486486 2:-0.495798 3:-0.876106 4:-0.926687 5:-0.94889 6:-0.893351 7:-0.933234 \n",
      "7 0:-1 1:-0.297297 2:-0.294118 3:-0.814159 4:-0.844165 5:-0.875588 6:-0.853851 7:-0.853513 \n",
      "7 0:1 1:-0.243243 2:-0.260504 3:-0.840708 4:-0.823623 5:-0.870881 6:-0.861751 7:-0.843548 \n",
      "7 0:1 1:-0.337838 2:-0.327731 3:-0.823009 4:-0.877103 5:-0.903161 6:-0.892034 7:-0.873443 \n",
      "12 0:-1 1:0.175676 2:0.159664 3:-0.769912 4:-0.545599 5:-0.638198 6:-0.56287 7:-0.59442 \n",
      "8 0:-1 1:-0.22973 2:-0.193277 3:-0.814159 4:-0.830707 5:-0.885003 6:-0.861751 7:-0.813652 \n",
      "8 0:1 1:-0.22973 2:-0.243697 3:-0.840708 4:-0.841686 5:-0.882313 6:-0.895984 7:-0.823617 \n",
      "12 0:-1 1:0.148649 2:0.092437 3:-0.725664 4:-0.5796 5:-0.71419 6:-0.577354 7:-0.604385 \n",
      "8 1:-0.121622 2:-0.092437 3:-0.787611 4:-0.77581 5:-0.821116 6:-0.852535 7:-0.813652 \n",
      "5 0:1 1:-0.391892 2:-0.445378 3:-0.858407 4:-0.915707 5:-0.937458 6:-0.890718 7:-0.933234 \n",
      "5 0:1 1:-0.567568 2:-0.596639 3:-0.929204 4:-0.951479 5:-0.956288 6:-0.961817 7:-0.963129 \n",
      "16 1:0.797297 2:0.831933 3:-0.654867 4:0.396848 5:-0.197041 6:0.074391 7:0.412058 \n",
      "11 0:-1 1:0.472973 2:0.378151 3:-0.663717 4:-0.0511776 5:-0.234028 6:-0.16524 7:-0.295466 \n",
      "14 0:-1 1:0.418919 2:0.411765 3:-0.663717 4:-0.231096 5:-0.459314 6:-0.302172 7:-0.355257 \n",
      "16 0:-1 1:0.391892 2:0.327731 3:-0.672566 4:-0.0926155 5:-0.365165 6:-0.274523 7:-0.155954 \n",
      "13 0:-1 1:0.472973 2:0.411765 3:-0.672566 4:-0.0628652 5:-0.188299 6:-0.14549 7:-0.345291 \n",
      "15 1:0.324324 2:0.327731 3:-0.654867 4:-0.290597 5:-0.455279 6:-0.341672 7:-0.434978 \n",
      "14 0:-1 1:0.351351 2:0.344538 3:-0.743363 4:-0.1762 5:-0.219906 6:-0.402238 7:-0.405082 \n",
      "14 1:0.472973 2:0.529412 3:-0.637168 4:-0.0469276 5:-0.359112 6:-0.325872 7:-0.0463378 \n",
      "12 0:-1 1:0.472973 2:0.378151 3:-0.672566 4:-0.0989906 5:-0.222596 6:-0.194207 7:-0.365222 \n",
      "12 1:0.162162 2:0.0756303 3:-0.681416 4:-0.599079 5:-0.688635 6:-0.607637 7:-0.664175 \n",
      "8 1:0.0405405 2:0.243697 3:-0.725664 4:-0.472995 5:-0.597176 6:-0.601053 7:-0.524664 \n",
      "13 0:-1 1:0.121622 2:0.12605 3:-0.752212 4:-0.50062 5:-0.62542 6:-0.425938 7:-0.644245 \n",
      "9 1:0.216216 2:0.226891 3:-0.716814 4:-0.465911 5:-0.632145 6:-0.545754 7:-0.454908 \n",
      "6 0:1 1:-0.283784 2:-0.310924 3:-0.858407 4:-0.859749 5:-0.893746 6:-0.855168 7:-0.893373 \n",
      "8 0:1 1:-0.189189 2:-0.159664 3:-0.79646 4:-0.809102 5:-0.877606 6:-0.806452 7:-0.823617 \n",
      "14 0:-1 1:0.445946 2:0.428571 3:-0.734513 4:-0.151408 5:-0.248151 6:-0.354839 7:-0.444943 \n",
      "8 1:0.445946 2:0.478992 3:-0.672566 4:-0.1847 5:-0.28043 6:-0.23634 7:-0.514699 \n",
      "22 1:0.378378 2:0.327731 3:-0.699115 4:-0.386223 5:-0.554136 6:-0.57077 7:-0.464873 \n",
      "12 0:-1 1:0.337838 2:0.361345 3:-0.752212 4:-0.326014 5:-0.400807 6:-0.457538 7:-0.514699 \n",
      "9 0:-1 1:0.364865 2:0.344538 3:-0.699115 4:-0.34266 5:-0.452589 6:-0.319289 7:-0.564524 \n",
      "16 0:-1 1:0.513514 2:0.546218 3:-0.699115 4:-0.0982823 5:-0.316745 6:-0.24819 7:-0.325361 \n",
      "20 0:-1 1:0.689189 2:0.764706 3:-0.637168 4:0.507349 5:-0.00403497 6:0.289006 7:0.152965 \n",
      "13 0:-1 1:0.621622 2:0.579832 3:-0.672566 4:0.12272 5:-0.0685945 6:-0.116524 7:-0.215745 \n",
      "18 1:0.540541 2:0.579832 3:-0.663717 4:0.279617 5:-0.0551446 6:0.0217248 7:-0.215745 \n",
      "17 0:-1 1:0.810811 2:0.781513 3:-0.619469 4:0.768727 5:0.244788 6:0.241606 7:0.392128 \n",
      "16 1:0.648649 2:0.647059 3:-0.681416 4:0.25093 5:0.00672495 6:0.0309414 7:-0.0363727 \n",
      "18 0:-1 1:0.432432 2:0.462185 3:-0.681416 4:-0.132283 5:-0.35575 6:-0.245556 7:-0.305431 \n",
      "12 1:0.391892 2:0.378151 3:-0.734513 4:-0.295201 5:-0.474109 6:-0.353522 7:-0.325361 \n",
      "20 1:0.554054 2:0.579832 3:-0.690265 4:0.00619798 5:-0.180901 6:-0.21264 7:-0.116094 \n",
      "16 1:0.418919 2:0.428571 3:-0.734513 4:-0.272534 5:-0.451917 6:-0.289006 7:-0.415047 \n",
      "12 1:0.472973 2:0.495798 3:-0.690265 4:-0.161325 5:-0.330868 6:-0.207373 7:-0.305431 \n",
      "19 0:-1 1:0.5 2:0.546218 3:-0.716814 4:-0.281743 5:-0.434432 6:-0.358789 7:-0.295466 \n",
      "11 0:-1 1:0.364865 2:0.378151 3:-0.743363 4:-0.373118 5:-0.408877 6:-0.565504 7:-0.474838 \n",
      "10 1:0.364865 2:0.344538 3:-0.787611 4:-0.241013 5:-0.357095 6:-0.281106 7:-0.474838 \n",
      "12 0:-1 1:0.5 2:0.462185 3:-0.681416 4:-0.200992 5:-0.38534 6:-0.273206 7:-0.375187 \n",
      "17 1:0.662162 2:0.697479 3:-0.619469 4:0.216221 5:-0.0867518 6:-0.213957 7:-0.0662681 \n",
      "16 1:0.554054 2:0.647059 3:-0.707965 4:0.107845 5:-0.107599 6:-0.0915076 7:-0.175884 \n",
      "16 1:0.581081 2:0.714286 3:-0.654867 4:0.245617 5:-0.070612 6:-0.14154 7:-0.00647733 \n",
      "19 1:0.635135 2:0.764706 3:-0.646018 4:0.264388 5:-0.214526 6:0.191573 7:0.192825 \n",
      "14 1:0.689189 2:0.747899 3:-0.699115 4:-0.0734903 5:-0.316073 6:-0.174457 7:-0.165919 \n",
      "13 0:-1 1:0.648649 2:0.563025 3:-0.734513 4:-0.0501151 5:-0.377942 6:-0.23239 7:-0.20578 \n",
      "20 1:0.621622 2:0.647059 3:-0.654867 4:0.2272 5:-0.0806994 6:-0.0283081 7:0.20279 \n",
      "11 0:-1 1:0.5 2:0.462185 3:-0.663717 4:-0.167345 5:-0.337592 6:-0.115207 7:-0.434978 \n",
      "10 1:0.378378 2:0.327731 3:-0.716814 4:-0.238534 5:-0.329523 6:-0.24424 7:-0.504733 \n",
      "15 0:-1 1:0.324324 2:0.378151 3:-0.690265 4:-0.296618 5:-0.477471 6:-0.519421 7:-0.265571 \n",
      "12 1:0.445946 2:0.478992 3:-0.672566 4:-0.216221 5:-0.503026 6:-0.175774 7:-0.345291 \n",
      "15 0:-1 1:0.432432 2:0.394958 3:-0.681416 4:-0.193554 5:-0.496301 6:-0.262673 7:-0.235675 \n",
      "10 0:-1 1:0.243243 2:0.226891 3:-0.743363 4:-0.441119 5:-0.557498 6:-0.503621 7:-0.504733 \n",
      "10 0:-1 1:0.108108 2:0.159664 3:-0.761062 4:-0.531787 5:-0.580363 6:-0.640553 7:-0.604385 \n",
      "12 0:-1 1:0.0675676 2:0.0756303 3:-0.787611 4:-0.607225 5:-0.697377 6:-0.680053 7:-0.61435 \n",
      "10 0:-1 1:0.27027 2:0.243697 3:-0.761062 4:-0.403223 5:-0.499664 6:-0.448321 7:-0.534629 \n",
      "9 1:0.027027 2:0.0588235 3:-0.814159 4:-0.652559 5:-0.710827 6:-0.673469 7:-0.733931 \n",
      "12 0:-1 1:0.256757 2:0.226891 3:-0.725664 4:-0.478307 5:-0.528581 6:-0.601053 7:-0.574489 \n",
      "10 0:-1 1:0.0405405 2:0.092437 3:-0.761062 4:-0.659996 5:-0.722932 6:-0.678736 7:-0.714001 \n",
      "14 0:-1 1:0.121622 2:0.226891 3:-0.778761 4:-0.570037 5:-0.679892 6:-0.623436 7:-0.564524 \n",
      "9 0:1 1:0.0540541 2:0.0756303 3:-0.787611 4:-0.667788 5:-0.702757 6:-0.687953 7:-0.723966 \n",
      "10 0:1 1:-0.0810811 2:-0.092437 3:-0.823009 4:-0.773685 5:-0.794889 6:-0.844635 7:-0.793722 \n",
      "10 0:-1 1:0.0810811 2:0.0756303 3:-0.778761 4:-0.581371 5:-0.628783 6:-0.698486 7:-0.644245 \n",
      "10 1:0.0675676 2:0.0756303 3:-0.778761 4:-0.603683 5:-0.662408 6:-0.640553 7:-0.644245 \n",
      "9 0:1 1:-0.216216 2:-0.193277 3:-0.831858 4:-0.824332 5:-0.856759 6:-0.857801 7:-0.843548 \n",
      "6 0:1 1:-0.27027 2:-0.260504 3:-0.831858 4:-0.860103 5:-0.899798 6:-0.860434 7:-0.863478 \n",
      "11 0:1 1:-0.148649 2:-0.142857 3:-0.823009 4:-0.787498 5:-0.845326 6:-0.832785 7:-0.773792 \n",
      "10 1:0.148649 2:0.142857 3:-0.752212 4:-0.494599 5:-0.575656 6:-0.537854 7:-0.524664 \n",
      "8 0:-1 1:0.0675676 2:0.092437 3:-0.743363 4:-0.585975 5:-0.680565 6:-0.62212 7:-0.63428 \n",
      "12 0:-1 1:0.243243 2:0.294118 3:-0.734513 4:-0.522224 5:-0.657028 6:-0.635286 7:-0.484803 \n",
      "11 0:-1 1:0.378378 2:0.344538 3:-0.734513 4:-0.302284 5:-0.415602 6:-0.454905 7:-0.385152 \n",
      "9 1:0.108108 2:0.0420168 3:-0.787611 4:-0.584558 5:-0.638198 6:-0.656353 7:-0.65421 \n",
      "9 0:-1 1:0.189189 2:0.344538 3:-0.761062 4:-0.489641 5:-0.604573 6:-0.573404 7:-0.534629 \n",
      "7 1:-0.027027 2:-0.092437 3:-0.80531 4:-0.694351 5:-0.761937 6:-0.741935 7:-0.694071 \n",
      "10 1:0.189189 2:0.210084 3:-0.752212 4:-0.510182 5:-0.582381 6:-0.601053 7:-0.604385 \n",
      "7 0:1 1:-0.0135135 2:-0.0252101 3:-0.787611 4:-0.742872 5:-0.778749 6:-0.782752 7:-0.783757 \n",
      "12 1:0.216216 2:0.294118 3:-0.734513 4:-0.40464 5:-0.505716 6:-0.478604 7:-0.524664 \n",
      "8 0:-1 1:0.0135135 2:0.00840336 3:-0.79646 4:-0.662121 5:-0.715535 6:-0.726136 7:-0.704036 \n",
      "16 0:-1 1:0.391892 2:0.445378 3:-0.787611 4:-0.356118 5:-0.476799 6:-0.522054 7:-0.425012 \n",
      "11 0:-1 1:0.297297 2:0.327731 3:-0.743363 4:-0.353285 5:-0.463349 6:-0.353522 7:-0.434978 \n",
      "8 0:-1 1:0.337838 2:0.294118 3:-0.831858 4:-0.415619 5:-0.544721 6:-0.418038 7:-0.534629 \n",
      "15 0:-1 1:0.391892 2:0.495798 3:-0.707965 4:-0.219054 5:-0.387357 6:-0.362739 7:-0.325361 \n",
      "14 0:-1 1:0.378378 2:0.411765 3:-0.787611 4:-0.332035 5:-0.449899 6:-0.444371 7:-0.444943 \n",
      "11 1:0.364865 2:0.361345 3:-0.787611 4:-0.29768 5:-0.379287 6:-0.373272 7:-0.444943 \n",
      "12 0:-1 1:0.27027 2:0.294118 3:-0.787611 4:-0.394723 5:-0.533961 6:-0.549704 7:-0.524664 \n",
      "14 1:0.432432 2:0.478992 3:-0.699115 4:-0.124137 5:-0.29119 6:-0.352205 7:-0.22571 \n",
      "15 1:0.472973 2:0.394958 3:-0.752212 4:-0.270055 5:-0.516476 6:-0.411455 7:-0.285501 \n",
      "20 1:0.5 2:0.495798 3:-0.699115 4:-0.0710112 5:-0.248823 6:-0.298223 7:-0.20578 \n",
      "20 0:-1 1:0.5 2:0.546218 3:-0.707965 4:-0.04374 5:-0.34499 6:-0.082291 7:-0.106129 \n",
      "16 1:0.5 2:0.495798 3:-0.725664 4:-0.289534 5:-0.507734 6:-0.477288 7:-0.285501 \n",
      "13 0:-1 1:0.27027 2:0.193277 3:-0.752212 4:-0.558704 5:-0.701412 6:-0.579987 7:-0.534629 \n",
      "14 1:0.608108 2:0.630252 3:-0.707965 4:0.0621569 5:-0.30464 6:-0.0585912 7:0.00348779 \n",
      "11 0:1 1:0.121622 2:0.092437 3:-0.787611 4:-0.626704 5:-0.710155 6:-0.635286 7:-0.694071 \n",
      "13 1:0.121622 2:0.12605 3:-0.761062 4:-0.591642 5:-0.669805 6:-0.677419 7:-0.604385 \n",
      "8 0:1 1:-0.418919 2:-0.428571 3:-0.876106 4:-0.929874 5:-0.952925 6:-0.939434 7:-0.933234 \n",
      "6 0:1 1:-0.5 2:-0.512605 3:-0.876106 4:-0.936249 5:-0.95158 6:-0.94865 7:-0.943199 \n",
      "13 0:-1 1:0.364865 2:0.327731 3:-0.690265 4:-0.244909 5:-0.429724 6:-0.466754 7:-0.365222 \n",
      "18 1:0.445946 2:0.445378 3:-0.707965 4:-0.228263 5:-0.472091 6:-0.360105 7:-0.345291 \n",
      "19 0:-1 1:0.418919 2:0.495798 3:-0.716814 4:-0.282451 5:-0.464022 6:-0.544437 7:-0.345291 \n",
      "21 1:0.310811 2:0.344538 3:-0.778761 4:-0.333451 5:-0.538668 6:-0.661619 7:-0.255605 \n",
      "18 1:0.351351 2:0.327731 3:-0.699115 4:-0.25943 5:-0.493611 6:-0.552337 7:-0.235675 \n",
      "18 1:0.337838 2:0.327731 3:-0.690265 4:-0.324597 5:-0.490249 6:-0.56287 7:-0.415047 \n",
      "20 0:-1 1:0.418919 2:0.394958 3:-0.725664 4:-0.267576 5:-0.412912 6:-0.485188 7:-0.355257 \n",
      "18 0:-1 1:0.324324 2:0.344538 3:-0.699115 4:-0.359306 5:-0.541358 6:-0.59052 7:-0.365222 \n",
      "22 0:-1 1:0.27027 2:0.226891 3:-0.752212 4:-0.469807 5:-0.66846 6:-0.658986 7:-0.494768 \n",
      "13 0:1 1:-0.0135135 2:-0.0252101 3:-0.823009 4:-0.742164 5:-0.837256 6:-0.763002 7:-0.763827 \n",
      "11 0:-1 1:0.148649 2:0.193277 3:-0.734513 4:-0.532495 5:-0.62273 6:-0.640553 7:-0.564524 \n",
      "7 0:1 1:-0.22973 2:-0.260504 3:-0.831858 4:-0.847707 5:-0.888366 6:-0.886768 7:-0.823617 \n",
      "14 0:1 1:-0.162162 2:-0.159664 3:-0.831858 4:-0.822915 5:-0.878278 6:-0.856485 7:-0.823617 \n",
      "9 0:-1 1:-0.148649 2:-0.176471 3:-0.840708 4:-0.785019 5:-0.809011 6:-0.831468 7:-0.833582 \n",
      "13 0:-1 1:0.148649 2:0.210084 3:-0.707965 4:-0.513724 5:-0.666443 6:-0.63792 7:-0.504733 \n",
      "10 0:1 1:-0.22973 2:-0.260504 3:-0.80531 4:-0.836019 5:-0.873571 6:-0.863068 7:-0.833582 \n",
      "8 0:1 1:-0.297297 2:-0.310924 3:-0.823009 4:-0.865415 5:-0.895763 6:-0.847268 7:-0.863478 \n",
      "19 1:0.162162 2:0.243697 3:-0.752212 4:-0.399327 5:-0.631473 6:-0.573404 7:-0.434978 \n",
      "10 0:1 1:-0.135135 2:-0.193277 3:-0.823009 4:-0.809102 5:-0.821116 6:-0.915734 7:-0.833582 \n",
      "10 1:-0.0945946 2:-0.092437 3:-0.814159 4:-0.743935 5:-0.787492 6:-0.798552 7:-0.763827 \n",
      "9 1:0.310811 2:0.344538 3:-0.663717 4:-0.495661 5:-0.620713 6:-0.661619 7:-0.454908 \n",
      "13 0:-1 1:0.324324 2:0.277311 3:-0.672566 4:-0.30618 5:-0.558843 6:-0.643186 7:-0.22571 \n",
      "16 0:-1 1:0.324324 2:0.344538 3:-0.672566 4:-0.345139 5:-0.525219 6:-0.58657 7:-0.255605 \n",
      "12 0:-1 1:0.432432 2:0.495798 3:-0.690265 4:-0.223659 5:-0.360457 6:-0.390388 7:-0.255605 \n",
      "18 1:0.324324 2:0.344538 3:-0.734513 4:-0.420223 5:-0.510424 6:-0.58262 7:-0.484803 \n",
      "16 0:-1 1:0.756757 2:0.714286 3:-0.619469 4:0.338056 5:-0.0632145 6:0.242923 7:0.152965 \n",
      "16 1:0.621622 2:0.613445 3:-0.716814 4:-0.00265628 5:-0.205111 6:-0.16524 7:-0.165919 \n",
      "17 1:0.594595 2:0.680672 3:-0.654867 4:0.0175314 5:-0.219906 6:-0.0691244 7:-0.285501 \n",
      "11 1:0.324324 2:0.462185 3:-0.725664 4:-0.346556 5:-0.456624 6:-0.423305 7:-0.494768 \n",
      "14 1:0.540541 2:0.663866 3:-0.690265 4:-0.0865947 5:-0.234701 6:-0.199473 7:-0.345291 \n",
      "11 0:-1 1:0.351351 2:0.394958 3:-0.752212 4:-0.408181 5:-0.532616 6:-0.544437 7:-0.524664 \n",
      "15 1:0.527027 2:0.630252 3:-0.690265 4:-0.136533 5:-0.3154 6:-0.319289 7:-0.22571 \n",
      "9 0:1 1:-0.22973 2:-0.243697 3:-0.814159 4:-0.860457 5:-0.907868 6:-0.882818 7:-0.843548 \n",
      "10 0:1 1:-0.0810811 2:-0.142857 3:-0.80531 4:-0.791394 5:-0.835911 6:-0.851218 7:-0.804684 \n",
      "11 1:0.216216 2:0.193277 3:-0.761062 4:-0.499557 5:-0.607263 6:-0.599737 7:-0.534629 \n",
      "11 0:-1 1:-0.175676 2:-0.226891 3:-0.823009 4:-0.812644 5:-0.846671 6:-0.840685 7:-0.853513 \n",
      "12 1:0.378378 2:0.378151 3:-0.699115 4:-0.299097 5:-0.481506 6:-0.411455 7:-0.474838 \n",
      "6 0:1 1:-0.554054 2:-0.563025 3:-0.876106 4:-0.95077 5:-0.966375 6:-0.953917 7:-0.953164 \n",
      "5 0:1 1:-0.608108 2:-0.630252 3:-0.902655 4:-0.962812 5:-0.972428 6:-0.969717 7:-0.963129 \n",
      "6 0:1 1:-0.513514 2:-0.529412 3:-0.876106 4:-0.949354 5:-0.967048 6:-0.94865 7:-0.953164 \n",
      "5 0:1 1:-0.72973 2:-0.764706 3:-0.911504 4:-0.984771 5:-0.990585 6:-0.9921 7:-0.987045 \n",
      "12 1:0.608108 2:0.663866 3:-0.663717 4:-0.0164689 5:-0.271688 6:-0.202107 7:-0.20578 \n",
      "13 0:-1 1:0.567568 2:0.596639 3:-0.654867 4:-0.0182398 5:-0.238736 6:-0.281106 7:-0.18585 \n",
      "17 1:0.635135 2:0.663866 3:-0.628319 4:0.234284 5:-0.197714 6:-0.198157 7:0.242651 \n",
      "21 0:-1 1:0.621622 2:0.680672 3:-0.646018 4:0.0175314 5:-0.268325 6:-0.299539 7:-0.0762332 \n",
      "9 1:0.22973 2:0.294118 3:-0.761062 4:-0.446432 5:-0.580363 6:-0.549704 7:-0.567514 \n",
      "10 1:0.189189 2:0.176471 3:-0.787611 4:-0.543829 5:-0.61197 6:-0.631336 7:-0.650224 \n",
      "9 0:1 1:-0.0405405 2:-0.0420168 3:-0.787611 4:-0.748185 5:-0.798252 6:-0.831468 7:-0.794718 \n",
      "11 1:0.202703 2:0.176471 3:-0.787611 4:-0.557287 5:-0.645595 6:-0.628703 7:-0.642252 \n",
      "10 1:0.27027 2:0.210084 3:-0.716814 4:-0.454932 5:-0.635508 6:-0.61817 7:-0.451918 \n",
      "17 0:-1 1:0.22973 2:0.210084 3:-0.690265 4:-0.477599 5:-0.650303 6:-0.63397 7:-0.475835 \n",
      "9 1:0.0540541 2:-0.00840336 3:-0.79646 4:-0.703205 5:-0.790854 6:-0.761685 7:-0.734928 \n",
      "17 0:-1 1:0.594595 2:0.630252 3:-0.690265 4:-0.0472817 5:-0.334902 6:-0.332456 7:-0.175884 \n",
      "21 0:-1 1:0.783784 2:0.798319 3:-0.60177 4:0.24243 5:-0.144586 6:-0.104674 7:0.152965 \n",
      "16 0:-1 1:0.581081 2:0.647059 3:-0.672566 4:-0.0664069 5:-0.287828 6:-0.307439 7:-0.0961634 \n",
      "29 1:0.689189 2:0.781513 3:-0.672566 4:0.278909 5:-0.0524546 6:-0.154707 7:-0.0563029 \n",
      "17 0:-1 1:0.351351 2:0.159664 3:-0.725664 4:-0.340889 5:-0.516476 6:-0.357472 7:-0.405082 \n",
      "15 0:-1 1:0.337838 2:0.378151 3:-0.778761 4:-0.400035 5:-0.492266 6:-0.536537 7:-0.524664 \n",
      "19 1:0.364865 2:0.361345 3:-0.734513 4:-0.296264 5:-0.424344 6:-0.443055 7:-0.484803 \n",
      "12 0:-1 1:0.5 2:0.428571 3:-0.743363 4:-0.28493 5:-0.431742 6:-0.377222 7:-0.395117 \n",
      "13 1:0.378378 2:0.378151 3:-0.752212 4:-0.358243 5:-0.488904 6:-0.576037 7:-0.375187 \n",
      "11 0:-1 1:0.283784 2:0.327731 3:-0.769912 4:-0.349743 5:-0.492939 6:-0.373272 7:-0.425012 \n",
      "15 1:0.486486 2:0.546218 3:-0.734513 4:-0.122012 5:-0.297243 6:-0.194207 7:-0.285501 \n",
      "11 0:-1 1:0.256757 2:0.226891 3:-0.761062 4:-0.429432 5:-0.532616 6:-0.528637 7:-0.534629 \n",
      "14 1:0.337838 2:0.344538 3:-0.707965 4:-0.25093 5:-0.409549 6:-0.423305 7:-0.434978 \n",
      "14 0:-1 1:0.391892 2:0.344538 3:-0.743363 4:-0.241367 5:-0.362475 6:-0.500987 7:-0.434978 \n",
      "13 0:-1 1:0.364865 2:0.361345 3:-0.769912 4:-0.349035 5:-0.521184 6:-0.524687 7:-0.425012 \n",
      "11 1:0.567568 2:0.529412 3:-0.725664 4:-0.0880113 5:-0.282448 6:-0.249506 7:-0.18585 \n",
      "15 0:-1 1:0.567568 2:0.596639 3:-0.690265 4:-0.106428 5:-0.34768 6:-0.307439 7:-0.175884 \n",
      "17 0:-1 1:0.486486 2:0.495798 3:-0.654867 4:-0.0316982 5:-0.211163 6:-0.425938 7:-0.265571 \n",
      "15 1:0.486486 2:0.495798 3:-0.734513 4:-0.326368 5:-0.537996 6:-0.412772 7:-0.395117 \n",
      "12 1:0.527027 2:0.563025 3:-0.690265 4:-0.117407 5:-0.430397 6:-0.317972 7:-0.0463378 \n",
      "19 1:0.432432 2:0.445378 3:-0.707965 4:-0.285638 5:-0.416274 6:-0.450955 7:-0.405082 \n",
      "11 1:0.459459 2:0.579832 3:-0.725664 4:-0.265805 5:-0.427034 6:-0.391705 7:-0.315396 \n",
      "10 0:-1 1:0.297297 2:0.327731 3:-0.690265 4:-0.382327 5:-0.560861 6:-0.469388 7:-0.395117 \n",
      "12 1:0.364865 2:0.294118 3:-0.681416 4:-0.396494 5:-0.508406 6:-0.57077 7:-0.514699 \n",
      "23 1:0.472973 2:0.563025 3:-0.60177 4:-0.163095 5:-0.492939 6:-0.290323 7:-0.215745 \n",
      "15 1:0.472973 2:0.394958 3:-0.60177 4:-0.211617 5:-0.492939 6:-0.436471 7:-0.285501 \n",
      "13 1:0.418919 2:0.512605 3:-0.663717 4:-0.2017 5:-0.411567 6:-0.327189 7:-0.285501 \n",
      "17 1:0.486486 2:0.445378 3:-0.663717 4:-0.169471 5:-0.411567 6:-0.394338 7:-0.165919 \n",
      "15 0:-1 1:0.418919 2:0.394958 3:-0.690265 4:-0.2187 5:-0.347007 6:-0.350889 7:-0.375187 \n",
      "12 0:-1 1:0.310811 2:0.361345 3:-0.584071 4:-0.406765 5:-0.554136 6:-0.587887 7:-0.395117 \n",
      "15 0:-1 1:0.378378 2:0.344538 3:-0.60177 4:-0.254117 5:-0.488231 6:-0.419355 7:-0.275536 \n",
      "11 0:-1 1:0.310811 2:0.277311 3:-0.681416 4:-0.371702 5:-0.517149 6:-0.464121 7:-0.504733 \n",
      "16 0:1 1:0.310811 2:0.310924 3:-0.725664 4:-0.382681 5:-0.597176 6:-0.450955 7:-0.454908 \n",
      "10 0:1 1:0.635135 2:0.596639 3:-0.672566 4:-0.215513 5:-0.410894 6:-0.356155 7:-0.325361 \n",
      "10 1:0.027027 2:-0.00840336 3:-0.752212 4:-0.634142 5:-0.704102 6:-0.668203 7:-0.733931 \n",
      "10 1:0.121622 2:0.092437 3:-0.743363 4:-0.525058 5:-0.666443 6:-0.524687 7:-0.584454 \n",
      "6 0:-1 1:-0.364865 2:-0.445378 3:-0.849558 4:-0.897999 5:-0.919301 6:-0.905201 7:-0.913303 \n",
      "5 1:-0.459459 2:-0.529412 3:-0.876106 4:-0.944749 5:-0.95965 6:-0.9447 7:-0.953164 \n",
      "6 0:-1 1:-0.472973 2:-0.529412 3:-0.858407 4:-0.930583 5:-0.949563 6:-0.949967 7:-0.943199 \n",
      "9 0:-1 1:-0.121622 2:-0.210084 3:-0.79646 4:-0.803435 5:-0.851379 6:-0.849901 7:-0.853513 \n",
      "5 0:-1 1:-0.445946 2:-0.512605 3:-0.858407 4:-0.936604 5:-0.95696 6:-0.9447 7:-0.943199 \n",
      "4 0:-1 1:-0.324324 2:-0.411765 3:-0.840708 4:-0.897291 5:-0.920646 6:-0.911784 7:-0.913303 \n",
      "6 1:-0.27027 2:-0.344538 3:-0.840708 4:-0.857624 5:-0.896436 6:-0.845951 7:-0.893373 \n",
      "3 0:-1 1:-0.635135 2:-0.680672 3:-0.911504 4:-0.974146 5:-0.980498 6:-0.978934 7:-0.983059 \n",
      "5 1:-0.22973 2:-0.277311 3:-0.840708 4:-0.867894 5:-0.887693 6:-0.899934 7:-0.893373 \n",
      "9 0:1 1:-0.216216 2:-0.310924 3:-0.79646 4:-0.846998 5:-0.875588 6:-0.884134 7:-0.863478 \n",
      "5 0:-1 1:-0.662162 2:-0.714286 3:-0.902655 4:-0.976625 5:-0.981843 6:-0.98025 7:-0.983059 \n",
      "4 0:-1 1:-0.567568 2:-0.647059 3:-0.893805 4:-0.962812 5:-0.965703 6:-0.9763 7:-0.973094 \n",
      "4 0:-1 1:-0.72973 2:-0.764706 3:-0.929204 4:-0.984417 5:-0.988568 6:-0.985517 7:-0.993024 \n",
      "3 0:-1 1:-0.783784 2:-0.815126 3:-0.929204 4:-0.990437 5:-0.992603 6:-0.993417 7:-0.993024 \n",
      "14 1:0.337838 2:0.310924 3:-0.725664 4:-0.482203 5:-0.622058 6:-0.58262 7:-0.534629 \n",
      "12 1:0.337838 2:0.327731 3:-0.716814 4:-0.313264 5:-0.468056 6:-0.329822 7:-0.484803 \n",
      "9 0:-1 1:-0.162162 2:-0.176471 3:-0.831858 4:-0.831415 5:-0.882313 6:-0.845951 7:-0.833582 \n",
      "20 0:1 1:0.22973 2:0.226891 3:-0.672566 4:-0.468744 5:-0.599193 6:-0.59052 7:-0.59442 \n",
      "13 1:0.0405405 2:0.00840336 3:-0.769912 4:-0.676997 5:-0.743107 6:-0.723502 7:-0.743896 \n",
      "12 0:1 1:0.0675676 2:0.0588235 3:-0.787611 4:-0.668142 5:-0.753194 6:-0.723502 7:-0.694071 \n",
      "9 1:-0.027027 2:-0.0588235 3:-0.80531 4:-0.732247 5:-0.773369 6:-0.774852 7:-0.783757 \n",
      "10 0:1 1:0.0675676 2:0.0588235 3:-0.752212 4:-0.648309 5:-0.71957 6:-0.678736 7:-0.714001 \n",
      "11 0:1 1:0.0540541 2:0.092437 3:-0.769912 4:-0.67983 5:-0.746469 6:-0.790652 7:-0.694071 \n",
      "11 0:1 1:0.202703 2:0.176471 3:-0.752212 4:-0.59235 5:-0.732347 6:-0.619487 7:-0.646238 \n",
      "7 0:-1 1:-0.418919 2:-0.411765 3:-0.867257 4:-0.918895 5:-0.94351 6:-0.934167 7:-0.923269 \n",
      "5 0:-1 1:-0.459459 2:-0.495798 3:-0.876106 4:-0.934833 5:-0.956288 6:-0.94865 7:-0.938216 \n",
      "10 1:-0.189189 2:-0.210084 3:-0.79646 4:-0.809811 5:-0.876261 6:-0.827518 7:-0.826607 \n",
      "15 1:0.148649 2:0.0756303 3:-0.752212 4:-0.573579 5:-0.675857 6:-0.628703 7:-0.646238 \n",
      "11 1:-0.0135135 2:0.00840336 3:-0.79646 4:-0.707455 5:-0.788164 6:-0.757735 7:-0.741903 \n",
      "15 0:-1 1:-0.0675676 2:-0.092437 3:-0.79646 4:-0.79706 5:-0.866846 6:-0.851218 7:-0.776781 \n",
      "8 0:-1 2:-0.00840336 3:-0.79646 4:-0.745352 5:-0.790854 6:-0.818302 7:-0.769806 \n",
      "10 1:-0.175676 2:-0.210084 3:-0.814159 4:-0.819373 5:-0.868191 6:-0.867018 7:-0.833582 \n",
      "11 0:-1 1:-0.337838 2:-0.361345 3:-0.867257 4:-0.89127 5:-0.922663 6:-0.901251 7:-0.905331 \n",
      "8 0:-1 1:-0.513514 2:-0.529412 3:-0.884956 4:-0.944749 5:-0.958978 6:-0.953917 7:-0.949178 \n",
      "7 0:-1 1:-0.648649 2:-0.663866 3:-0.920354 4:-0.971312 5:-0.97848 6:-0.986833 7:-0.972098 \n",
      "12 1:0.324324 2:0.327731 3:-0.716814 4:-0.438286 5:-0.516476 6:-0.591837 7:-0.544594 \n",
      "11 0:1 1:0.297297 2:0.243697 3:-0.681416 4:-0.381619 5:-0.504371 6:-0.473338 7:-0.494768 \n",
      "17 0:1 1:0.554054 2:0.546218 3:-0.716814 4:-0.177971 5:-0.335575 6:-0.466754 7:-0.345291 \n",
      "13 0:1 1:0.459459 2:0.462185 3:-0.725664 4:-0.301222 5:-0.443847 6:-0.487821 7:-0.315396 \n",
      "12 0:1 1:0.310811 2:0.294118 3:-0.707965 4:-0.434744 5:-0.550773 6:-0.544437 7:-0.504733 \n",
      "13 0:1 1:0.0945946 2:0.0588235 3:-0.787611 4:-0.63733 5:-0.72226 6:-0.656353 7:-0.694071 \n",
      "9 0:1 1:0.108108 2:0.12605 3:-0.778761 4:-0.582787 5:-0.615333 6:-0.63002 7:-0.763827 \n",
      "10 0:1 1:0.148649 2:0.109244 3:-0.734513 4:-0.557641 5:-0.650975 6:-0.56287 7:-0.684106 \n",
      "14 0:1 1:0.216216 2:0.176471 3:-0.734513 4:-0.438286 5:-0.587761 6:-0.461488 7:-0.494768 \n",
      "12 1:0.581081 2:0.495798 3:-0.707965 4:-0.158137 5:-0.384667 6:-0.21659 7:-0.265571 \n",
      "13 1:0.581081 2:0.596639 3:-0.699115 4:-0.0621569 5:-0.303295 6:-0.310072 7:-0.126059 \n",
      "13 0:1 1:0.202703 2:0.159664 3:-0.743363 4:-0.533912 5:-0.642233 6:-0.723502 7:-0.564524 \n",
      "8 1:-0.0135135 2:-0.0420168 3:-0.814159 4:-0.743581 5:-0.802286 6:-0.789335 7:-0.768809 \n",
      "13 0:1 1:0.189189 2:0.159664 3:-0.787611 4:-0.53462 5:-0.637525 6:-0.529954 7:-0.664175 \n",
      "11 1:0.0810811 2:-0.00840336 3:-0.79646 4:-0.681247 5:-0.770679 6:-0.759052 7:-0.694071 \n",
      "11 1:0.27027 2:0.210084 3:-0.734513 4:-0.481849 5:-0.62542 6:-0.572087 7:-0.567514 \n",
      "14 1:0.0675676 2:0.00840336 3:-0.769912 4:-0.614308 5:-0.731675 6:-0.669519 7:-0.63428 \n",
      "10 0:-1 1:-0.256757 2:-0.327731 3:-0.884956 4:-0.874624 5:-0.906523 6:-0.899934 7:-0.883408 \n",
      "12 0:1 1:0.108108 2:0.00840336 3:-0.769912 4:-0.589871 5:-0.671822 6:-0.65372 7:-0.668161 \n",
      "12 0:1 1:-0.027027 2:-0.0756303 3:-0.778761 4:-0.71383 5:-0.774714 6:-0.723502 7:-0.811659 \n",
      "7 0:-1 1:-0.445946 2:-0.478992 3:-0.858407 4:-0.924562 5:-0.9462 6:-0.931534 7:-0.93423 \n",
      "11 1:-0.0945946 2:-0.109244 3:-0.79646 4:-0.727289 5:-0.779422 6:-0.736669 7:-0.806677 \n",
      "16 0:1 1:0.0135135 2:-0.00840336 3:-0.752212 4:-0.665663 5:-0.718897 6:-0.714286 7:-0.748879 \n",
      "14 0:1 1:0.0135135 2:-0.0252101 3:-0.761062 4:-0.687622 5:-0.735709 6:-0.770902 7:-0.768809 \n",
      "20 1:0.391892 2:0.344538 3:-0.725664 4:-0.246325 5:-0.487559 6:-0.402238 7:-0.175884 \n",
      "17 1:0.337838 2:0.294118 3:-0.752212 4:-0.326014 5:-0.492266 6:-0.472021 7:-0.395117 \n",
      "10 0:1 1:0.445946 2:0.411765 3:-0.734513 4:-0.316805 5:-0.443847 6:-0.474654 7:-0.315396 \n",
      "11 1:0.445946 2:0.411765 3:-0.752212 4:-0.198867 5:-0.291863 6:-0.381172 7:-0.305431 \n",
      "10 0:1 1:0.310811 2:0.243697 3:-0.752212 4:-0.351514 5:-0.462677 6:-0.481238 7:-0.484803 \n",
      "10 1:0.378378 2:0.277311 3:-0.690265 4:-0.305826 5:-0.455952 6:-0.344305 7:-0.464873 \n",
      "11 0:1 1:0.364865 2:0.310924 3:-0.734513 4:-0.373473 5:-0.486214 6:-0.450955 7:-0.494768 \n",
      "17 1:0.5 2:0.428571 3:-0.690265 4:-0.0327608 5:-0.326833 6:-0.202107 7:0.023418 \n",
      "11 1:0.486486 2:0.462185 3:-0.690265 4:-0.128033 5:-0.252858 6:-0.350889 7:-0.275536 \n",
      "14 0:1 1:0.283784 2:0.243697 3:-0.734513 4:-0.430494 5:-0.495629 6:-0.55102 7:-0.514699 \n",
      "19 1:0.540541 2:0.579832 3:-0.663717 4:0.03524 5:-0.111634 6:-0.0967742 7:-0.136024 \n",
      "13 0:1 1:0.0405405 2:0.00840336 3:-0.752212 4:-0.651851 5:-0.710827 6:-0.651086 7:-0.773792 \n",
      "11 1:-0.0945946 2:-0.159664 3:-0.823009 4:-0.744289 5:-0.767989 6:-0.830151 7:-0.783757 \n",
      "11 0:1 1:0.135135 2:0.12605 3:-0.778761 4:-0.530016 5:-0.619368 6:-0.57472 7:-0.604385 \n",
      "10 0:1 1:0.202703 2:0.243697 3:-0.699115 4:-0.519391 5:-0.624748 6:-0.54312 7:-0.61435 \n",
      "13 1:0.283784 2:0.193277 3:-0.743363 4:-0.414556 5:-0.585071 6:-0.499671 7:-0.504733 \n",
      "14 0:-1 1:0.0135135 2:-0.0588235 3:-0.752212 4:-0.673809 5:-0.780767 6:-0.801185 7:-0.704036 \n",
      "13 1:-0.108108 2:-0.142857 3:-0.787611 4:-0.782185 5:-0.815736 6:-0.848585 7:-0.813652 \n",
      "13 0:1 1:0.175676 2:0.159664 3:-0.734513 4:-0.473703 5:-0.616005 6:-0.560237 7:-0.534629 \n",
      "9 1:-0.202703 2:-0.210084 3:-0.79646 4:-0.824332 5:-0.852051 6:-0.851218 7:-0.853513 \n",
      "18 0:1 1:0.216216 2:0.193277 3:-0.690265 4:-0.382327 5:-0.519166 6:-0.456221 7:-0.59442 \n",
      "19 1:0.581081 2:0.563025 3:-0.681416 4:0.0710112 5:-0.29388 6:-0.217907 7:-0.165919 \n",
      "12 0:-1 1:0.243243 2:0.226891 3:-0.734513 4:-0.505932 5:-0.65501 6:-0.59842 7:-0.524664 \n",
      "13 0:1 1:0.351351 2:0.344538 3:-0.681416 4:-0.397556 5:-0.595831 6:-0.520737 7:-0.405082 \n",
      "9 1:0.283784 2:0.260504 3:-0.752212 4:-0.496016 5:-0.656355 6:-0.511521 7:-0.554559 \n",
      "13 0:1 1:0.432432 2:0.394958 3:-0.752212 4:-0.336285 5:-0.546066 6:-0.472021 7:-0.365222 \n",
      "12 0:1 1:0.432432 2:0.478992 3:-0.743363 4:-0.254826 5:-0.505044 6:-0.407505 7:-0.285501 \n",
      "16 1:0.310811 2:0.310924 3:-0.654867 4:-0.306534 5:-0.591123 6:-0.410138 7:-0.335326 \n",
      "17 0:1 1:0.243243 2:0.226891 3:-0.743363 4:-0.345493 5:-0.466039 6:-0.483871 7:-0.504733 \n",
      "10 1:-0.162162 2:-0.12605 3:-0.80531 4:-0.798831 5:-0.836584 6:-0.834101 7:-0.836572 \n",
      "13 1:-0.148649 2:-0.176471 3:-0.823009 4:-0.813706 5:-0.856759 6:-0.843318 7:-0.830593 \n",
      "11 0:1 1:0.0675676 2:-0.0252101 3:-0.79646 4:-0.655392 5:-0.731675 6:-0.716919 7:-0.672147 \n",
      "12 0:1 1:0.189189 2:0.12605 3:-0.752212 4:-0.607933 5:-0.732347 6:-0.702436 7:-0.557549 \n",
      "13 0:1 1:-0.0540541 2:-0.0252101 3:-0.778761 4:-0.700372 5:-0.786147 6:-0.791968 7:-0.696064 \n",
      "9 0:-1 1:-0.27027 2:-0.277311 3:-0.840708 4:-0.86329 5:-0.896436 6:-0.881501 7:-0.885401 \n",
      "10 0:1 1:0.108108 2:0.0588235 3:-0.769912 4:-0.676997 5:-0.757902 6:-0.703752 7:-0.731938 \n",
      "10 0:-1 1:-0.202703 2:-0.226891 3:-0.823009 4:-0.839915 5:-0.910558 6:-0.823568 7:-0.841555 \n",
      "12 0:-1 1:-0.256757 2:-0.294118 3:-0.840708 4:-0.875686 5:-0.924008 6:-0.890718 7:-0.867464 \n",
      "12 1:-0.0135135 2:-0.0252101 3:-0.699115 4:-0.71206 5:-0.799597 6:-0.784068 7:-0.701046 \n",
      "4 0:-1 1:-0.675676 2:-0.697479 3:-0.911504 4:-0.97875 5:-0.987895 6:-0.98025 7:-0.979073 \n",
      "9 0:-1 1:-0.324324 2:-0.378151 3:-0.867257 4:-0.891624 5:-0.937458 6:-0.907834 7:-0.883408 \n",
      "18 0:1 1:0.135135 2:0.0588235 3:-0.778761 4:-0.663184 5:-0.752522 6:-0.815668 7:-0.666168 \n",
      "15 0:1 1:0.0135135 2:-0.00840336 3:-0.743363 4:-0.629538 5:-0.720915 6:-0.737986 7:-0.673144 \n",
      "13 0:-1 1:-0.0810811 2:-0.0252101 3:-0.761062 4:-0.727643 5:-0.829186 6:-0.816985 7:-0.708022 \n",
      "12 1:0.0675676 2:0.00840336 3:-0.752212 4:-0.694705 5:-0.796234 6:-0.751152 7:-0.70005 \n",
      "7 0:-1 1:-0.337838 2:-0.378151 3:-0.849558 4:-0.880999 5:-0.913248 6:-0.877551 7:-0.905331 \n",
      "9 0:-1 1:-0.364865 2:-0.428571 3:-0.867257 4:-0.909687 5:-0.940148 6:-0.913101 7:-0.915296 \n",
      "7 0:-1 1:-0.567568 2:-0.613445 3:-0.902655 4:-0.964937 5:-0.977135 6:-0.973667 7:-0.964126 \n",
      "10 0:-1 1:-0.27027 2:-0.327731 3:-0.858407 4:-0.881707 5:-0.920646 6:-0.889401 7:-0.895366 \n",
      "18 0:1 1:0.108108 2:0.092437 3:-0.752212 4:-0.524703 5:-0.70881 6:-0.658986 7:-0.61435 \n",
      "12 1:0.148649 2:0.109244 3:-0.79646 4:-0.520808 5:-0.605245 6:-0.63792 7:-0.61435 \n",
      "17 1:0.148649 2:0.109244 3:-0.814159 4:-0.648663 5:-0.759919 6:-0.712969 7:-0.664175 \n",
      "15 0:1 1:0.0540541 2:0.0252101 3:-0.814159 4:-0.648663 5:-0.713517 6:-0.695853 7:-0.723966 \n",
      "12 1:0.216216 2:0.176471 3:-0.716814 4:-0.535328 5:-0.64425 6:-0.705069 7:-0.554559 \n",
      "10 1:-0.0540541 2:-0.0588235 3:-0.831858 4:-0.773331 5:-0.839274 6:-0.840685 7:-0.753861 \n",
      "8 1:-0.175676 2:-0.159664 3:-0.831858 4:-0.802019 5:-0.832549 6:-0.863068 7:-0.823617 \n",
      "21 0:1 1:0.22973 2:0.210084 3:-0.743363 4:-0.332743 5:-0.484196 6:-0.514154 7:-0.474838 \n",
      "10 0:-1 1:-0.283784 2:-0.294118 3:-0.849558 4:-0.871436 5:-0.897781 6:-0.880184 7:-0.873443 \n",
      "9 0:1 1:0.0810811 2:0.0420168 3:-0.79646 4:-0.65433 5:-0.701412 6:-0.676103 7:-0.736921 \n",
      "7 1:-0.0405405 2:-0.0420168 3:-0.787611 4:-0.724455 5:-0.792199 6:-0.751152 7:-0.72297 \n",
      "10 0:-1 1:0.0405405 2:0.0420168 3:-0.778761 4:-0.670622 5:-0.746469 6:-0.752469 7:-0.688092 \n",
      "9 0:1 1:0.0675676 2:0.0252101 3:-0.769912 4:-0.631309 5:-0.735037 6:-0.720869 7:-0.674141 \n",
      "9 0:-1 1:-0.22973 2:-0.193277 3:-0.823009 4:-0.852311 5:-0.912576 6:-0.863068 7:-0.853513 \n",
      "8 0:-1 1:-0.243243 2:-0.294118 3:-0.840708 4:-0.882415 5:-0.934095 6:-0.893351 7:-0.877429 \n",
      "7 0:-1 1:-0.175676 2:-0.394958 3:-0.823009 4:-0.818665 5:-0.859449 6:-0.859118 7:-0.843548 \n",
      "6 0:-1 1:-0.243243 2:-0.310924 3:-0.849558 4:-0.866478 5:-0.892401 6:-0.873601 7:-0.893373 \n",
      "14 0:1 1:-0.0135135 2:-0.0252101 3:-0.787611 4:-0.656455 5:-0.737054 6:-0.716919 7:-0.684106 \n",
      "13 1:0.175676 2:0.159664 3:-0.769912 4:-0.595183 5:-0.706792 6:-0.64187 7:-0.61435 \n",
      "7 0:-1 1:-0.324324 2:-0.378151 3:-0.849558 4:-0.878874 5:-0.894418 6:-0.901251 7:-0.903338 \n",
      "13 0:1 1:0.472973 2:0.445378 3:-0.681416 4:-0.166637 5:-0.372562 6:-0.302172 7:-0.22571 \n",
      "19 1:0.391892 2:0.327731 3:-0.716814 4:-0.36391 5:-0.519839 6:-0.59052 7:-0.375187 \n",
      "7 0:-1 1:-0.310811 2:-0.327731 3:-0.831858 4:-0.868603 5:-0.902488 6:-0.882818 7:-0.883408 \n",
      "9 0:-1 1:0.0135135 2:-0.0420168 3:-0.769912 4:-0.738268 5:-0.785474 6:-0.791968 7:-0.793722 \n",
      "11 0:1 2:-0.0756303 3:-0.787611 4:-0.755622 5:-0.839946 6:-0.780118 7:-0.793722 \n",
      "6 0:-1 1:-0.310811 2:-0.462185 3:-0.867257 4:-0.920312 5:-0.94082 6:-0.931534 7:-0.933234 \n",
      "9 0:-1 1:0.0945946 2:0.0756303 3:-0.743363 4:-0.451036 5:-0.710827 6:-0.658986 7:-0.664175 \n",
      "11 0:1 1:0.0405405 2:-0.00840336 3:-0.787611 4:-0.655392 5:-0.741762 6:-0.724819 7:-0.694071 \n",
      "9 1:0.0810811 2:0.0252101 3:-0.778761 4:-0.684788 5:-0.773369 6:-0.788018 7:-0.723966 \n",
      "5 0:-1 1:-0.513514 2:-0.579832 3:-0.884956 4:-0.945458 5:-0.955615 6:-0.96445 7:-0.953164 \n",
      "6 0:1 1:-0.297297 2:-0.361345 3:-0.840708 4:-0.883478 5:-0.921318 6:-0.895984 7:-0.883408 \n",
      "11 0:1 1:0.0675676 2:-0.00840336 3:-0.769912 4:-0.67133 5:-0.753194 6:-0.740619 7:-0.714001 \n",
      "7 0:-1 1:-0.364865 2:-0.428571 3:-0.858407 4:-0.906145 5:-0.928716 6:-0.938117 7:-0.903338 \n",
      "10 1:-0.202703 2:-0.243697 3:-0.80531 4:-0.838144 5:-0.874243 6:-0.878868 7:-0.853513 \n",
      "7 0:-1 1:-0.405405 2:-0.462185 3:-0.867257 4:-0.910041 5:-0.934095 6:-0.923634 7:-0.923269 \n",
      "17 1:0.297297 2:0.277311 3:-0.707965 4:-0.314326 5:-0.549428 6:-0.391705 7:-0.415047 \n",
      "16 1:0.459459 2:0.546218 3:-0.699115 4:-0.193908 5:-0.422327 6:-0.410138 7:-0.165919 \n",
      "18 0:1 1:0.364865 2:0.462185 3:-0.654867 4:-0.0688861 5:-0.287828 6:-0.332456 7:-0.18585 \n",
      "11 1:0.378378 2:0.411765 3:-0.672566 4:-0.322472 5:-0.443847 6:-0.576037 7:-0.345291 \n",
      "18 0:1 1:0.554054 2:0.579832 3:-0.681416 4:0.150345 5:-0.198386 6:-0.0941409 7:0.0533134 \n",
      "11 0:1 1:0.243243 2:0.327731 3:-0.699115 4:-0.448203 5:-0.590451 6:-0.591837 7:-0.415047 \n",
      "10 1:-0.0810811 2:-0.0420168 3:-0.769912 4:-0.741101 5:-0.804976 6:-0.768269 7:-0.763827 \n",
      "12 1:-0.175676 2:-0.159664 3:-0.814159 4:-0.802373 5:-0.860794 6:-0.839368 7:-0.823617 \n",
      "19 0:1 1:0.0135135 2:0.00840336 3:-0.787611 4:-0.70958 5:-0.847344 6:-0.826201 7:-0.684106 \n",
      "10 1:-0.135135 2:-0.193277 3:-0.831858 4:-0.842394 5:-0.896436 6:-0.859118 7:-0.843548 \n",
      "11 0:-1 1:0.027027 2:-0.00840336 3:-0.787611 4:-0.658934 5:-0.757229 6:-0.62212 7:-0.684106 \n",
      "15 1:0.108108 2:0.092437 3:-0.734513 4:-0.572871 5:-0.7115 6:-0.632653 7:-0.644245 \n",
      "13 0:-1 1:0.283784 2:0.243697 3:-0.725664 4:-0.351514 5:-0.62811 6:-0.361422 7:-0.335326 \n",
      "13 1:0.0135135 2:-0.00840336 3:-0.743363 4:-0.617142 5:-0.763954 6:-0.677419 7:-0.65421 \n",
      "14 0:-1 1:0.0810811 2:0.109244 3:-0.743363 4:-0.564016 5:-0.685272 6:-0.716919 7:-0.574489 \n",
      "17 1:0.148649 2:0.092437 3:-0.725664 4:-0.537454 5:-0.677875 6:-0.624753 7:-0.59442 \n",
      "19 1:0.22973 2:0.193277 3:-0.707965 4:-0.426598 5:-0.678547 6:-0.556287 7:-0.524664 \n",
      "21 0:-1 1:0.121622 2:0.12605 3:-0.734513 4:-0.595537 5:-0.698722 6:-0.674786 7:-0.664175 \n",
      "23 1:0.121622 2:0.109244 3:-0.734513 4:-0.444307 5:-0.677202 6:-0.632653 7:-0.524664 \n",
      "22 1:0.202703 2:0.142857 3:-0.681416 4:-0.548079 5:-0.788837 6:-0.711652 7:-0.514699 \n",
      "12 0:-1 1:0.256757 2:0.210084 3:-0.743363 4:-0.477245 5:-0.64694 6:-0.55892 7:-0.514699 \n",
      "11 1:0.148649 2:0.0756303 3:-0.79646 4:-0.580308 5:-0.752522 6:-0.611587 7:-0.624315 \n",
      "23 1:0.0135135 2:0.092437 3:-0.707965 4:-0.423056 5:-0.665098 6:-0.497038 7:-0.474838 \n",
      "8 1:-0.202703 2:-0.260504 3:-0.823009 4:-0.843811 5:-0.876261 6:-0.932851 7:-0.843548 \n",
      "7 0:1 1:-0.202703 2:-0.260504 3:-0.823009 4:-0.838852 5:-0.882313 6:-0.878868 7:-0.863478 \n",
      "10 0:-1 1:0.108108 2:0.0588235 3:-0.752212 4:-0.595892 5:-0.726967 6:-0.628703 7:-0.65421 \n",
      "7 1:-0.027027 2:-0.092437 3:-0.79646 4:-0.724101 5:-0.794217 6:-0.753785 7:-0.763827 \n",
      "16 0:-1 1:0.243243 2:0.176471 3:-0.672566 4:-0.410306 5:-0.574311 6:-0.54707 7:-0.425012 \n",
      "10 0:-1 1:0.175676 2:0.159664 3:-0.752212 4:-0.539933 5:-0.67115 6:-0.56287 7:-0.63428 \n",
      "15 0:-1 1:0.324324 2:0.294118 3:-0.672566 4:-0.357535 5:-0.538668 6:-0.389072 7:-0.494768 \n",
      "13 1:0.243243 2:0.159664 3:-0.734513 4:-0.431557 5:-0.551446 6:-0.441738 7:-0.584454 \n",
      "16 1:0.243243 2:0.176471 3:-0.778761 4:-0.344785 5:-0.651648 6:-0.62607 7:-0.315396 \n",
      "11 0:-1 1:0.216216 2:0.159664 3:-0.699115 4:-0.483974 5:-0.626093 6:-0.460171 7:-0.61435 \n",
      "11 0:-1 1:0.391892 2:0.294118 3:-0.734513 4:-0.324597 5:-0.509079 6:-0.362739 7:-0.415047 \n",
      "10 0:-1 1:0.148649 2:0.0756303 3:-0.734513 4:-0.550912 5:-0.66039 6:-0.619487 7:-0.624315 \n",
      "5 0:1 1:-0.513514 2:-0.546218 3:-0.867257 4:-0.940145 5:-0.954943 6:-0.947334 7:-0.953164 \n",
      "11 1:-0.0405405 2:-0.092437 3:-0.79646 4:-0.727643 5:-0.802959 6:-0.720869 7:-0.783757 \n",
      "9 0:-1 1:-0.175676 2:-0.210084 3:-0.787611 4:-0.800956 5:-0.843309 6:-0.828835 7:-0.833582 \n",
      "4 0:1 1:-0.756757 2:-0.815126 3:-0.964602 4:-0.987958 5:-0.992603 6:-0.994733 7:-0.993024 \n",
      "7 0:1 1:-0.351351 2:-0.411765 3:-0.840708 4:-0.910395 5:-0.94351 6:-0.895984 7:-0.923269 \n",
      "4 0:1 1:-0.783784 2:-0.831933 3:-0.911504 4:-0.989021 5:-0.99462 6:-0.9921 7:-0.993024 \n",
      "5 0:-1 1:-0.445946 2:-0.495798 3:-0.823009 4:-0.918895 5:-0.928043 6:-0.926267 7:-0.943199 \n",
      "13 1:-0.0405405 2:-0.0588235 3:-0.787611 4:-0.686913 5:-0.792872 6:-0.699803 7:-0.723966 \n",
      "16 1:-0.135135 2:-0.12605 3:-0.814159 4:-0.752435 5:-0.841964 6:-0.761685 7:-0.764823 \n",
      "12 0:-1 1:-0.162162 2:-0.226891 3:-0.814159 4:-0.795644 5:-0.837929 6:-0.820935 7:-0.8286 \n",
      "14 1:0.0945946 2:0.109244 3:-0.761062 4:-0.621746 5:-0.746469 6:-0.627386 7:-0.658196 \n",
      "9 2:-0.0756303 3:-0.814159 4:-0.680893 5:-0.759247 6:-0.730086 7:-0.757848 \n",
      "12 0:-1 1:-0.135135 2:-0.193277 3:-0.79646 4:-0.777581 5:-0.839274 6:-0.844635 7:-0.78276 \n",
      "9 0:-1 1:-0.121622 2:-0.176471 3:-0.778761 4:-0.706039 5:-0.744452 6:-0.764319 7:-0.768809 \n",
      "10 0:-1 1:-0.0810811 2:-0.092437 3:-0.752212 4:-0.706039 5:-0.794889 6:-0.734036 7:-0.715994 \n",
      "11 0:-1 1:-0.351351 2:-0.344538 3:-0.840708 4:-0.857624 5:-0.918628 6:-0.903884 7:-0.844544 \n",
      "10 1:-0.27027 2:-0.310924 3:-0.840708 4:-0.85479 5:-0.897108 6:-0.886768 7:-0.850523 \n",
      "10 0:-1 1:-0.22973 2:-0.193277 3:-0.769912 4:-0.80556 5:-0.880968 6:-0.851218 7:-0.802691 \n",
      "7 0:1 1:-0.405405 2:-0.428571 3:-0.840708 4:-0.923145 5:-0.946873 6:-0.936801 7:-0.939213 \n",
      "7 0:1 1:-0.324324 2:-0.344538 3:-0.858407 4:-0.876749 5:-0.921318 6:-0.907834 7:-0.877429 \n",
      "11 0:-1 1:-0.189189 2:-0.176471 3:-0.823009 4:-0.826811 5:-0.861466 6:-0.876234 7:-0.837569 \n",
      "7 0:1 1:-0.445946 2:-0.495798 3:-0.902655 4:-0.92102 5:-0.94082 6:-0.928901 7:-0.936223 \n",
      "8 0:-1 1:-0.243243 2:-0.294118 3:-0.849558 4:-0.85904 5:-0.908541 6:-0.861751 7:-0.864474 \n",
      "9 0:-1 1:-0.256757 2:-0.327731 3:-0.858407 4:-0.865769 5:-0.893746 6:-0.899934 7:-0.877429 \n",
      "7 0:1 1:-0.459459 2:-0.512605 3:-0.884956 4:-0.928103 5:-0.937458 6:-0.947334 7:-0.943199 \n",
      "8 0:1 1:-0.418919 2:-0.495798 3:-0.876106 4:-0.932353 5:-0.952925 6:-0.951284 7:-0.933234 \n",
      "6 0:1 1:-0.527027 2:-0.546218 3:-0.893805 4:-0.947229 5:-0.952925 6:-0.971034 7:-0.95416 \n",
      "4 0:1 1:-0.716216 2:-0.764706 3:-0.938053 4:-0.982646 5:-0.988568 6:-0.986833 7:-0.986049 \n",
      "2 0:1 1:-0.797297 2:-0.848739 3:-0.955752 4:-0.990792 5:-0.995293 6:-0.990783 7:-0.993024 \n",
      "3 0:1 1:-0.77027 2:-0.815126 3:-0.955752 4:-0.988667 5:-0.992603 6:-0.986833 7:-0.993024 \n",
      "13 0:-1 1:0.297297 2:0.344538 3:-0.716814 4:-0.252346 5:-0.473436 6:-0.400922 7:-0.419033 \n",
      "15 0:-1 1:0.297297 2:0.294118 3:-0.734513 4:-0.227909 5:-0.441829 6:-0.443055 7:-0.125062 \n",
      "15 0:-1 1:0.216216 2:0.193277 3:-0.769912 4:-0.300159 5:-0.481506 6:-0.361422 7:-0.415047 \n",
      "11 0:-1 1:0.0540541 2:0.0252101 3:-0.858407 4:-0.655746 5:-0.744452 6:-0.672153 7:-0.694071 \n",
      "17 1:0.121622 2:0.0252101 3:-0.80531 4:-0.646892 5:-0.784802 6:-0.719552 7:-0.61435 \n",
      "10 0:-1 1:-0.121622 2:-0.159664 3:-0.849558 4:-0.791039 5:-0.856086 6:-0.815668 7:-0.803687 \n",
      "12 1:0.0945946 2:0.0756303 3:-0.814159 4:-0.629538 5:-0.707465 6:-0.686636 7:-0.694071 \n",
      "13 0:-1 1:0.162162 2:0.159664 3:-0.778761 4:-0.455994 5:-0.63349 6:-0.58262 7:-0.494768 \n",
      "15 1:0.202703 2:0.159664 3:-0.787611 4:-0.539933 5:-0.650303 6:-0.470704 7:-0.674141 \n",
      "11 0:-1 1:0.216216 2:0.159664 3:-0.769912 4:-0.413848 5:-0.677875 6:-0.520737 7:-0.454908 \n",
      "13 0:-1 1:0.27027 2:0.226891 3:-0.769912 4:-0.378785 5:-0.498319 6:-0.55497 7:-0.544594 \n",
      "15 0:-1 1:0.202703 2:0.159664 3:-0.787611 4:-0.418452 5:-0.600538 6:-0.526004 7:-0.474838 \n",
      "13 0:-1 1:0.162162 2:0.092437 3:-0.769912 4:-0.536745 5:-0.696032 6:-0.531271 7:-0.564524 \n",
      "18 0:-1 1:0.216216 2:0.243697 3:-0.787611 4:-0.387639 5:-0.621385 6:-0.537854 7:-0.425012 \n",
      "10 0:-1 1:0.175676 2:0.12605 3:-0.778761 4:-0.536391 5:-0.648958 6:-0.518104 7:-0.65421 \n",
      "12 0:-1 1:0.202703 2:0.109244 3:-0.79646 4:-0.527537 5:-0.680565 6:-0.548387 7:-0.59442 \n",
      "12 1:0.202703 2:0.176471 3:-0.778761 4:-0.545599 5:-0.67653 6:-0.544437 7:-0.584454 \n",
      "14 0:-1 1:0.243243 2:0.193277 3:-0.761062 4:-0.390827 5:-0.61735 6:-0.599737 7:-0.365222 \n",
      "10 0:-1 2:-0.0252101 3:-0.840708 4:-0.732601 5:-0.809011 6:-0.806452 7:-0.753861 \n",
      "14 0:-1 1:0.22973 2:0.294118 3:-0.637168 4:-0.409952 5:-0.570948 6:-0.428571 7:-0.514699 \n",
      "8 1:-0.22973 2:-0.294118 3:-0.840708 4:-0.855144 5:-0.896436 6:-0.851218 7:-0.883408 \n",
      "14 1:0.243243 2:0.226891 3:-0.734513 4:-0.479724 5:-0.626765 6:-0.511521 7:-0.574489 \n",
      "17 1:0.202703 2:0.176471 3:-0.752212 4:-0.422348 5:-0.62542 6:-0.519421 7:-0.484803 \n",
      "20 0:-1 1:0.22973 2:0.210084 3:-0.769912 4:-0.40464 5:-0.631473 6:-0.489138 7:-0.474838 \n",
      "17 1:0.22973 2:0.226891 3:-0.769912 4:-0.292368 5:-0.544048 6:-0.406188 7:-0.474838 \n",
      "17 1:0.581081 2:0.563025 3:-0.646018 4:0.185762 5:-0.0961668 6:0.263989 7:-0.106129 \n",
      "9 0:-1 1:0.202703 2:0.109244 3:-0.752212 4:-0.534266 5:-0.667115 6:-0.465438 7:-0.684106 \n",
      "14 0:-1 1:0.243243 2:0.226891 3:-0.769912 4:-0.430848 5:-0.596503 6:-0.524687 7:-0.444943 \n",
      "15 0:-1 1:0.675676 2:0.546218 3:-0.690265 4:0.072782 5:-0.223941 6:0.0796577 7:-0.22571 \n",
      "13 1:0.175676 2:0.12605 3:-0.814159 4:-0.567912 5:-0.749832 6:-0.60632 7:-0.61435 \n",
      "10 0:-1 1:0.108108 2:0.00840336 3:-0.787611 4:-0.613954 5:-0.712172 6:-0.576037 7:-0.723966 \n",
      "13 1:0.432432 2:0.361345 3:-0.699115 4:-0.206658 5:-0.534633 6:-0.199473 7:-0.375187 \n",
      "13 1:0.364865 2:0.344538 3:-0.707965 4:-0.196387 5:-0.505044 6:-0.210007 7:-0.454908 \n",
      "20 0:-1 1:0.554054 2:0.546218 3:-0.690265 4:0.0472817 5:-0.289173 6:-0.285056 7:0.0433483 \n",
      "13 0:-1 1:0.472973 2:0.512605 3:-0.672566 4:0.0805738 5:-0.073302 6:-0.0322581 7:-0.305431 \n",
      "20 0:-1 1:0.459459 2:0.579832 3:-0.725664 4:-0.195679 5:-0.507734 6:-0.379855 7:-0.265571 \n",
      "15 1:0.432432 2:0.478992 3:-0.663717 4:0.0164689 5:-0.370545 6:-0.302172 7:-0.18585 \n",
      "13 0:-1 1:0.337838 2:0.294118 3:-0.725664 4:-0.210908 5:-0.359112 6:-0.391705 7:-0.464873 \n",
      "15 0:-1 1:0.337838 2:0.260504 3:-0.787611 4:-0.249513 5:-0.533289 6:-0.561554 7:-0.385152 \n",
      "15 0:-1 1:0.378378 2:0.176471 3:-0.734513 4:-0.111387 5:-0.416274 6:-0.469388 7:-0.355257 \n",
      "16 1:0.283784 2:0.294118 3:-0.725664 4:-0.331326 5:-0.580363 6:-0.520737 7:-0.335326 \n",
      "17 1:0.256757 2:0.294118 3:-0.761062 4:-0.322118 5:-0.680565 6:-0.419355 7:-0.405082 \n",
      "14 0:-1 1:0.527027 2:0.529412 3:-0.663717 4:0.141137 5:-0.165434 6:-0.0506912 7:-0.0662681 \n",
      "14 1:0.445946 2:0.394958 3:-0.743363 4:-0.1847 5:-0.459314 6:-0.221856 7:-0.365222 \n",
      "11 0:-1 1:0.27027 2:0.327731 3:-0.734513 4:-0.308659 5:-0.548756 6:-0.499671 7:-0.405082 \n",
      "13 1:0.391892 2:0.310924 3:-0.769912 4:-0.199221 5:-0.486886 6:-0.385122 7:-0.365222 \n",
      "9 0:-1 1:-0.27027 2:-0.277311 3:-0.831858 4:-0.861874 5:-0.911903 6:-0.869651 7:-0.863478 \n",
      "11 1:0.283784 2:0.260504 3:-0.725664 4:-0.445369 5:-0.612643 6:-0.403555 7:-0.538615 \n",
      "17 1:0.22973 2:0.243697 3:-0.699115 4:-0.329201 5:-0.532616 6:-0.370639 7:-0.448929 \n",
      "11 1:0.22973 2:0.344538 3:-0.707965 4:-0.306889 5:-0.577001 6:-0.260039 7:-0.412058 \n",
      "8 0:1 1:0.108108 2:0.0756303 3:-0.752212 4:-0.632371 5:-0.732347 6:-0.677419 7:-0.664175 \n",
      "8 0:-1 1:-0.162162 2:-0.260504 3:-0.79646 4:-0.811227 5:-0.870208 6:-0.784068 7:-0.833582 \n",
      "9 0:-1 1:0.027027 2:-0.0420168 3:-0.761062 4:-0.674163 5:-0.776059 6:-0.585253 7:-0.763827 \n",
      "10 0:-1 1:0.121622 2:0.092437 3:-0.752212 4:-0.462724 5:-0.671822 6:-0.561554 7:-0.63428 \n",
      "11 0:-1 1:0.22973 2:0.193277 3:-0.707965 4:-0.482911 5:-0.747142 6:-0.553654 7:-0.385152 \n",
      "15 0:-1 1:0.162162 2:0.109244 3:-0.743363 4:-0.521516 5:-0.683927 6:-0.529954 7:-0.604385 \n",
      "13 0:-1 1:0.121622 2:0.092437 3:-0.752212 4:-0.549141 5:-0.691325 6:-0.627386 7:-0.61435 \n",
      "15 0:-1 1:0.0540541 2:-0.00840336 3:-0.752212 4:-0.593767 5:-0.73033 6:-0.605003 7:-0.624315 \n",
      "15 1:0.0675676 2:0.0252101 3:-0.743363 4:-0.621038 5:-0.769334 6:-0.639236 7:-0.61435 \n",
      "13 0:-1 1:0.310811 2:0.193277 3:-0.707965 4:-0.34266 5:-0.529926 6:-0.377222 7:-0.405082 \n",
      "12 0:-1 1:0.162162 2:0.109244 3:-0.734513 4:-0.547016 5:-0.670477 6:-0.601053 7:-0.574489 \n",
      "10 0:-1 1:0.189189 2:0.277311 3:-0.743363 4:-0.377014 5:-0.608608 6:-0.458855 7:-0.494768 \n",
      "6 0:1 1:-0.162162 2:-0.243697 3:-0.778761 4:-0.828582 5:-0.864156 6:-0.901251 7:-0.833582 \n",
      "5 0:1 1:-0.621622 2:-0.663866 3:-0.893805 4:-0.964229 5:-0.9731 6:-0.957867 7:-0.973094 \n",
      "15 0:-1 1:0.283784 2:0.210084 3:-0.690265 4:-0.263326 5:-0.558171 6:-0.389072 7:-0.424016 \n",
      "9 1:0.189189 2:0.12605 3:-0.769912 4:-0.593767 5:-0.735709 6:-0.658986 7:-0.635277 \n",
      "12 0:-1 1:0.135135 2:0.109244 3:-0.761062 4:-0.499203 5:-0.717552 6:-0.639236 7:-0.480817 \n",
      "9 1:0.162162 2:0.12605 3:-0.716814 4:-0.545245 5:-0.66846 6:-0.468071 7:-0.67713 \n",
      "12 1:0.418919 2:0.378151 3:-0.707965 4:-0.372764 5:-0.585743 6:-0.353522 7:-0.480817 \n",
      "10 1:0.337838 2:0.378151 3:-0.716814 4:-0.368514 5:-0.578346 6:-0.323239 7:-0.478824 \n",
      "10 1:0.108108 2:0.0756303 3:-0.761062 4:-0.607579 5:-0.742434 6:-0.655036 7:-0.667165 \n",
      "9 0:-1 1:0.0675676 2:0.0588235 3:-0.681416 4:-0.640163 5:-0.743779 6:-0.662936 7:-0.679123 \n",
      "12 0:-1 1:0.351351 2:0.327731 3:-0.707965 4:-0.348681 5:-0.560861 6:-0.408822 7:-0.492775 \n",
      "12 0:-1 1:0.364865 2:0.378151 3:-0.716814 4:-0.268638 5:-0.577673 6:-0.316656 7:-0.278525 \n",
      "15 0:-1 1:0.189189 2:0.176471 3:-0.743363 4:-0.50912 5:-0.712172 6:-0.57077 7:-0.536622 \n",
      "11 0:-1 1:0.22973 2:0.193277 3:-0.725664 4:-0.494599 5:-0.624075 6:-0.557604 7:-0.576482 \n",
      "9 0:-1 1:-0.0135135 2:-0.0588235 3:-0.80531 4:-0.72233 5:-0.790182 6:-0.748519 7:-0.759841 \n",
      "11 0:-1 1:0.202703 2:0.226891 3:-0.716814 4:-0.473703 5:-0.658373 6:-0.587887 7:-0.428002 \n",
      "11 1:-0.0540541 2:-0.0252101 3:-0.80531 4:-0.74181 5:-0.833221 6:-0.788018 7:-0.769806 \n",
      "10 0:-1 1:0.0405405 2:-0.0420168 3:-0.761062 4:-0.650788 5:-0.778749 6:-0.693219 7:-0.63428 \n",
      "10 0:-1 1:0.0135135 2:-0.0588235 3:-0.778761 4:-0.754206 5:-0.841291 6:-0.723502 7:-0.773792 \n",
      "9 0:-1 1:-0.0540541 2:-0.0756303 3:-0.769912 4:-0.689393 5:-0.796907 6:-0.755102 7:-0.694071 \n",
      "7 0:1 1:-0.202703 2:-0.260504 3:-0.823009 4:-0.845582 5:-0.874916 6:-0.882818 7:-0.873443 \n",
      "10 0:-1 1:0.189189 2:0.092437 3:-0.761062 4:-0.532849 5:-0.61466 6:-0.449638 7:-0.694071 \n",
      "7 0:-1 1:-0.108108 2:-0.159664 3:-0.787611 4:-0.77581 5:-0.835239 6:-0.763002 7:-0.813652 \n",
      "6 0:1 1:-0.445946 2:-0.495798 3:-0.876106 4:-0.92952 5:-0.9462 6:-0.922317 7:-0.943199 \n",
      "15 1:0.0945946 2:0.159664 3:-0.778761 4:-0.463786 5:-0.715535 6:-0.529954 7:-0.524664 \n",
      "10 1:-0.0135135 2:-0.0420168 3:-0.769912 4:-0.704268 5:-0.795562 6:-0.697169 7:-0.743896 \n",
      "12 1:0.202703 2:0.193277 3:-0.79646 4:-0.429786 5:-0.61735 6:-0.529954 7:-0.534629 \n",
      "12 0:-1 1:0.162162 2:0.176471 3:-0.752212 4:-0.381619 5:-0.642905 6:-0.54312 7:-0.434978 \n",
      "21 1:0.121622 2:0.0420168 3:-0.769912 4:-0.517266 5:-0.779422 6:-0.655036 7:-0.59442 \n",
      "6 0:1 1:-0.567568 2:-0.596639 3:-0.902655 4:-0.953958 5:-0.96503 6:-0.9684 7:-0.967115 \n",
      "5 0:1 1:-0.513514 2:-0.563025 3:-0.893805 4:-0.939083 5:-0.952253 6:-0.946017 7:-0.95715 \n",
      "7 0:1 1:-0.351351 2:-0.378151 3:-0.849558 4:-0.879936 5:-0.905851 6:-0.910467 7:-0.896363 \n",
      "7 0:1 1:-0.324324 2:-0.344538 3:-0.858407 4:-0.87852 5:-0.898453 6:-0.910467 7:-0.905331 \n",
      "6 0:1 1:-0.297297 2:-0.344538 3:-0.858407 4:-0.87179 5:-0.902488 6:-0.895984 7:-0.888391 \n",
      "6 0:1 1:-0.256757 2:-0.277311 3:-0.840708 4:-0.855853 5:-0.900471 6:-0.849901 7:-0.879422 \n",
      "6 0:1 1:-0.256757 2:-0.344538 3:-0.876106 4:-0.873915 5:-0.913248 6:-0.874918 7:-0.895366 \n",
      "7 0:1 1:-0.22973 2:-0.176471 3:-0.849558 4:-0.810165 5:-0.841964 6:-0.832785 7:-0.854509 \n",
      "7 0:1 1:-0.216216 2:-0.260504 3:-0.761062 4:-0.831415 5:-0.856086 6:-0.884134 7:-0.856502 \n",
      "6 0:1 1:-0.202703 2:-0.260504 3:-0.752212 4:-0.844519 5:-0.870881 6:-0.881501 7:-0.880419 \n",
      "7 0:1 1:-0.175676 2:-0.260504 3:-0.831858 4:-0.90402 5:-0.885676 6:-0.847268 7:-0.882412 \n",
      "6 0:1 1:-0.162162 2:-0.210084 3:-0.831858 4:-0.780414 5:-0.809011 6:-0.834101 7:-0.831589 \n",
      "7 0:1 1:-0.162162 2:-0.176471 3:-0.823009 4:-0.796352 5:-0.837929 6:-0.835418 7:-0.823617 \n",
      "7 0:1 1:-0.135135 2:-0.210084 3:-0.831858 4:-0.775456 5:-0.815736 6:-0.790652 7:-0.839562 \n",
      "6 0:1 1:-0.135135 2:-0.210084 3:-0.831858 4:-0.786081 5:-0.830531 6:-0.780118 7:-0.849527 \n",
      "6 0:1 1:-0.121622 2:-0.142857 3:-0.823009 4:-0.784664 5:-0.826496 6:-0.843318 7:-0.815645 \n",
      "6 0:1 1:-0.0945946 2:-0.092437 3:-0.823009 4:-0.72233 5:-0.721587 6:-0.828835 7:-0.791729 \n",
      "7 0:1 1:-0.0810811 2:-0.109244 3:-0.80531 4:-0.736851 5:-0.765972 6:-0.802502 7:-0.78575 \n",
      "6 0:-1 1:-0.0810811 2:-0.159664 3:-0.823009 4:-0.771206 5:-0.791527 6:-0.868334 7:-0.821624 \n",
      "6 0:1 1:-0.0540541 2:-0.092437 3:-0.823009 4:-0.719497 5:-0.841964 6:-0.831468 7:-0.814649 \n",
      "9 0:1 1:-0.0135135 2:0.0420168 3:-0.79646 4:-0.646538 5:-0.67384 6:-0.780118 7:-0.711011 \n",
      "7 0:1 2:-0.0588235 3:-0.823009 4:-0.654684 5:-0.632145 6:-0.774852 7:-0.78276 \n",
      "7 0:1 2:-0.092437 3:-0.823009 4:-0.733664 5:-0.759919 6:-0.737986 7:-0.82561 \n",
      "8 0:1 1:0.0135135 2:-0.00840336 3:-0.769912 4:-0.613954 5:-0.671822 6:-0.631336 7:-0.72297 \n",
      "8 0:-1 1:0.0675676 2:0.0756303 3:-0.787611 4:-0.590225 5:-0.643578 6:-0.755102 7:-0.666168 \n",
      "9 0:1 1:0.0810811 2:0.0420168 3:-0.778761 4:-0.614308 5:-0.693342 6:-0.689269 7:-0.660189 \n",
      "8 1:0.0945946 2:0.0420168 3:-0.761062 4:-0.548433 5:-0.605245 6:-0.703752 7:-0.65421 \n",
      "9 0:1 1:0.108108 2:0.00840336 3:-0.814159 4:-0.648663 5:-0.70881 6:-0.748519 7:-0.699053 \n",
      "8 0:-1 1:0.121622 2:0.109244 3:-0.778761 4:-0.570037 5:-0.589106 6:-0.748519 7:-0.649228 \n",
      "9 1:0.135135 2:0.193277 3:-0.778761 4:-0.466265 5:-0.550101 6:-0.661619 7:-0.576482 \n",
      "9 0:-1 1:0.148649 2:0.159664 3:-0.778761 4:-0.578183 5:-0.638198 6:-0.665569 7:-0.672147 \n",
      "8 0:-1 1:0.162162 2:0.294118 3:-0.752212 4:-0.415265 5:-0.542031 6:-0.512837 7:-0.526657 \n",
      "8 0:-1 1:0.216216 2:0.142857 3:-0.769912 4:-0.460599 5:-0.547411 6:-0.62607 7:-0.554559 \n",
      "9 0:-1 1:0.256757 2:0.176471 3:-0.778761 4:-0.370285 5:-0.353732 6:-0.497038 7:-0.600399 \n",
      "11 1:0.256757 2:0.226891 3:-0.752212 4:-0.431202 5:-0.505044 6:-0.54707 7:-0.584454 \n",
      "10 1:0.27027 2:0.294118 3:-0.761062 4:-0.350806 5:-0.424344 6:-0.470704 7:-0.529646 \n",
      "8 1:0.283784 2:0.260504 3:-0.778761 4:-0.347618 5:-0.458642 6:-0.540487 7:-0.438964 \n",
      "10 0:-1 1:0.283784 2:0.327731 3:-0.734513 4:-0.282805 5:-0.453934 6:-0.470704 7:-0.429995 \n",
      "10 1:0.283784 2:0.327731 3:-0.734513 4:-0.381619 5:-0.514459 6:-0.539171 7:-0.451918 \n",
      "9 0:-1 1:0.297297 2:0.277311 3:-0.743363 4:-0.315389 5:-0.330868 6:-0.55892 7:-0.527653 \n",
      "10 0:-1 1:0.324324 2:0.327731 3:-0.725664 4:-0.25093 5:-0.364492 6:-0.369322 7:-0.474838 \n",
      "9 0:-1 1:0.337838 2:0.344538 3:-0.734513 4:-0.327076 5:-0.477471 6:-0.433838 7:-0.455904 \n",
      "10 0:-1 1:0.337838 2:0.277311 3:-0.769912 4:-0.467682 5:-0.531944 6:-0.539171 7:-0.616343 \n",
      "9 1:0.351351 2:0.378151 3:-0.752212 4:-0.322826 5:-0.406859 6:-0.523371 7:-0.463876 \n",
      "9 0:-1 1:0.391892 2:0.411765 3:-0.707965 4:-0.238534 5:-0.390047 6:-0.358789 7:-0.386148 \n",
      "12 0:-1 1:0.391892 2:0.361345 3:-0.769912 4:-0.220825 5:-0.389375 6:-0.460171 7:-0.345291 \n",
      "10 1:0.405405 2:0.428571 3:-0.734513 4:-0.215158 5:-0.33154 6:-0.400922 7:-0.345291 \n",
      "8 1:0.405405 2:0.428571 3:-0.716814 4:-0.144679 5:-0.298588 6:-0.221856 7:-0.365222 \n",
      "6 1:0.405405 2:0.411765 3:-0.716814 4:-0.193554 5:-0.265636 6:-0.393022 7:-0.46288 \n",
      "12 1:0.405405 2:0.378151 3:-0.752212 4:-0.213033 5:-0.305313 6:-0.358789 7:-0.395117 \n",
      "11 0:-1 1:0.418919 2:0.411765 3:-0.690265 4:-0.0490526 5:-0.262946 6:-0.24424 7:-0.285501 \n",
      "10 1:0.418919 2:0.411765 3:-0.725664 4:-0.144324 5:-0.123067 6:-0.55497 7:-0.364225 \n",
      "11 0:-1 1:0.418919 2:0.478992 3:-0.690265 4:-0.0876572 5:-0.186281 6:-0.274523 7:-0.316393 \n",
      "10 1:0.432432 2:0.411765 3:-0.690265 4:-0.0224898 5:-0.182246 6:-0.389072 7:-0.208769 \n",
      "9 0:-1 1:0.432432 2:0.344538 3:-0.716814 4:-0.219763 5:-0.435104 6:-0.207373 7:-0.355257 \n",
      "12 1:0.459459 2:0.495798 3:-0.690265 4:-0.0260315 5:-0.250168 6:-0.132324 7:-0.421026 \n",
      "10 1:0.459459 2:0.563025 3:-0.734513 4:-0.0497609 5:-0.155346 6:-0.315339 7:-0.315396 \n",
      "9 0:-1 1:0.459459 2:0.529412 3:-0.734513 4:-0.0834071 5:-0.268325 6:-0.128374 7:-0.365222 \n",
      "12 0:-1 1:0.459459 2:0.512605 3:-0.707965 4:-0.0522401 5:-0.286483 6:-0.260039 7:-0.18585 \n",
      "11 1:0.472973 2:0.512605 3:-0.716814 4:-0.0292191 5:-0.156019 6:-0.277156 7:-0.26856 \n",
      "9 0:-1 1:0.472973 2:0.495798 3:-0.707965 4:-0.0756154 5:-0.146604 6:-0.331139 7:-0.375187 \n",
      "11 1:0.486486 2:0.462185 3:-0.725664 4:-0.145387 5:-0.375925 6:-0.57472 7:-0.183857 \n",
      "11 1:0.486486 2:0.462185 3:-0.646018 4:-0.0221356 5:-0.208473 6:-0.250823 7:-0.243647 \n",
      "14 0:-1 1:0.5 2:0.512605 3:-0.707965 4:-0.108907 5:-0.392737 6:-0.275839 7:-0.193822 \n",
      "9 0:-1 1:0.513514 2:0.529412 3:-0.699115 4:-0.0412608 5:-0.168796 6:-0.198157 7:-0.22571 \n",
      "10 1:0.513514 2:0.495798 3:-0.734513 4:-0.0267399 5:-0.127774 6:-0.0506912 7:-0.385152 \n",
      "9 1:0.513514 2:0.445378 3:-0.707965 4:-0.0844696 5:-0.102892 6:-0.315339 7:-0.461883 \n",
      "8 1:0.527027 2:0.529412 3:-0.707965 4:0.0511776 5:0.0201748 6:-0.127057 7:-0.363229 \n",
      "11 0:-1 1:0.554054 2:0.579832 3:-0.690265 4:0.0409067 5:-0.0934768 6:-0.171824 7:-0.207773 \n",
      "9 0:-1 1:0.567568 2:0.563025 3:-0.707965 4:-0.00301045 5:-0.213181 6:-0.235023 7:-0.195815 \n",
      "17 0:-1 1:0.567568 2:0.764706 3:-0.637168 4:0.472286 5:0.2885 6:-0.102041 7:0.194818 \n",
      "15 0:-1 1:0.581081 2:0.596639 3:-0.699115 4:-0.0164689 5:-0.207128 6:-0.443055 7:-0.100149 \n",
      "10 0:-1 1:0.581081 2:0.563025 3:-0.663717 4:0.102178 5:0.0141224 6:-0.21659 7:-0.20578 \n",
      "10 1:0.608108 2:0.781513 3:-0.716814 4:-0.0741987 5:-0.268998 6:-0.225806 7:-0.17987 \n",
      "12 1:0.621622 2:0.579832 3:-0.699115 4:0.280326 5:0.0531271 6:0.0283081 7:-0.0961634 \n",
      "9 1:0.621622 2:0.579832 3:-0.725664 4:0.0458651 5:-0.156691 6:-0.104674 7:-0.165919 \n",
      "11 1:0.635135 2:0.697479 3:-0.654867 4:0.257659 5:0.156691 6:-0.15339 7:-0.175884 \n",
      "11 1:0.648649 2:0.630252 3:-0.716814 4:0.179741 5:0.119032 6:-0.00724161 7:-0.0563029 \n",
      "11 1:0.675676 2:0.697479 3:-0.610619 4:0.29768 5:0.135844 6:0.109941 7:-0.0961634 \n",
      "17 0:-1 1:0.77027 2:0.815126 3:-0.59292 4:1 5:0.540686 6:0.102041 7:0.784753 \n",
      "6 0:1 1:-0.648649 2:-0.714286 3:-0.911504 4:-0.968833 5:-0.979153 6:-0.969717 7:-0.976084 \n",
      "5 0:1 1:-0.554054 2:-0.596639 3:-0.902655 4:-0.951479 5:-0.96772 6:-0.96445 7:-0.961136 \n",
      "3 0:1 1:-0.554054 2:-0.596639 3:-0.884956 4:-0.954312 5:-0.95965 6:-0.965767 7:-0.969108 \n",
      "6 0:1 1:-0.513514 2:-0.546218 3:-0.911504 4:-0.942624 5:-0.961668 6:-0.9447 7:-0.949178 \n",
      "4 0:1 1:-0.513514 2:-0.579832 3:-0.902655 4:-0.942624 5:-0.95965 6:-0.9447 7:-0.963129 \n",
      "6 0:1 1:-0.486486 2:-0.529412 3:-0.893805 4:-0.936249 5:-0.954943 6:-0.935484 7:-0.95416 \n",
      "4 0:1 1:-0.445946 2:-0.781513 3:-0.867257 4:-0.918541 5:-0.940148 6:-0.924951 7:-0.93423 \n",
      "5 0:1 1:-0.405405 2:-0.411765 3:-0.858407 4:-0.886311 5:-0.913921 6:-0.869651 7:-0.926258 \n",
      "4 0:1 1:-0.391892 2:-0.394958 3:-0.858407 4:-0.908624 5:-0.934095 6:-0.931534 7:-0.917289 \n",
      "6 0:1 1:-0.391892 2:-0.411765 3:-0.831858 4:-0.903311 5:-0.926026 6:-0.905201 7:-0.929248 \n",
      "5 0:1 1:-0.378378 2:-0.445378 3:-0.876106 4:-0.901541 5:-0.917956 6:-0.919684 7:-0.929248 \n",
      "6 0:1 1:-0.351351 2:-0.394958 3:-0.867257 4:-0.896228 5:-0.922663 6:-0.902567 7:-0.918286 \n",
      "8 0:1 1:-0.351351 2:-0.411765 3:-0.876106 4:-0.899416 5:-0.930061 6:-0.921001 7:-0.923269 \n",
      "5 0:1 1:-0.337838 2:-0.378151 3:-0.840708 4:-0.889853 5:-0.907196 6:-0.931534 7:-0.918286 \n",
      "6 0:1 1:-0.324324 2:-0.378151 3:-0.867257 4:-0.868957 5:-0.890383 6:-0.884134 7:-0.903338 \n",
      "6 0:1 1:-0.310811 2:-0.294118 3:-0.849558 4:-0.862582 5:-0.897108 6:-0.921001 7:-0.9143 \n",
      "7 0:1 1:-0.297297 2:-0.344538 3:-0.867257 4:-0.872145 5:-0.906523 6:-0.885451 7:-0.893373 \n",
      "7 0:1 1:-0.297297 2:-0.344538 3:-0.867257 4:-0.869665 5:-0.874243 6:-0.901251 7:-0.9143 \n",
      "5 0:1 1:-0.283784 2:-0.344538 3:-0.867257 4:-0.874978 5:-0.911903 6:-0.881501 7:-0.913303 \n",
      "7 0:1 1:-0.283784 2:-0.344538 3:-0.876106 4:-0.843811 5:-0.861466 6:-0.889401 7:-0.893373 \n",
      "7 0:1 1:-0.27027 2:-0.294118 3:-0.823009 4:-0.827519 5:-0.852051 6:-0.860434 7:-0.873443 \n",
      "6 0:1 1:-0.202703 2:-0.210084 3:-0.831858 4:-0.82504 5:-0.860794 6:-0.848585 7:-0.869457 \n",
      "7 0:1 1:-0.202703 2:-0.243697 3:-0.831858 4:-0.798477 5:-0.798924 6:-0.819618 7:-0.844544 \n",
      "6 0:1 1:-0.189189 2:-0.243697 3:-0.840708 4:-0.849123 5:-0.888366 6:-0.843318 7:-0.893373 \n",
      "6 0:1 1:-0.162162 2:-0.294118 3:-0.858407 4:-0.823623 5:-0.834566 6:-0.903884 7:-0.863478 \n",
      "7 0:1 1:-0.0945946 2:-0.142857 3:-0.840708 4:-0.761289 5:-0.792872 6:-0.818302 7:-0.823617 \n",
      "7 0:1 1:-0.0945946 2:-0.159664 3:-0.840708 4:-0.751018 5:-0.790182 6:-0.805135 7:-0.803687 \n",
      "6 0:1 1:-0.0945946 2:-0.142857 3:-0.840708 4:-0.765185 5:-0.781439 6:-0.840685 7:-0.821624 \n",
      "6 0:1 1:-0.0810811 2:-0.0756303 3:-0.840708 4:-0.746768 5:-0.772697 6:-0.788018 7:-0.823617 \n",
      "5 0:1 1:-0.0675676 2:-0.109244 3:-0.79646 4:-0.735081 5:-0.774042 6:-0.759052 7:-0.803687 \n",
      "6 0:1 1:-0.0675676 2:-0.12605 3:-0.823009 4:-0.758102 5:-0.790182 6:-0.791968 7:-0.823617 \n",
      "6 0:1 1:-0.0540541 2:-0.0420168 3:-0.823009 4:-0.730831 5:-0.780767 6:-0.748519 7:-0.803687 \n",
      "6 0:1 1:-0.0540541 2:-0.12605 3:-0.823009 4:-0.734372 5:-0.780094 6:-0.811718 7:-0.793722 \n",
      "7 0:1 1:-0.0405405 2:-0.092437 3:-0.823009 4:-0.743226 5:-0.789509 6:-0.784068 7:-0.793722 \n",
      "7 0:1 1:-0.0405405 2:-0.092437 3:-0.840708 4:-0.700372 5:-0.709482 6:-0.772219 7:-0.813652 \n",
      "8 0:1 1:-0.027027 2:-0.092437 3:-0.787611 4:-0.718435 5:-0.757229 6:-0.840685 7:-0.778774 \n",
      "6 0:1 1:-0.027027 2:-0.0420168 3:-0.79646 4:-0.723393 5:-0.755884 6:-0.795918 7:-0.774788 \n",
      "6 0:1 1:-0.0135135 2:-0.0252101 3:-0.769912 4:-0.683018 5:-0.720242 6:-0.781435 7:-0.735924 \n",
      "7 0:1 1:-0.0135135 2:-0.092437 3:-0.840708 4:-0.753497 5:-0.802286 6:-0.824885 7:-0.793722 \n",
      "6 2:-0.0588235 3:-0.80531 4:-0.692934 5:-0.728985 6:-0.712969 7:-0.764823 \n",
      "7 0:1 2:-0.00840336 3:-0.769912 4:-0.704268 5:-0.773369 6:-0.752469 7:-0.764823 \n",
      "7 0:1 1:0.0135135 2:0.0252101 3:-0.769912 4:-0.66283 5:-0.744452 6:-0.666886 7:-0.729945 \n",
      "8 0:1 1:0.0135135 2:0.00840336 3:-0.814159 4:-0.686559 5:-0.736382 6:-0.756419 7:-0.736921 \n",
      "7 0:1 1:0.0135135 2:-0.0252101 3:-0.80531 4:-0.668497 5:-0.6846 6:-0.776169 7:-0.776781 \n",
      "7 0:1 1:0.0135135 2:-0.0588235 3:-0.814159 4:-0.684788 5:-0.68729 6:-0.59842 7:-0.76582 \n",
      "7 0:1 1:0.027027 2:0.00840336 3:-0.778761 4:-0.624225 5:-0.698722 6:-0.669519 7:-0.711011 \n",
      "6 0:1 1:0.027027 2:0.0756303 3:-0.787611 4:-0.649371 5:-0.6846 6:-0.723502 7:-0.744893 \n",
      "7 0:1 1:0.0405405 2:0.0252101 3:-0.823009 4:-0.673101 5:-0.688635 6:-0.756419 7:-0.773792 \n",
      "7 0:1 1:0.0405405 2:-0.0252101 3:-0.814159 4:-0.683372 5:-0.737727 6:-0.752469 7:-0.750872 \n",
      "7 0:1 1:0.0540541 2:0.0420168 3:-0.79646 4:-0.670622 5:-0.68998 6:-0.757735 7:-0.777778 \n",
      "7 0:1 1:0.0540541 2:0.0588235 3:-0.79646 4:-0.623163 5:-0.650303 6:-0.743252 7:-0.717987 \n",
      "7 0:1 1:0.0540541 2:-0.0252101 3:-0.80531 4:-0.688684 5:-0.765299 6:-0.763002 7:-0.763827 \n",
      "6 1:0.0540541 2:-0.00840336 3:-0.778761 4:-0.659996 5:-0.691997 6:-0.722186 7:-0.784753 \n",
      "8 0:-1 1:0.0675676 2:0.0420168 3:-0.787611 4:-0.567912 5:-0.561533 6:-0.60632 7:-0.723966 \n",
      "8 1:0.0675676 2:0.0420168 3:-0.787611 4:-0.589162 5:-0.611298 6:-0.759052 7:-0.712008 \n",
      "9 0:-1 1:0.0810811 2:0.0588235 3:-0.778761 4:-0.621038 5:-0.702757 6:-0.681369 7:-0.704036 \n",
      "6 1:0.0810811 2:0.0252101 3:-0.787611 4:-0.582433 5:-0.564896 6:-0.711652 7:-0.749875 \n",
      "6 0:-1 1:0.0945946 2:0.0756303 3:-0.79646 4:-0.522224 5:-0.570276 6:-0.720869 7:-0.664175 \n",
      "6 0:-1 1:0.0945946 2:0.109244 3:-0.743363 4:-0.548079 5:-0.607935 6:-0.631336 7:-0.689088 \n",
      "8 0:-1 1:0.0945946 2:0.0252101 3:-0.823009 4:-0.690455 5:-0.740417 6:-0.740619 7:-0.773792 \n",
      "7 0:-1 1:0.0945946 2:0.0420168 3:-0.787611 4:-0.57535 5:-0.581708 6:-0.693219 7:-0.723966 \n",
      "7 1:0.108108 2:0.0588235 3:-0.79646 4:-0.662476 5:-0.73302 6:-0.749835 7:-0.745889 \n",
      "8 0:-1 1:0.121622 2:0.109244 3:-0.778761 4:-0.541704 5:-0.570948 6:-0.674786 7:-0.665172 \n",
      "8 0:-1 1:0.135135 2:0.142857 3:-0.761062 4:-0.552683 5:-0.593141 6:-0.660303 7:-0.705032 \n",
      "7 0:-1 1:0.135135 2:0.159664 3:-0.761062 4:-0.569329 5:-0.635508 6:-0.623436 7:-0.715994 \n",
      "6 0:-1 1:0.148649 2:0.12605 3:-0.761062 4:-0.534266 5:-0.578346 6:-0.597103 7:-0.691081 \n",
      "8 0:1 1:0.148649 2:0.109244 3:-0.787611 4:-0.604746 5:-0.62004 6:-0.730086 7:-0.733931 \n",
      "7 0:-1 1:0.148649 2:0.109244 3:-0.761062 4:-0.546308 5:-0.571621 6:-0.661619 7:-0.69706 \n",
      "7 0:-1 1:0.148649 2:0.159664 3:-0.778761 4:-0.525058 5:-0.549428 6:-0.685319 7:-0.639263 \n",
      "9 1:0.162162 2:0.12605 3:-0.769912 4:-0.523995 5:-0.575656 6:-0.63002 7:-0.647235 \n",
      "8 0:1 1:0.162162 2:0.12605 3:-0.734513 4:-0.516203 5:-0.514459 6:-0.656353 7:-0.692078 \n",
      "9 0:-1 1:0.162162 2:0.193277 3:-0.778761 4:-0.546662 5:-0.612643 6:-0.651086 7:-0.694071 \n",
      "8 0:1 1:0.162162 2:0.00840336 3:-0.778761 4:-0.575704 5:-0.665098 6:-0.684003 7:-0.63428 \n",
      "8 0:-1 1:0.175676 2:0.12605 3:-0.761062 4:-0.456703 5:-0.472091 6:-0.61817 7:-0.624315 \n",
      "7 0:1 1:0.175676 2:0.0756303 3:-0.823009 4:-0.591642 5:-0.681237 6:-0.678736 7:-0.65421 \n",
      "7 0:1 1:0.175676 2:0.176471 3:-0.761062 4:-0.456703 5:-0.509751 6:-0.583937 7:-0.644245 \n",
      "8 0:-1 1:0.175676 2:0.176471 3:-0.734513 4:-0.503099 5:-0.534633 6:-0.648453 7:-0.627304 \n",
      "8 0:-1 1:0.175676 2:0.193277 3:-0.743363 4:-0.437577 5:-0.481506 6:-0.523371 7:-0.613353 \n",
      "8 1:0.189189 2:0.260504 3:-0.752212 4:-0.410661 5:-0.507734 6:-0.474654 7:-0.544594 \n",
      "7 0:-1 1:0.189189 2:0.12605 3:-0.725664 4:-0.496724 5:-0.504371 6:-0.640553 7:-0.694071 \n",
      "8 1:0.216216 2:0.210084 3:-0.752212 4:-0.488578 5:-0.533961 6:-0.545754 7:-0.65421 \n",
      "11 0:-1 1:0.216216 2:0.159664 3:-0.752212 4:-0.482557 5:-0.552118 6:-0.58657 7:-0.664175 \n",
      "8 1:0.22973 2:0.243697 3:-0.769912 4:-0.46414 5:-0.564223 6:-0.482554 7:-0.59442 \n",
      "8 1:0.22973 2:0.243697 3:-0.734513 4:-0.399681 5:-0.560188 6:-0.390388 7:-0.600399 \n",
      "7 0:-1 1:0.22973 2:0.176471 3:-0.778761 4:-0.539933 5:-0.63618 6:-0.57867 7:-0.632287 \n",
      "8 1:0.243243 2:0.159664 3:-0.761062 4:-0.419515 5:-0.472091 6:-0.485188 7:-0.59442 \n",
      "8 0:-1 1:0.243243 2:0.260504 3:-0.752212 4:-0.493891 5:-0.61735 6:-0.581303 7:-0.573493 \n",
      "8 0:-1 1:0.243243 2:0.277311 3:-0.752212 4:-0.382327 5:-0.498991 6:-0.398288 7:-0.565521 \n",
      "9 1:0.283784 2:0.310924 3:-0.725664 4:-0.299805 5:-0.26967 6:-0.532587 7:-0.56851 \n",
      "9 1:0.283784 2:0.260504 3:-0.752212 4:-0.427307 5:-0.506389 6:-0.577354 7:-0.454908 \n",
      "9 1:0.310811 2:0.344538 3:-0.716814 4:-0.316451 5:-0.392737 6:-0.456221 7:-0.456901 \n",
      "8 1:0.324324 2:0.159664 3:-0.769912 4:-0.507349 5:-0.587761 6:-0.56287 7:-0.644245 \n",
      "10 0:-1 1:0.337838 2:0.327731 3:-0.725664 4:-0.15495 5:-0.244788 6:-0.325872 7:-0.415047 \n",
      "10 0:-1 1:0.337838 2:0.327731 3:-0.725664 4:-0.156012 5:-0.311365 6:-0.448321 7:-0.319382 \n",
      "8 1:0.337838 2:0.344538 3:-0.734513 4:-0.217284 5:-0.27505 6:-0.329822 7:-0.464873 \n",
      "8 0:-1 1:0.337838 2:0.310924 3:-0.752212 4:-0.248096 5:-0.293208 6:-0.423305 7:-0.524664 \n",
      "7 0:-1 1:0.337838 2:0.361345 3:-0.699115 4:-0.361431 5:-0.453262 6:-0.491771 7:-0.576482 \n",
      "9 0:-1 1:0.351351 2:0.411765 3:-0.716814 4:-0.212325 5:-0.334902 6:-0.278473 7:-0.425012 \n",
      "9 1:0.351351 2:0.361345 3:-0.716814 4:-0.220117 5:-0.27774 6:-0.419355 7:-0.506726 \n",
      "7 1:0.364865 2:0.361345 3:-0.734513 4:-0.211263 5:-0.251513 6:-0.407505 7:-0.425012 \n",
      "8 1:0.364865 2:0.361345 3:-0.681416 4:-0.256596 5:-0.450572 6:-0.317972 7:-0.452915 \n",
      "10 0:-1 1:0.364865 2:0.344538 3:-0.734513 4:-0.284576 5:-0.330868 6:-0.444371 7:-0.437967 \n",
      "8 1:0.364865 2:0.327731 3:-0.743363 4:-0.196033 5:-0.250168 6:-0.421988 7:-0.425012 \n",
      "9 0:-1 1:0.364865 2:0.462185 3:-0.769912 4:-0.198512 5:-0.213181 6:-0.325872 7:-0.530643 \n",
      "11 0:-1 1:0.391892 2:0.378151 3:-0.725664 4:-0.196742 5:-0.295898 6:-0.312706 7:-0.454908 \n",
      "8 0:-1 1:0.391892 2:0.394958 3:-0.716814 4:-0.147158 5:-0.357095 6:-0.362739 7:-0.387145 \n",
      "8 1:0.391892 2:0.344538 3:-0.743363 4:-0.248451 5:-0.308003 6:-0.357472 7:-0.504733 \n",
      "11 1:0.405405 2:0.394958 3:-0.725664 4:-0.207367 5:-0.394082 6:-0.532587 7:-0.694071 \n",
      "11 1:0.405405 2:0.327731 3:-0.734513 4:-0.212325 5:-0.212508 6:-0.420671 7:-0.504733 \n",
      "9 0:-1 1:0.405405 2:0.411765 3:-0.707965 4:-0.142199 5:-0.166106 6:-0.360105 7:-0.456901 \n",
      "9 1:0.405405 2:0.361345 3:-0.752212 4:-0.289888 5:-0.375252 6:-0.449638 7:-0.501744 \n",
      "9 0:-1 1:0.405405 2:0.344538 3:-0.734513 4:-0.261909 5:-0.30464 6:-0.420671 7:-0.464873 \n",
      "9 1:0.432432 2:0.462185 3:-0.734513 4:-0.197804 5:-0.293208 6:-0.304806 7:-0.415047 \n",
      "9 0:-1 1:0.432432 2:0.411765 3:-0.725664 4:-0.179033 5:-0.232011 6:-0.354839 7:-0.454908 \n",
      "11 0:-1 1:0.432432 2:0.394958 3:-0.707965 4:-0.129095 5:-0.190989 6:-0.311389 7:-0.42003 \n",
      "11 0:-1 1:0.445946 2:0.394958 3:-0.734513 4:-0.177971 5:-0.241426 6:-0.321922 7:-0.388142 \n",
      "10 0:-1 1:0.445946 2:0.411765 3:-0.725664 4:-0.174075 5:-0.256221 6:-0.371955 7:-0.346288 \n",
      "8 1:0.459459 2:0.428571 3:-0.716814 4:-0.11422 5:-0.214526 6:-0.317972 7:-0.345291 \n",
      "11 1:0.472973 2:0.529412 3:-0.681416 4:-0.058261 5:-0.202421 6:-0.274523 7:-0.229696 \n",
      "10 1:0.486486 2:0.428571 3:-0.699115 4:-0.0433859 5:-0.162744 6:-0.269256 7:-0.275536 \n",
      "10 0:-1 1:0.486486 2:0.462185 3:-0.690265 4:-0.0575527 5:-0.234028 6:-0.287689 7:-0.195815 \n",
      "10 1:0.486486 2:0.411765 3:-0.690265 4:-0.191429 5:-0.361802 6:-0.349572 7:-0.307424 \n",
      "9 1:0.486486 2:0.495798 3:-0.707965 4:-0.0890738 5:-0.230666 6:-0.202107 7:-0.375187 \n",
      "9 1:0.486486 2:0.445378 3:-0.646018 4:-0.0239065 5:-0.215198 6:-0.206057 7:-0.203787 \n",
      "8 0:-1 1:0.5 2:0.445378 3:-0.725664 4:-0.0961573 5:-0.144586 6:-0.277156 7:-0.385152 \n",
      "11 1:0.5 2:0.478992 3:-0.707965 4:-0.0752612 5:-0.195696 6:-0.253456 7:-0.375187 \n",
      "8 0:-1 1:0.5 2:0.428571 3:-0.734513 4:-0.166637 5:-0.303968 6:-0.348255 7:-0.358246 \n",
      "11 0:-1 1:0.513514 2:0.462185 3:-0.690265 4:-0.0274482 5:-0.163416 6:-0.289006 7:-0.215745 \n",
      "10 0:-1 1:0.513514 2:0.579832 3:-0.672566 4:-0.00513547 5:-0.0813719 6:-0.211323 7:-0.256602 \n",
      "10 0:-1 1:0.527027 2:0.512605 3:-0.725664 4:-0.00796883 5:-0.0531271 6:-0.302172 7:-0.335326 \n",
      "11 1:0.527027 2:0.495798 3:-0.699115 4:0.0734903 5:-0.069267 6:-0.142857 7:-0.187843 \n",
      "10 1:0.527027 2:0.495798 3:-0.690265 4:-0.0139897 5:-0.337592 6:-0.235023 7:-0.20578 \n",
      "10 1:0.540541 2:0.495798 3:-0.725664 4:-0.136887 5:-0.174849 6:-0.379855 7:-0.368211 \n",
      "11 0:-1 1:0.540541 2:0.563025 3:-0.690265 4:0.157429 5:0.0464022 6:-0.100724 7:-0.142003 \n",
      "8 0:-1 1:0.540541 2:0.563025 3:-0.690265 4:0.104303 5:-0.0477471 6:-0.0651745 7:-0.20578 \n",
      "8 1:0.540541 2:0.512605 3:-0.707965 4:0.0132814 5:-0.0813719 6:-0.190257 7:-0.333333 \n",
      "11 0:-1 1:0.540541 2:0.495798 3:-0.690265 4:-0.0533026 5:-0.149966 6:-0.213957 7:-0.307424 \n",
      "10 1:0.540541 2:0.495798 3:-0.716814 4:-0.11847 5:-0.264963 6:-0.140224 7:-0.405082 \n",
      "10 1:0.540541 2:0.546218 3:-0.734513 4:-0.142908 5:-0.308675 6:-0.460171 7:-0.235675 \n",
      "9 0:-1 1:0.554054 2:0.478992 3:-0.716814 4:-0.0777404 5:-0.234701 6:-0.179724 7:-0.260588 \n",
      "11 0:-1 1:0.554054 2:0.563025 3:-0.628319 4:0.187533 5:-0.104909 6:-0.190257 7:-0.0861983 \n",
      "10 0:-1 1:0.554054 2:0.579832 3:-0.672566 4:0.147512 5:-0.107599 6:-0.152074 7:-0.0523169 \n",
      "9 1:0.567568 2:0.361345 3:-0.716814 4:0.0568443 5:-0.0739744 6:-0.129691 7:-0.639263 \n",
      "10 1:0.567568 2:0.529412 3:-0.690265 4:0.169116 5:0.143914 6:-0.115207 7:-0.199801 \n",
      "11 1:0.581081 2:0.512605 3:-0.672566 4:0.0809279 5:-0.073302 6:-0.20474 7:-0.124066 \n",
      "9 0:-1 1:0.581081 2:0.613445 3:-0.663717 4:0.125199 5:-0.137189 6:-0.219223 7:0.0284006 \n",
      "10 0:-1 1:0.581081 2:0.478992 3:-0.654867 4:0.151408 5:-0.202421 6:-0.0546412 7:-0.0363727 \n",
      "6 1:0.581081 2:0.411765 3:-0.681416 4:-0.0313441 5:-0.139206 6:-0.227123 7:-0.335326 \n",
      "11 0:-1 1:0.608108 2:0.579832 3:-0.707965 4:0.137949 5:-0.0840619 6:-0.17314 7:-0.204783 \n",
      "10 1:0.621622 2:0.731092 3:-0.60177 4:0.12272 5:-0.00739744 6:-0.21264 7:-0.136024 \n",
      "12 1:0.621622 2:0.714286 3:-0.654867 4:0.300159 5:0.0268998 6:-0.0493746 7:0.099153 \n",
      "12 0:-1 1:0.635135 2:0.613445 3:-0.672566 4:0.136887 5:-0.0268998 6:-0.154707 7:-0.0104634 \n",
      "8 0:-1 1:0.662162 2:0.579832 3:-0.690265 4:0.203117 5:0.108944 6:-0.0480579 7:-0.195815 \n",
      "10 0:-1 1:0.662162 2:0.512605 3:-0.646018 4:0.324597 5:0.199731 6:0.0559579 7:-0.0463378 \n",
      "10 1:0.675676 2:0.613445 3:-0.690265 4:0.300868 5:0.0793544 6:0.0414747 7:-0.000498256 \n",
      "12 1:0.702703 2:0.613445 3:-0.681416 4:0.192137 5:-0.069267 6:0.104674 7:-0.196811 \n",
      "11 0:-1 1:0.716216 2:0.714286 3:-0.637168 4:0.555516 5:0.359785 6:0.374589 7:0.0881913 \n",
      "10 0:-1 1:0.72973 2:0.714286 3:-0.690265 4:0.381619 5:0.0268998 6:0.100724 7:-0.178874 \n",
      "11 1:0.72973 2:0.579832 3:-0.672566 4:0.103595 5:-0.106254 6:0.00724161 7:-0.195815 \n",
      "11 1:0.783784 2:0.831933 3:-0.610619 4:0.808394 5:0.523201 6:0.15734 7:0.192825 \n",
      "12 0:-1 1:0.864865 2:0.831933 3:-0.610619 4:0.629184 5:0.35306 6:0.339039 7:0.233682 \n",
      "4 0:1 1:-0.702703 2:-0.747899 3:-0.920354 4:-0.980875 5:-0.985205 6:-0.981567 7:-0.984056 \n",
      "3 0:1 1:-0.675676 2:-0.680672 3:-0.920354 4:-0.974854 5:-0.977135 6:-0.985517 7:-0.981066 \n",
      "4 0:1 1:-0.675676 2:-0.731092 3:-0.929204 4:-0.978396 5:-0.983188 6:-0.98815 7:-0.984056 \n",
      "4 0:1 1:-0.662162 2:-0.663866 3:-0.929204 4:-0.970604 5:-0.980498 6:-0.977617 7:-0.989038 \n",
      "4 0:1 1:-0.594595 2:-0.630252 3:-0.902655 4:-0.959625 5:-0.96503 6:-0.9684 7:-0.973094 \n",
      "4 0:1 1:-0.540541 2:-0.579832 3:-0.884956 4:-0.951124 5:-0.960995 6:-0.967084 7:-0.96014 \n",
      "5 0:1 1:-0.527027 2:-0.579832 3:-0.884956 4:-0.952895 5:-0.968393 6:-0.9605 7:-0.958146 \n",
      "7 0:1 1:-0.486486 2:-0.529412 3:-0.902655 4:-0.941916 5:-0.952253 6:-0.955234 7:-0.953164 \n",
      "6 0:1 1:-0.459459 2:-0.529412 3:-0.884956 4:-0.926333 5:-0.928716 6:-0.94865 7:-0.947185 \n",
      "5 0:1 1:-0.445946 2:-0.478992 3:-0.849558 4:-0.92527 5:-0.945528 6:-0.938117 7:-0.935227 \n",
      "5 0:1 1:-0.432432 2:-0.445378 3:-0.884956 4:-0.933416 5:-0.946873 6:-0.947334 7:-0.943199 \n",
      "6 0:1 1:-0.391892 2:-0.445378 3:-0.858407 4:-0.91252 5:-0.927371 6:-0.931534 7:-0.925262 \n",
      "6 0:1 1:-0.351351 2:-0.394958 3:-0.902655 4:-0.894457 5:-0.913921 6:-0.930217 7:-0.925262 \n",
      "6 0:1 1:-0.337838 2:-0.428571 3:-0.849558 4:-0.901186 5:-0.910558 6:-0.923634 7:-0.922272 \n",
      "5 0:1 1:-0.283784 2:-0.294118 3:-0.858407 4:-0.858686 5:-0.880296 6:-0.876234 7:-0.893373 \n",
      "6 0:1 1:-0.202703 2:-0.243697 3:-0.823009 4:-0.844873 5:-0.844654 6:-0.931534 7:-0.876432 \n",
      "6 0:1 1:-0.189189 2:-0.243697 3:-0.858407 4:-0.835311 5:-0.850034 6:-0.881501 7:-0.869457 \n",
      "6 0:1 1:-0.189189 2:-0.260504 3:-0.823009 4:-0.836727 5:-0.844654 6:-0.890718 7:-0.873443 \n",
      "7 0:1 1:-0.162162 2:-0.210084 3:-0.858407 4:-0.825394 5:-0.837256 6:-0.870968 7:-0.873443 \n",
      "6 0:1 1:-0.121622 2:-0.109244 3:-0.831858 4:-0.754914 5:-0.740417 6:-0.861751 7:-0.829596 \n",
      "7 0:1 1:-0.108108 2:-0.176471 3:-0.80531 4:-0.774748 5:-0.770007 6:-0.885451 7:-0.817638 \n",
      "7 0:1 1:-0.0945946 2:-0.176471 3:-0.823009 4:-0.801665 5:-0.832549 6:-0.851218 7:-0.8286 \n",
      "6 0:1 1:-0.0945946 2:-0.092437 3:-0.823009 4:-0.77156 5:-0.823806 6:-0.811718 7:-0.791729 \n",
      "7 0:1 1:-0.0675676 2:-0.176471 3:-0.814159 4:-0.777581 5:-0.832549 6:-0.816985 7:-0.796712 \n",
      "7 0:1 1:-0.0675676 2:-0.109244 3:-0.80531 4:-0.744643 5:-0.767317 6:-0.834101 7:-0.793722 \n",
      "6 0:1 1:-0.0675676 2:-0.142857 3:-0.831858 4:-0.80379 5:-0.832549 6:-0.867018 7:-0.827603 \n",
      "7 0:1 1:-0.0540541 2:-0.092437 3:-0.79646 4:-0.740393 5:-0.783457 6:-0.773535 7:-0.794718 \n",
      "7 0:-1 1:-0.0405405 2:-0.0588235 3:-0.787611 4:-0.720205 5:-0.734364 6:-0.773535 7:-0.796712 \n",
      "6 0:1 1:-0.027027 2:-0.0756303 3:-0.80531 4:-0.708872 5:-0.72495 6:-0.748519 7:-0.811659 \n",
      "7 0:1 1:-0.027027 2:-0.0252101 3:-0.79646 4:-0.70533 5:-0.702757 6:-0.807768 7:-0.791729 \n",
      "7 0:1 1:-0.0135135 2:-0.0756303 3:-0.80531 4:-0.738976 5:-0.793544 6:-0.780118 7:-0.763827 \n",
      "7 0:1 2:-0.0252101 3:-0.814159 4:-0.711705 5:-0.776059 6:-0.734036 7:-0.769806 \n",
      "7 0:1 2:-0.0420168 3:-0.743363 4:-0.693997 5:-0.739744 6:-0.763002 7:-0.743896 \n",
      "8 0:1 2:-0.0588235 3:-0.80531 4:-0.710289 5:-0.734364 6:-0.755102 7:-0.78575 \n",
      "8 0:1 1:0.0135135 2:0.0420168 3:-0.778761 4:-0.674163 5:-0.71419 6:-0.741935 7:-0.740907 \n",
      "6 0:1 1:0.0135135 2:-0.0420168 3:-0.787611 4:-0.652559 5:-0.677202 6:-0.718236 7:-0.763827 \n",
      "7 0:1 1:0.0135135 2:-0.0756303 3:-0.814159 4:-0.738268 5:-0.750504 6:-0.794602 7:-0.80867 \n",
      "6 0:1 1:0.0135135 2:-0.0756303 3:-0.823009 4:-0.710289 5:-0.739744 6:-0.737986 7:-0.807673 \n",
      "6 0:1 1:0.0135135 2:-0.0756303 3:-0.80531 4:-0.740393 5:-0.786147 6:-0.768269 7:-0.799701 \n",
      "7 0:1 1:0.0405405 2:-0.00840336 3:-0.79646 4:-0.707101 5:-0.759247 6:-0.743252 7:-0.768809 \n",
      "7 0:-1 1:0.0675676 2:0.0252101 3:-0.814159 4:-0.616079 5:-0.638198 6:-0.63397 7:-0.745889 \n",
      "6 0:1 1:0.0675676 2:0.092437 3:-0.778761 4:-0.658226 5:-0.717552 6:-0.718236 7:-0.71998 \n",
      "6 0:1 1:0.0810811 2:-0.00840336 3:-0.80531 4:-0.678059 5:-0.724277 6:-0.740619 7:-0.743896 \n",
      "7 0:1 1:0.0810811 2:-0.00840336 3:-0.823009 4:-0.679476 5:-0.710155 6:-0.709019 7:-0.773792 \n",
      "8 0:1 1:0.0945946 2:0.092437 3:-0.778761 4:-0.559058 5:-0.544721 6:-0.715602 7:-0.671151 \n",
      "7 0:-1 1:0.121622 2:0.378151 3:-0.778761 4:-0.631309 5:-0.685272 6:-0.658986 7:-0.721973 \n",
      "7 0:1 1:0.148649 2:0.0756303 3:-0.752212 4:-0.612183 5:-0.667787 6:-0.706386 7:-0.687095 \n",
      "9 0:1 1:0.148649 2:0.0756303 3:-0.787611 4:-0.617496 5:-0.712172 6:-0.695853 7:-0.664175 \n",
      "8 0:1 1:0.148649 2:0.092437 3:-0.778761 4:-0.633788 5:-0.667115 6:-0.702436 7:-0.735924 \n",
      "6 0:-1 1:0.148649 2:0.12605 3:-0.778761 4:-0.632017 5:-0.667115 6:-0.693219 7:-0.741903 \n",
      "7 1:0.162162 2:0.12605 3:-0.778761 4:-0.615725 5:-0.670477 6:-0.60632 7:-0.72297 \n",
      "8 0:1 1:0.175676 2:0.176471 3:-0.778761 4:-0.520099 5:-0.535306 6:-0.63397 7:-0.640259 \n",
      "6 1:0.175676 2:0.159664 3:-0.778761 4:-0.615371 5:-0.650303 6:-0.698486 7:-0.726956 \n",
      "9 0:1 1:0.175676 2:0.159664 3:-0.778761 4:-0.606517 5:-0.64963 6:-0.686636 7:-0.699053 \n",
      "8 0:1 1:0.175676 2:0.092437 3:-0.79646 4:-0.636267 5:-0.712172 6:-0.702436 7:-0.672147 \n",
      "8 0:1 1:0.189189 2:0.109244 3:-0.778761 4:-0.568266 5:-0.574311 6:-0.668203 7:-0.704036 \n",
      "9 0:-1 1:0.202703 2:0.159664 3:-0.743363 4:-0.45139 5:-0.527236 6:-0.515471 7:-0.63428 \n",
      "8 0:1 1:0.202703 2:0.092437 3:-0.761062 4:-0.619267 5:-0.69267 6:-0.651086 7:-0.690085 \n",
      "8 0:1 1:0.202703 2:0.092437 3:-0.778761 4:-0.608642 5:-0.613988 6:-0.660303 7:-0.670154 \n",
      "7 1:0.202703 2:0.361345 3:-0.734513 4:-0.279617 5:-0.297915 6:-0.478604 7:-0.496761 \n",
      "8 0:1 1:0.216216 2:0.159664 3:-0.769912 4:-0.544183 5:-0.564223 6:-0.673469 7:-0.664175 \n",
      "9 0:1 1:0.216216 2:0.159664 3:-0.752212 4:-0.575704 5:-0.648285 6:-0.662936 7:-0.63727 \n",
      "9 0:-1 1:0.216216 2:0.176471 3:-0.787611 4:-0.466265 5:-0.496301 6:-0.591837 7:-0.602392 \n",
      "8 0:1 1:0.216216 2:0.142857 3:-0.787611 4:-0.570746 5:-0.601883 6:-0.63397 7:-0.72297 \n",
      "8 0:1 1:0.22973 2:0.159664 3:-0.778761 4:-0.56437 5:-0.626093 6:-0.666886 7:-0.624315 \n",
      "9 0:1 1:0.243243 2:0.12605 3:-0.778761 4:-0.577121 5:-0.65232 6:-0.608953 7:-0.666168 \n",
      "10 0:1 1:0.256757 2:0.226891 3:-0.752212 4:-0.529308 5:-0.581036 6:-0.63792 7:-0.625311 \n",
      "7 0:-1 1:0.27027 2:0.12605 3:-0.761062 4:-0.446432 5:-0.433087 6:-0.523371 7:-0.692078 \n",
      "9 0:-1 1:0.27027 2:0.193277 3:-0.787611 4:-0.439702 5:-0.417619 6:-0.631336 7:-0.624315 \n",
      "9 0:-1 1:0.27027 2:0.210084 3:-0.752212 4:-0.420577 5:-0.380632 6:-0.666886 7:-0.56851 \n",
      "8 1:0.283784 2:0.210084 3:-0.761062 4:-0.424473 5:-0.427034 6:-0.512837 7:-0.65421 \n",
      "8 1:0.283784 2:0.260504 3:-0.734513 4:-0.40641 5:-0.470074 6:-0.487821 7:-0.558545 \n",
      "10 0:-1 1:0.283784 2:0.243697 3:-0.734513 4:-0.412431 5:-0.448554 6:-0.536537 7:-0.5715 \n",
      "9 0:-1 1:0.310811 2:0.260504 3:-0.743363 4:-0.364264 5:-0.37727 6:-0.533904 7:-0.536622 \n",
      "8 0:-1 1:0.310811 2:0.310924 3:-0.716814 4:-0.366389 5:-0.436449 6:-0.428571 7:-0.561535 \n",
      "8 1:0.310811 2:0.294118 3:-0.725664 4:-0.547724 5:-0.549428 6:-0.536537 7:-0.514699 \n",
      "8 0:-1 1:0.310811 2:0.210084 3:-0.743363 4:-0.39791 5:-0.422999 6:-0.504937 7:-0.59442 \n",
      "9 0:-1 1:0.324324 2:0.344538 3:-0.734513 4:-0.321764 5:-0.387357 6:-0.370639 7:-0.544594 \n",
      "8 0:-1 1:0.324324 2:0.277311 3:-0.734513 4:-0.300159 5:-0.221923 6:-0.520737 7:-0.592427 \n",
      "9 1:0.324324 2:0.327731 3:-0.690265 4:-0.286347 5:-0.400134 6:-0.375905 7:-0.475835 \n",
      "9 0:-1 1:0.337838 2:0.361345 3:-0.734513 4:-0.266513 5:-0.273033 6:-0.465438 7:-0.504733 \n",
      "8 1:0.337838 2:0.310924 3:-0.743363 4:-0.379848 5:-0.447209 6:-0.429888 7:-0.564524 \n",
      "8 0:1 1:0.337838 2:0.294118 3:-0.734513 4:-0.466619 5:-0.540686 6:-0.579987 7:-0.556552 \n",
      "7 1:0.351351 2:0.361345 3:-0.743363 4:-0.296972 5:-0.37458 6:-0.398288 7:-0.474838 \n",
      "10 1:0.351351 2:0.327731 3:-0.716814 4:-0.244909 5:-0.253531 6:-0.437788 7:-0.489786 \n",
      "9 0:-1 1:0.351351 2:0.277311 3:-0.752212 4:-0.402515 5:-0.462004 6:-0.498354 7:-0.560538 \n",
      "9 1:0.351351 2:0.394958 3:-0.707965 4:-0.385869 5:-0.416274 6:-0.482554 7:-0.52865 \n",
      "9 0:-1 1:0.351351 2:0.344538 3:-0.761062 4:-0.358952 5:-0.430397 6:-0.482554 7:-0.484803 \n",
      "10 0:1 1:0.351351 2:0.277311 3:-0.769912 4:-0.431202 5:-0.577001 6:-0.433838 7:-0.514699 \n",
      "9 0:-1 1:0.351351 2:0.310924 3:-0.699115 4:-0.277138 5:-0.262946 6:-0.428571 7:-0.54858 \n",
      "9 0:-1 1:0.351351 2:0.310924 3:-0.743363 4:-0.401452 5:-0.443174 6:-0.489138 7:-0.564524 \n",
      "8 0:-1 1:0.364865 2:0.344538 3:-0.734513 4:-0.212325 5:-0.360457 6:-0.433838 7:-0.474838 \n",
      "9 0:-1 1:0.364865 2:0.344538 3:-0.654867 4:0.315389 5:0.26967 6:0.120474 7:-0.124066 \n",
      "9 0:-1 1:0.364865 2:0.310924 3:-0.761062 4:-0.424827 5:-0.493611 6:-0.497038 7:-0.564524 \n",
      "9 0:-1 1:0.364865 2:0.327731 3:-0.752212 4:-0.320347 5:-0.34768 6:-0.523371 7:-0.498754 \n",
      "8 0:-1 1:0.364865 2:0.327731 3:-0.743363 4:-0.291305 5:-0.265636 6:-0.481238 7:-0.54559 \n",
      "9 1:0.364865 2:0.327731 3:-0.725664 4:-0.34266 5:-0.483524 6:-0.353522 7:-0.474838 \n",
      "9 0:-1 1:0.378378 2:0.361345 3:-0.743363 4:-0.340181 5:-0.35844 6:-0.520737 7:-0.534629 \n",
      "7 0:-1 1:0.378378 2:0.378151 3:-0.716814 4:-0.324597 5:-0.383322 6:-0.379855 7:-0.474838 \n",
      "8 0:-1 1:0.391892 2:0.394958 3:-0.734513 4:-0.296264 5:-0.354405 6:-0.390388 7:-0.524664 \n",
      "9 1:0.418919 2:0.411765 3:-0.716814 4:-0.274305 5:-0.349025 6:-0.344305 7:-0.491779 \n",
      "8 0:-1 1:0.418919 2:0.344538 3:-0.699115 4:-0.157429 5:-0.065232 6:-0.370639 7:-0.524664 \n",
      "8 1:0.418919 2:0.378151 3:-0.734513 4:-0.220471 5:-0.267653 6:-0.311389 7:-0.504733 \n",
      "10 0:-1 1:0.418919 2:0.378151 3:-0.725664 4:-0.281388 5:-0.31271 6:-0.352205 7:-0.554559 \n",
      "9 1:0.432432 2:0.394958 3:-0.707965 4:-0.167345 5:-0.179556 6:-0.402238 7:-0.421026 \n",
      "9 0:-1 1:0.432432 2:0.411765 3:-0.752212 4:-0.209846 5:-0.254876 6:-0.324556 7:-0.456901 \n",
      "10 0:-1 1:0.432432 2:0.428571 3:-0.699115 4:-0.163095 5:-0.218561 6:-0.378539 7:-0.371201 \n",
      "9 1:0.432432 2:0.411765 3:-0.707965 4:-0.253409 5:-0.418964 6:-0.423305 7:-0.29148 \n",
      "8 0:-1 1:0.445946 2:0.445378 3:-0.716814 4:-0.282805 5:-0.286483 6:-0.443055 7:-0.521674 \n",
      "9 0:-1 1:0.445946 2:0.445378 3:-0.743363 4:-0.0589694 5:0.0517821 6:-0.407505 7:-0.431988 \n",
      "9 0:-1 1:0.445946 2:0.394958 3:-0.707965 4:-0.256242 5:-0.33154 6:-0.364055 7:-0.470852 \n",
      "10 0:-1 1:0.459459 2:0.361345 3:-0.699115 4:-0.253055 5:-0.353732 6:-0.285056 7:-0.464873 \n",
      "9 1:0.459459 2:0.378151 3:-0.734513 4:-0.347618 5:-0.380632 6:-0.520737 7:-0.521674 \n",
      "9 1:0.459459 2:0.411765 3:-0.725664 4:-0.273951 5:-0.400134 6:-0.342989 7:-0.434978 \n",
      "8 0:-1 1:0.472973 2:0.394958 3:-0.761062 4:-0.279263 5:-0.286483 6:-0.473338 7:-0.509716 \n",
      "8 0:-1 1:0.472973 2:0.327731 3:-0.646018 4:-0.39366 5:-0.425017 6:-0.599737 7:-0.523667 \n",
      "8 1:0.472973 2:0.428571 3:-0.716814 4:-0.213388 5:-0.243443 6:-0.357472 7:-0.44295 \n",
      "9 1:0.486486 2:0.445378 3:-0.690265 4:-0.0278024 5:-0.0147949 6:-0.286373 7:-0.341305 \n",
      "8 0:-1 1:0.486486 2:0.428571 3:-0.672566 4:-0.146804 5:-0.211836 6:-0.237656 7:-0.432985 \n",
      "8 0:-1 1:0.5 2:0.394958 3:-0.725664 4:-0.199221 5:-0.209146 6:-0.445688 7:-0.430992 \n",
      "10 0:-1 1:0.5 2:0.495798 3:-0.690265 4:-0.10572 5:-0.243443 6:-0.194207 7:-0.320379 \n",
      "9 1:0.513514 2:0.478992 3:-0.973451 4:-0.182221 5:-0.313383 6:-0.190257 7:-0.428002 \n",
      "10 0:-1 1:0.527027 2:0.546218 3:-0.707965 4:-0.0316982 5:-0.151311 6:-0.102041 7:-0.289487 \n",
      "10 0:-1 1:0.540541 2:0.596639 3:-0.654867 4:-0.0168231 5:-0.131809 6:-0.225806 7:-0.258595 \n",
      "10 1:0.540541 2:0.428571 3:-0.699115 4:-0.197804 5:-0.29119 6:-0.332456 7:-0.395117 \n",
      "9 1:0.554054 2:0.495798 3:-0.663717 4:0.0355941 5:-0.138534 6:-0.108624 7:-0.156951 \n",
      "11 0:-1 1:0.554054 2:0.495798 3:-0.725664 4:-0.149991 5:-0.241426 6:-0.175774 7:-0.41704 \n",
      "11 0:-1 1:0.567568 2:0.546218 3:-0.716814 4:-0.0734903 5:-0.257566 6:-0.0296248 7:-0.315396 \n",
      "10 1:0.567568 2:0.529412 3:-0.690265 4:0.000885426 5:-0.209818 6:-0.0204082 7:-0.277529 \n",
      "11 1:0.581081 2:0.596639 3:-0.672566 4:-0.0479901 5:-0.26698 6:-0.289006 7:-0.0543099 \n",
      "8 0:-1 1:0.594595 2:0.579832 3:-0.716814 4:-0.0359483 5:-0.155346 6:-0.266623 7:-0.325361 \n",
      "9 0:1 1:0.594595 2:0.495798 3:-0.699115 4:-0.0823446 5:-0.189644 6:-0.235023 7:-0.286497 \n",
      "9 1:0.608108 2:0.512605 3:-0.637168 4:-0.0348858 5:-0.184264 6:-0.20474 7:-0.299452 \n",
      "14 1:0.648649 2:0.630252 3:-0.619469 4:0.204533 5:-0.108272 6:-0.0388413 7:-0.0592925 \n",
      "10 0:-1 1:0.648649 2:0.563025 3:-0.707965 4:0.0745529 5:-0.061197 6:-0.0309414 7:-0.20578 \n",
      "9 1:0.662162 2:0.630252 3:-0.725664 4:0.0285107 5:-0.162071 6:-0.183673 7:-0.22571 \n",
      "12 0:-1 1:0.662162 2:0.596639 3:-0.628319 4:0.119887 5:-0.0121049 6:0.0651745 7:-0.232686 \n",
      "9 1:0.662162 2:0.596639 3:-0.699115 4:0.0989906 5:0.0672495 6:-0.0836076 7:-0.266567 \n",
      "10 0:-1 1:0.675676 2:0.697479 3:-0.672566 4:0.231096 5:0.188971 6:-0.0230415 7:-0.131041 \n",
      "9 0:-1 1:0.689189 2:0.714286 3:-0.681416 4:0.238888 5:0.202421 6:-0.117841 7:-0.114101 \n",
      "12 0:-1 1:0.689189 2:0.747899 3:-0.663717 4:0.608642 5:0.471419 6:0.0994075 7:0.26856 \n",
      "9 1:0.689189 2:0.579832 3:-0.663717 4:0.164866 5:0.147949 6:-0.19289 7:-0.206776 \n",
      "9 1:0.702703 2:0.663866 3:-0.699115 4:-0.137949 5:-0.141224 6:-0.379855 7:-0.403089 \n",
      "11 1:0.716216 2:0.697479 3:-0.681416 4:0.168762 5:-0.0127774 6:0.00197498 7:-0.101146 \n",
      "12 0:-1 1:0.72973 2:0.663866 3:-0.663717 4:0.418452 5:0.406187 6:0.0704411 7:0.00847035 \n",
      "10 0:-1 1:0.72973 2:0.613445 3:-0.663717 4:0.185408 5:0.194351 6:-0.17709 7:-0.165919 \n",
      "10 1:0.743243 2:0.764706 3:-0.654867 4:0.488224 5:0.379287 6:0.262673 7:0.0682611 \n",
      "10 1:0.743243 2:0.663866 3:-0.646018 4:0.412786 5:0.213853 6:0.233706 7:0.0363727 \n",
      "8 0:-1 1:0.743243 2:0.714286 3:-0.743363 4:-0.160616 5:-0.071957 6:-0.489138 7:-0.467862 \n",
      "12 0:-1 1:0.756757 2:0.512605 3:-0.672566 4:0.399681 5:0.378615 6:0.119157 7:-0.105132 \n",
      "12 1:0.77027 2:0.747899 3:-0.672566 4:0.32991 5:0.250841 6:-0.000658328 7:-0.0413553 \n",
      "11 0:-1 1:0.783784 2:0.781513 3:-0.672566 4:0.503099 5:0.279085 6:0.447005 7:-0.00647733 \n",
      "11 0:-1 1:0.810811 2:0.714286 3:-0.619469 4:0.366389 5:0.203766 6:0.206057 7:-0.00647733 \n",
      "11 1:0.824324 2:0.731092 3:-0.628319 4:0.582433 5:0.490249 6:0.366689 7:0.0832088 \n",
      "11 1:0.837838 2:0.915966 3:-0.628319 4:0.772977 5:0.607935 6:0.349572 7:0.349278 \n",
      "10 0:-1 1:0.837838 2:0.764706 3:-0.637168 4:0.419869 5:0.114324 6:0.0559579 7:0.18286 \n",
      "11 1:0.905405 2:0.932773 3:-0.619469 4:0.880645 5:1 6:0.311389 7:0.164923 \n",
      "6 0:1 1:-0.702703 2:0.0756303 3:-0.787611 4:-0.672392 5:-0.737727 6:-0.726136 7:-0.704036 \n",
      "4 0:1 1:-0.540541 2:-0.495798 3:-0.893805 4:-0.947229 5:-0.955615 6:-0.96445 7:-0.96014 \n",
      "5 0:1 1:-0.527027 2:-0.563025 3:-0.884956 4:-0.952895 5:-0.961668 6:-0.96445 7:-0.958146 \n",
      "5 0:1 1:-0.527027 2:-0.546218 3:-0.884956 4:-0.94227 5:-0.94889 6:-0.961817 7:-0.953164 \n",
      "6 0:1 1:-0.459459 2:-0.529412 3:-0.840708 4:-0.921728 5:-0.928043 6:-0.923634 7:-0.932237 \n",
      "5 0:1 1:-0.378378 2:-0.462185 3:-0.884956 4:-0.92527 5:-0.942165 6:-0.947334 7:-0.927255 \n",
      "5 0:1 1:-0.364865 2:-0.428571 3:-0.876106 4:-0.926687 5:-0.416274 6:-0.961817 7:-0.923269 \n",
      "5 0:1 1:-0.351351 2:-0.411765 3:-0.858407 4:-0.90402 5:-0.928043 6:-0.919684 7:-0.9143 \n",
      "5 0:1 1:-0.351351 2:-0.411765 3:-0.876106 4:-0.920312 5:-0.939475 6:-0.939434 7:-0.926258 \n",
      "6 0:1 1:-0.324324 2:-0.428571 3:-0.867257 4:-0.902957 5:-0.925353 6:-0.917051 7:-0.823617 \n",
      "7 0:1 1:-0.310811 2:-0.344538 3:-0.831858 4:-0.853728 5:-0.864156 6:-0.897301 7:-0.899352 \n",
      "5 0:1 1:-0.310811 2:-0.495798 3:-0.831858 4:-0.888436 5:-0.897781 6:-0.917051 7:-0.916293 \n",
      "7 0:1 1:-0.297297 2:-0.361345 3:-0.840708 4:-0.858686 5:-0.872226 6:-0.894668 7:-0.907324 \n",
      "6 0:1 1:-0.283784 2:-0.344538 3:-0.840708 4:-0.874624 5:-0.897108 6:-0.914417 7:-0.893373 \n",
      "6 0:1 1:-0.27027 2:-0.327731 3:-0.831858 4:-0.863644 5:-0.876933 6:-0.903884 7:-0.893373 \n",
      "5 0:1 1:-0.27027 2:-0.327731 3:-0.849558 4:-0.859394 5:-0.860121 6:-0.903884 7:-0.903338 \n",
      "6 0:1 1:-0.256757 2:-0.277311 3:-0.867257 4:-0.849123 5:-0.866846 6:-0.906517 7:-0.873443 \n",
      "8 0:1 1:-0.256757 2:-0.327731 3:-0.840708 4:-0.874978 5:-0.886348 6:-0.921001 7:-0.898356 \n",
      "4 0:1 1:-0.22973 2:-0.277311 3:-0.849558 4:-0.862582 5:-0.883658 6:-0.909151 7:-0.875436 \n",
      "6 0:1 1:-0.216216 2:-0.277311 3:-0.849558 4:-0.868603 5:-0.892401 6:-0.890718 7:-0.887394 \n",
      "7 0:1 1:-0.216216 2:-0.277311 3:-0.849558 4:-0.862582 5:-0.890383 6:-0.902567 7:-0.883408 \n",
      "7 0:1 1:-0.216216 2:-0.294118 3:-0.849558 4:-0.85054 5:-0.874243 6:-0.872284 7:-0.883408 \n",
      "7 0:1 1:-0.202703 2:-0.210084 3:-0.840708 4:-0.828227 5:-0.881641 6:-0.828835 7:-0.853513 \n",
      "6 0:1 1:-0.202703 2:-0.243697 3:-0.849558 4:-0.847707 5:-0.854069 6:-0.909151 7:-0.879422 \n",
      "5 0:1 1:-0.189189 2:-0.210084 3:-0.831858 4:-0.85054 5:-0.872226 6:-0.893351 7:-0.881415 \n",
      "7 0:1 1:-0.189189 2:-0.210084 3:-0.849558 4:-0.832477 5:-0.842636 6:-0.882818 7:-0.864474 \n",
      "6 0:1 1:-0.189189 2:-0.260504 3:-0.840708 4:-0.846998 5:-0.876261 6:-0.894668 7:-0.852516 \n",
      "6 0:1 1:-0.189189 2:-0.260504 3:-0.831858 4:-0.826811 5:-0.853396 6:-0.892034 7:-0.84853 \n",
      "8 0:1 1:-0.189189 2:-0.243697 3:-0.858407 4:-0.857978 5:-0.890383 6:-0.874918 7:-0.873443 \n",
      "7 0:1 1:-0.189189 2:-0.277311 3:-0.849558 4:-0.846998 5:-0.874243 6:-0.898618 7:-0.863478 \n",
      "6 0:1 1:-0.175676 2:-0.260504 3:-0.80531 4:-0.820081 5:-0.853396 6:-0.860434 7:-0.852516 \n",
      "8 0:1 1:-0.175676 2:-0.277311 3:-0.858407 4:-0.852311 5:-0.885003 6:-0.890718 7:-0.863478 \n",
      "9 0:1 1:-0.162162 2:-0.210084 3:-0.840708 4:-0.816186 5:-0.852051 6:-0.844635 7:-0.854509 \n",
      "8 0:1 1:-0.162162 2:-0.243697 3:-0.849558 4:-0.847353 5:-0.870881 6:-0.901251 7:-0.869457 \n",
      "7 0:1 1:-0.162162 2:-0.176471 3:-0.831858 4:-0.787498 5:-0.796907 6:-0.839368 7:-0.856502 \n",
      "5 0:1 1:-0.162162 2:-0.243697 3:-0.840708 4:-0.839915 5:-0.863484 6:-0.890718 7:-0.872446 \n",
      "7 0:1 1:-0.148649 2:-0.176471 3:-0.831858 4:-0.770143 5:-0.777404 6:-0.849901 7:-0.82561 \n",
      "8 0:1 1:-0.135135 2:-0.159664 3:-0.814159 4:-0.800248 5:-0.848689 6:-0.844635 7:-0.814649 \n",
      "8 0:1 1:-0.135135 2:-0.193277 3:-0.831858 4:-0.808394 5:-0.846671 6:-0.836735 7:-0.833582 \n",
      "5 0:1 1:-0.135135 2:-0.277311 3:-0.823009 4:-0.789977 5:-0.806994 6:-0.840685 7:-0.839562 \n",
      "5 0:1 1:-0.121622 2:-0.210084 3:-0.823009 4:-0.811936 5:-0.839274 6:-0.842001 7:-0.850523 \n",
      "6 0:1 1:-0.108108 2:-0.226891 3:-0.840708 4:-0.814061 5:-0.831204 6:-0.868334 7:-0.853513 \n",
      "7 0:1 1:-0.0945946 2:-0.0588235 3:-0.80531 4:-0.767664 5:-0.790182 6:-0.815668 7:-0.664175 \n",
      "6 0:1 1:-0.0675676 2:-0.159664 3:-0.840708 4:-0.769081 5:-0.775387 6:-0.839368 7:-0.839562 \n",
      "7 0:1 1:-0.0540541 2:-0.092437 3:-0.80531 4:-0.765185 5:-0.768662 6:-0.882818 7:-0.803687 \n",
      "7 0:1 1:-0.0540541 2:-0.109244 3:-0.823009 4:-0.785019 5:-0.831876 6:-0.843318 7:-0.791729 \n",
      "7 0:1 1:-0.0540541 2:-0.142857 3:-0.840708 4:-0.788206 5:-0.815064 6:-0.830151 7:-0.843548 \n",
      "8 0:1 1:-0.0405405 2:-0.0420168 3:-1 4:-0.698247 5:-0.723605 6:-0.774852 7:-0.773792 \n",
      "8 0:1 1:-0.0405405 2:-0.12605 3:-0.831858 4:-0.733664 5:-0.765972 6:-0.790652 7:-0.794718 \n",
      "7 0:1 1:-0.027027 2:-0.12605 3:-0.80531 4:-0.740393 5:-0.784129 6:-0.813035 7:-0.763827 \n",
      "9 0:1 1:-0.0135135 2:-0.0420168 3:-0.787611 4:-0.691163 5:-0.71688 6:-0.782752 7:-0.763827 \n",
      "9 0:1 1:0.0135135 2:-0.0756303 3:-0.814159 4:-0.68408 5:-0.721587 6:-0.766952 7:-0.763827 \n",
      "8 0:1 1:0.027027 2:-0.0252101 3:-0.814159 4:-0.717726 5:-0.780767 6:-0.802502 7:-0.751868 \n",
      "8 1:0.027027 2:0.0420168 3:-0.79646 4:-0.696476 5:-0.753867 6:-0.716919 7:-0.754858 \n",
      "8 0:1 1:0.027027 2:-0.0756303 3:-0.823009 4:-0.737914 5:-0.519839 6:-0.797235 7:-0.783757 \n",
      "7 0:1 1:0.0405405 2:0.0252101 3:-0.814159 4:-0.67133 5:-0.702085 6:-0.740619 7:-0.783757 \n",
      "9 0:1 1:0.0405405 2:-0.00840336 3:-0.814159 4:-0.738976 5:-0.789509 6:-0.798552 7:-0.775785 \n",
      "8 1:0.0405405 2:0.0420168 3:-0.778761 4:-0.662476 5:-0.724277 6:-0.726136 7:-0.721973 \n",
      "11 0:1 1:0.0540541 2:-0.0420168 3:-0.80531 4:-0.756331 5:-0.809684 6:-0.809085 7:-0.777778 \n",
      "8 0:1 1:0.0675676 2:0.0420168 3:-0.823009 4:-0.710289 5:-0.765972 6:-0.776169 7:-0.733931 \n",
      "8 0:1 1:0.0675676 2:0.00840336 3:-0.681416 4:-0.661413 5:-0.72495 6:-0.724819 7:-0.703039 \n",
      "8 0:1 1:0.0675676 2:0.00840336 3:-0.787611 4:-0.723039 5:-0.776732 6:-0.768269 7:-0.773792 \n",
      "8 0:1 1:0.0810811 2:0.00840336 3:-0.823009 4:-0.644767 5:-0.66039 6:-0.761685 7:-0.723966 \n",
      "8 0:1 1:0.0810811 2:0.092437 3:-0.787611 4:-0.689038 5:-0.761264 6:-0.768269 7:-0.703039 \n",
      "8 0:1 1:0.0810811 2:0.0252101 3:-0.80531 4:-0.652913 5:-0.717552 6:-0.711652 7:-0.704036 \n",
      "9 0:1 1:0.0945946 2:0.0588235 3:-0.778761 4:-0.616434 5:-0.673167 6:-0.735352 7:-0.674141 \n",
      "8 0:1 1:0.0945946 2:0.00840336 3:-0.79646 4:-0.666726 5:-0.723605 6:-0.706386 7:-0.73991 \n",
      "8 0:1 1:0.0945946 2:0.0420168 3:-0.823009 4:-0.674872 5:-0.704775 6:-0.781435 7:-0.733931 \n",
      "8 0:1 1:0.135135 2:0.00840336 3:-0.787611 4:-0.649726 5:-0.713517 6:-0.726136 7:-0.705032 \n",
      "7 0:1 1:0.135135 2:0.092437 3:-0.769912 4:-0.638392 5:-0.707465 6:-0.695853 7:-0.684106 \n",
      "8 0:-1 1:0.135135 2:0.142857 3:-0.787611 4:-0.609704 5:-0.700067 6:-0.639236 7:-0.670154 \n",
      "9 0:1 1:0.148649 2:0.092437 3:-0.761062 4:-0.580662 5:-0.605245 6:-0.727452 7:-0.691081 \n",
      "9 0:-1 1:0.148649 2:0.226891 3:-0.761062 4:-0.522224 5:-0.595158 6:-0.628703 7:-0.59143 \n",
      "9 0:1 1:0.148649 2:0.0756303 3:-0.743363 4:-0.590933 5:-0.679892 6:-0.639236 7:-0.63428 \n",
      "9 0:1 1:0.148649 2:0.193277 3:-0.752212 4:-0.532849 5:-0.653665 6:-0.573404 7:-0.612357 \n",
      "7 0:1 1:0.148649 2:0.0756303 3:-0.778761 4:-0.598017 5:-0.652993 6:-0.674786 7:-0.690085 \n",
      "9 0:1 1:0.148649 2:0.142857 3:-0.752212 4:-0.561183 5:-0.607935 6:-0.684003 7:-0.61435 \n",
      "8 0:1 1:0.162162 2:0.176471 3:-0.769912 4:-0.57535 5:-0.595831 6:-0.711652 7:-0.644245 \n",
      "7 0:1 1:0.162162 2:0.092437 3:-0.787611 4:-0.580662 5:-0.65232 6:-0.623436 7:-0.644245 \n",
      "8 0:1 1:0.162162 2:0.142857 3:-0.814159 4:-0.611121 5:-0.667787 6:-0.730086 7:-0.662182 \n",
      "7 0:1 1:0.189189 2:0.092437 3:-0.787611 4:-0.558704 5:-0.562206 6:-0.660303 7:-0.684106 \n",
      "13 0:1 1:0.189189 2:0.226891 3:-0.761062 4:-0.497786 5:-0.548083 6:-0.62212 7:-0.59442 \n",
      "9 0:1 1:0.189189 2:0.159664 3:-0.761062 4:-0.508057 5:-0.570948 6:-0.670836 7:-0.65421 \n",
      "10 0:1 1:0.202703 2:0.159664 3:-0.769912 4:-0.588808 5:-0.687962 6:-0.64187 7:-0.644245 \n",
      "9 0:1 1:0.202703 2:0.142857 3:-0.778761 4:-0.531787 5:-0.597176 6:-0.656353 7:-0.623318 \n",
      "9 0:1 1:0.216216 2:0.159664 3:-0.778761 4:-0.508057 5:-0.505044 6:-0.636603 7:-0.676134 \n",
      "9 0:-1 1:0.216216 2:0.226891 3:-0.725664 4:-0.404994 5:-0.425689 6:-0.628703 7:-0.595416 \n",
      "9 0:1 1:0.22973 2:0.210084 3:-0.769912 4:-0.509828 5:-0.476126 6:-0.709019 7:-0.670154 \n",
      "10 0:1 1:0.22973 2:0.226891 3:-0.725664 4:-0.427661 5:-0.365837 6:-0.709019 7:-0.620329 \n",
      "9 0:1 1:0.22973 2:0.210084 3:-0.80531 4:-0.594475 5:-0.661735 6:-0.676103 7:-0.626308 \n",
      "9 0:1 1:0.22973 2:0.243697 3:-0.769912 4:-0.457765 5:-0.437794 6:-0.684003 7:-0.584454 \n",
      "9 0:1 1:0.243243 2:0.159664 3:-0.761062 4:-0.574641 5:-0.61197 6:-0.682686 7:-0.696064 \n",
      "8 0:1 1:0.243243 2:0.210084 3:-0.734513 4:-0.593058 5:-0.517821 6:-0.64582 7:-0.554559 \n",
      "9 1:0.243243 2:0.193277 3:-0.769912 4:-0.495307 5:-0.550773 6:-0.62212 7:-0.589437 \n",
      "9 0:-1 1:0.243243 2:0.277311 3:-0.734513 4:-0.493536 5:-0.533961 6:-0.620803 7:-0.616343 \n",
      "10 1:0.256757 2:0.226891 3:-0.743363 4:-0.388348 5:-0.420982 6:-0.572087 7:-0.569507 \n",
      "9 0:1 1:0.256757 2:0.226891 3:-0.752212 4:-0.486807 5:-0.570276 6:-0.620803 7:-0.546587 \n",
      "8 0:1 1:0.27027 2:0.277311 3:-0.761062 4:-0.454932 5:-0.501009 6:-0.611587 7:-0.550573 \n",
      "9 1:0.27027 2:0.310924 3:-0.734513 4:-0.434744 5:-0.525891 6:-0.572087 7:-0.590433 \n",
      "10 0:1 1:0.27027 2:0.260504 3:-0.734513 4:-0.48539 5:-0.595158 6:-0.655036 7:-0.495765 \n",
      "8 0:1 1:0.27027 2:0.176471 3:-0.761062 4:-0.580308 5:-0.638198 6:-0.689269 7:-0.63428 \n",
      "11 0:1 1:0.283784 2:0.260504 3:-0.743363 4:-0.442182 5:-0.497646 6:-0.55102 7:-0.558545 \n",
      "9 1:0.283784 2:0.176471 3:-0.778761 4:-0.540287 5:-0.602555 6:-0.640553 7:-0.604385 \n",
      "9 0:-1 1:0.283784 2:0.260504 3:-0.734513 4:-0.381973 5:-0.445864 6:-0.499671 7:-0.508719 \n",
      "10 0:1 1:0.283784 2:0.277311 3:-0.752212 4:-0.467682 5:-0.559516 6:-0.591837 7:-0.539611 \n",
      "9 0:1 1:0.283784 2:0.243697 3:-0.761062 4:-0.483974 5:-0.554136 6:-0.59447 7:-0.574489 \n",
      "9 0:-1 1:0.297297 2:0.294118 3:-0.752212 4:-0.384806 5:-0.453934 6:-0.59052 7:-0.494768 \n",
      "10 0:1 1:0.297297 2:0.260504 3:-0.725664 4:-0.477599 5:-0.579691 6:-0.623436 7:-0.444943 \n",
      "9 0:1 1:0.297297 2:0.260504 3:-0.752212 4:-0.458474 5:-0.542703 6:-0.56682 7:-0.544594 \n",
      "9 0:1 1:0.297297 2:0.243697 3:-0.743363 4:-0.441473 5:-0.532616 6:-0.536537 7:-0.554559 \n",
      "9 0:1 1:0.310811 2:0.243697 3:-0.761062 4:-0.420223 5:-0.501681 6:-0.514154 7:-0.532636 \n",
      "9 0:1 1:0.310811 2:0.243697 3:-0.743363 4:-0.514078 5:-0.585071 6:-0.65767 7:-0.5715 \n",
      "10 1:0.310811 2:0.310924 3:-0.725664 4:-0.134408 5:-0.252858 6:-0.152074 7:-0.465869 \n",
      "9 0:1 1:0.310811 2:0.344538 3:-0.743363 4:-0.311493 5:-0.265636 6:-0.576037 7:-0.534629 \n",
      "11 0:1 1:0.324324 2:0.294118 3:-0.690265 4:-0.382681 5:-0.444519 6:-0.448321 7:-0.584454 \n",
      "8 1:0.324324 2:0.327731 3:-0.743363 4:-0.399681 5:-0.434432 6:-0.557604 7:-0.554559 \n",
      "8 0:-1 1:0.324324 2:0.310924 3:-0.734513 4:-0.437577 5:-0.512441 6:-0.516787 7:-0.566517 \n",
      "11 0:-1 1:0.324324 2:0.12605 3:-0.778761 4:-0.474411 5:-0.527909 6:-0.658986 7:-0.667165 \n",
      "10 0:1 1:0.337838 2:0.327731 3:-0.743363 4:-0.469453 5:-0.621385 6:-0.423305 7:-0.561535 \n",
      "8 0:1 1:0.337838 2:0.327731 3:-0.761062 4:-0.438994 5:-0.488231 6:-0.628703 7:-0.514699 \n",
      "9 1:0.337838 2:0.361345 3:-0.761062 4:-0.307597 5:-0.467384 6:-0.336406 7:-0.473842 \n",
      "10 0:-1 1:0.337838 2:0.277311 3:-0.699115 4:-0.383035 5:-0.487559 6:-0.519421 7:-0.463876 \n",
      "10 0:1 1:0.337838 2:0.294118 3:-0.769912 4:-0.458474 5:-0.534633 6:-0.531271 7:-0.600399 \n",
      "9 0:-1 1:0.337838 2:0.277311 3:-0.778761 4:-0.366389 5:-0.486214 6:-0.518104 7:-0.454908 \n",
      "10 1:0.351351 2:0.226891 3:-0.761062 4:-0.394369 5:-0.381305 6:-0.61422 7:-0.579472 \n",
      "9 1:0.351351 2:0.428571 3:-0.707965 4:-0.237825 5:-0.314055 6:-0.449638 7:-0.393124 \n",
      "10 0:-1 1:0.351351 2:0.361345 3:-0.725664 4:-0.369577 5:-0.407532 6:-0.537854 7:-0.564524 \n",
      "10 0:-1 1:0.364865 2:0.361345 3:-0.725664 4:0.0182398 5:-0.0981843 6:-0.282423 7:-0.414051 \n",
      "10 0:-1 1:0.364865 2:0.344538 3:-0.761062 4:-0.437932 5:-0.456624 6:-0.561554 7:-0.596413 \n",
      "8 1:0.364865 2:0.310924 3:-0.734513 4:-0.39366 5:-0.463349 6:-0.59052 7:-0.498754 \n",
      "9 0:-1 1:0.378378 2:0.378151 3:-0.725664 4:-0.353639 5:-0.388702 6:-0.483871 7:-0.534629 \n",
      "10 0:-1 1:0.378378 2:0.462185 3:-0.672566 4:-0.17195 5:-0.29926 6:-0.333772 7:-0.335326 \n",
      "10 0:1 1:0.378378 2:0.411765 3:-0.716814 4:-0.257305 5:-0.35575 6:-0.385122 7:-0.434978 \n",
      "10 0:-1 1:0.378378 2:0.361345 3:-0.707965 4:-0.212679 5:-0.217888 6:-0.383805 7:-0.456901 \n",
      "10 0:-1 1:0.378378 2:0.394958 3:-0.707965 4:-0.00336462 5:0.0746469 6:-0.398288 7:-0.415047 \n",
      "10 0:-1 1:0.378378 2:0.411765 3:-0.734513 4:-0.247034 5:-0.286483 6:-0.477288 7:-0.428002 \n",
      "8 0:-1 1:0.378378 2:0.327731 3:-0.681416 4:-0.435098 5:-0.549428 6:-0.512837 7:-0.530643 \n",
      "9 0:1 1:0.391892 2:0.310924 3:-0.761062 4:-0.454932 5:-0.560188 6:-0.541804 7:-0.544594 \n",
      "9 0:-1 1:0.391892 2:0.394958 3:-0.681416 4:-0.160616 5:-0.196369 6:-0.403555 7:-0.385152 \n",
      "10 0:-1 1:0.391892 2:0.344538 3:-0.725664 4:-0.374181 5:-0.479489 6:-0.506254 7:-0.454908 \n",
      "10 1:0.405405 2:0.378151 3:-0.734513 4:-0.307243 5:-0.447882 6:-0.485188 7:-0.553563 \n",
      "12 1:0.405405 2:0.378151 3:-0.725664 4:-0.274659 5:-0.376597 6:-0.706386 7:-0.395117 \n",
      "10 0:-1 1:0.418919 2:0.411765 3:-0.699115 4:-0.199929 5:-0.31809 6:-0.285056 7:-0.387145 \n",
      "11 0:-1 1:0.418919 2:0.428571 3:-0.725664 4:-0.283159 5:-0.394755 6:-0.504937 7:-0.355257 \n",
      "10 0:1 1:0.418919 2:0.411765 3:-0.734513 4:-0.208075 5:-0.241426 6:-0.352205 7:-0.464873 \n",
      "11 1:0.418919 2:0.378151 3:-0.725664 4:-0.264742 5:-0.361802 6:-0.342989 7:-0.444943 \n",
      "10 1:0.418919 2:0.344538 3:-0.743363 4:-0.371348 5:-0.437794 6:-0.549704 7:-0.466866 \n",
      "11 0:-1 1:0.418919 2:0.361345 3:-0.725664 4:-0.321764 5:-0.402152 6:-0.503621 7:-0.415047 \n",
      "11 0:1 1:0.432432 2:0.445378 3:-0.734513 4:-0.124491 5:-0.151984 6:-0.406188 7:-0.345291 \n",
      "10 0:-1 1:0.432432 2:0.462185 3:-0.752212 4:-0.31043 5:-0.437794 6:-0.458855 7:-0.375187 \n",
      "11 0:1 1:0.432432 2:0.277311 3:-0.769912 4:-0.362139 5:-0.420309 6:-0.54312 7:-0.484803 \n",
      "9 1:0.432432 2:0.411765 3:-0.690265 4:-0.239242 5:-0.378615 6:-0.423305 7:-0.335326 \n",
      "12 1:0.432432 2:0.394958 3:-0.716814 4:-0.23393 5:-0.274378 6:-0.418038 7:-0.454908 \n",
      "10 0:-1 1:0.445946 2:0.327731 3:-0.734513 4:-0.384452 5:-0.453934 6:-0.518104 7:-0.504733 \n",
      "12 0:-1 1:0.445946 2:0.428571 3:-0.707965 4:-0.120241 5:-0.147949 6:-0.324556 7:-0.395117 \n",
      "10 0:-1 1:0.445946 2:0.411765 3:-0.699115 4:-0.274305 5:-0.416274 6:-0.386438 7:-0.398107 \n",
      "9 0:1 1:0.445946 2:0.378151 3:-0.734513 4:-0.321055 5:-0.396772 6:-0.54707 7:-0.432985 \n",
      "10 0:-1 1:0.445946 2:0.428571 3:-0.699115 4:-0.196033 5:-0.387357 6:-0.237656 7:-0.31141 \n",
      "10 0:-1 1:0.445946 2:0.361345 3:-0.716814 4:-0.293076 5:-0.33692 6:-0.482554 7:-0.454908 \n",
      "10 1:0.459459 2:0.411765 3:-0.725664 4:-0.290243 5:-0.399462 6:-0.493088 7:-0.426009 \n",
      "10 0:-1 1:0.459459 2:0.394958 3:-0.707965 4:-0.202408 5:-0.400807 6:-0.423305 7:-0.325361 \n",
      "10 0:-1 1:0.459459 2:0.495798 3:-0.699115 4:-0.254826 5:-0.349697 6:-0.400922 7:-0.415047 \n",
      "12 1:0.459459 2:0.411765 3:-0.707965 4:-0.276784 5:-0.341627 6:-0.486504 7:-0.398107 \n",
      "10 0:-1 1:0.459459 2:0.411765 3:-0.699115 4:-0.2017 5:-0.356422 6:-0.206057 7:-0.405082 \n",
      "9 0:-1 1:0.459459 2:0.428571 3:-0.690265 4:-0.209492 5:-0.401479 6:-0.159974 7:-0.405082 \n",
      "10 1:0.459459 2:0.411765 3:-0.725664 4:-0.211617 5:-0.35037 6:-0.444371 7:-0.295466 \n",
      "10 0:-1 1:0.472973 2:0.529412 3:-0.690265 4:-0.0936781 5:-0.232683 6:-0.373272 7:-0.22571 \n",
      "12 0:-1 1:0.472973 2:0.478992 3:-0.681416 4:-0.112095 5:-0.225958 6:-0.332456 7:-0.295466 \n",
      "10 1:0.472973 2:0.495798 3:-0.734513 4:-0.0855321 5:-0.199731 6:-0.175774 7:-0.297459 \n",
      "10 1:0.472973 2:0.411765 3:-0.716814 4:-0.201346 5:-0.378615 6:-0.294273 7:-0.345291 \n",
      "9 0:-1 1:0.486486 2:0.344538 3:-0.699115 4:-0.234992 5:-0.334902 6:-0.383805 7:-0.375187 \n",
      "11 1:0.486486 2:0.512605 3:-0.690265 4:-0.186825 5:-0.264963 6:-0.327189 7:-0.396114 \n",
      "9 1:0.486486 2:0.546218 3:-0.716814 4:-0.106074 5:-0.232683 6:-0.142857 7:-0.363229 \n",
      "12 1:0.486486 2:0.428571 3:-0.725664 4:-0.148929 5:-0.212508 6:-0.371955 7:-0.368211 \n",
      "9 1:0.5 2:0.445378 3:-0.699115 4:-0.0660528 5:-0.201748 6:-0.0928242 7:-0.315396 \n",
      "11 0:1 1:0.5 2:0.512605 3:-0.681416 4:-0.100407 5:-0.190989 6:-0.22449 7:-0.375187 \n",
      "12 0:-1 1:0.5 2:0.445378 3:-0.743363 4:-0.249159 5:-0.320108 6:-0.531271 7:-0.332337 \n",
      "10 0:1 1:0.5 2:0.411765 3:-0.743363 4:-0.250221 5:-0.306658 6:-0.423305 7:-0.444943 \n",
      "12 0:-1 1:0.5 2:0.478992 3:-0.716814 4:-0.2272 5:-0.332885 6:-0.419355 7:-0.375187 \n",
      "10 0:-1 1:0.513514 2:0.462185 3:-0.716814 4:-0.221534 5:-0.28312 6:-0.510204 7:-0.3144 \n",
      "9 1:0.513514 2:0.495798 3:-0.707965 4:0.0324066 5:-0.0531271 6:-0.304806 7:-0.22571 \n",
      "9 1:0.513514 2:0.478992 3:-0.690265 4:-0.143616 5:-0.0504371 6:-0.283739 7:-0.359243 \n",
      "10 0:-1 1:0.513514 2:0.411765 3:-0.699115 4:-0.156012 5:-0.301278 6:-0.291639 7:-0.272546 \n",
      "10 0:-1 1:0.513514 2:0.529412 3:-0.725664 4:-0.302993 5:-0.456624 6:-0.407505 7:-0.385152 \n",
      "15 0:-1 1:0.527027 2:0.714286 3:-0.59292 4:0.0759695 5:-0.135171 6:-0.0217248 7:-0.193822 \n",
      "10 0:-1 1:0.527027 2:0.579832 3:-0.681416 4:-0.0710112 5:-0.347007 6:-0.21264 7:-0.190832 \n",
      "11 0:-1 1:0.540541 2:0.529412 3:-0.716814 4:-0.163095 5:-0.253531 6:-0.373272 7:-0.315396 \n",
      "10 0:-1 1:0.540541 2:0.495798 3:-0.654867 4:-0.00903134 5:-0.172159 6:-0.0757077 7:-0.260588 \n",
      "11 0:-1 1:0.540541 2:0.579832 3:-0.716814 4:0.0664069 5:0.00134499 6:-0.356155 7:-0.141006 \n",
      "11 1:0.554054 2:0.512605 3:-0.707965 4:-0.179741 5:-0.357767 6:-0.279789 7:-0.307424 \n",
      "11 1:0.554054 2:0.798319 3:-0.610619 4:0.175846 5:0.0342972 6:-0.00592495 7:-0.136024 \n",
      "11 0:-1 1:0.554054 2:0.579832 3:-0.690265 4:0.0869488 5:-0.0780094 6:-0.0572745 7:-0.195815 \n",
      "9 0:-1 1:0.554054 2:0.529412 3:-0.663717 4:0.0908447 5:-0.0390047 6:-0.0177749 7:-0.255605 \n",
      "10 1:0.554054 2:0.529412 3:-0.699115 4:0.108553 5:-0.0268998 6:-0.082291 7:-0.223717 \n",
      "10 1:0.567568 2:0.579832 3:-0.663717 4:-0.0384275 5:-0.242771 6:-0.154707 7:-0.208769 \n",
      "13 0:-1 1:0.567568 2:0.613445 3:-0.637168 4:0.16345 5:-0.0188299 6:-0.0546412 7:-0.0861983 \n",
      "10 1:0.567568 2:0.563025 3:-0.663717 4:0.0288649 5:-0.194351 6:0.0164582 7:-0.239661 \n",
      "10 0:-1 1:0.567568 2:0.462185 3:-0.690265 4:-0.0391358 5:-0.141224 6:-0.227123 7:-0.275536 \n",
      "10 1:0.581081 2:0.478992 3:-0.628319 4:0.0950947 5:-0.0275723 6:-0.0730744 7:-0.220727 \n",
      "10 1:0.581081 2:0.546218 3:-0.699115 4:-0.0543651 5:-0.174176 6:-0.178407 7:-0.290483 \n",
      "10 1:0.594595 2:0.596639 3:-0.681416 4:0.0547193 5:-0.147949 6:-0.100724 7:-0.136024 \n",
      "11 1:0.608108 2:0.596639 3:-0.60177 4:0.104657 5:-0.154001 6:0.281106 7:-0.260588 \n",
      "10 1:0.608108 2:0.512605 3:-0.690265 4:-0.282805 5:-0.412912 6:-0.287689 7:-0.256602 \n",
      "13 0:-1 1:0.621622 2:0.647059 3:-0.672566 4:0.229325 5:0.176866 6:-0.175774 7:-0.0682611 \n",
      "9 0:-1 1:0.648649 2:0.647059 3:-0.637168 4:0.268284 5:0.0941493 6:0.0941409 7:-0.0842053 \n",
      "11 1:0.675676 2:0.714286 3:-0.663717 4:0.247742 5:0.00268998 6:0.0493746 7:-0.0114599 \n",
      "12 1:0.689189 2:0.647059 3:-0.769912 4:0.100761 5:-0.0968393 6:-0.0164582 7:-0.61435 \n",
      "10 0:-1 1:0.702703 2:0.714286 3:-0.0884956 4:0.564016 5:0.488231 6:0.279789 7:0.017439 \n",
      "11 0:-1 1:0.702703 2:0.680672 3:-0.619469 4:0.515141 5:0.406187 6:0.00724161 7:0.0493274 \n",
      "12 1:0.702703 2:0.731092 3:-0.681416 4:0.0855321 5:0.289845 6:0.103357 7:-0.145989 \n",
      "11 1:0.716216 2:0.663866 3:-0.699115 4:0.141845 5:-0.00201748 6:-0.0928242 7:-0.106129 \n",
      "13 1:0.743243 2:0.747899 3:-0.699115 4:0.36816 5:0.226631 6:0.0230415 7:0.0134529 \n",
      "12 0:-1 1:0.743243 2:0.747899 3:-0.619469 4:0.537808 5:0.278413 6:0.483871 7:0.0662681 \n",
      "11 1:0.756757 2:0.831933 3:-0.646018 4:0.228971 5:-0.063887 6:-0.0572745 7:0.18286 \n",
      "11 1:0.77027 2:0.764706 3:-0.663717 4:0.229325 5:-0.0887693 6:0.142857 7:0.0333832 \n",
      "12 1:0.783784 2:0.714286 3:-0.637168 4:0.505578 5:0.27505 6:0.210007 7:0.123069 \n",
      "9 1:0.810811 2:0.731092 3:-0.619469 4:0.59235 5:0.554136 6:0.17314 7:0.109118 \n",
      "14 1:0.824324 2:0.865546 3:-0.584071 4:0.775456 5:0.655683 6:0.365372 7:0.216741 \n",
      "14 1:1 2:1 3:-0.557522 4:0.595892 5:0.196369 6:0.104674 7:0.586447 \n",
      "3 0:1 1:-0.824324 2:-0.831933 3:-0.938053 4:-0.9915 5:-0.993948 6:-0.994733 7:-0.995017 \n",
      "4 0:1 1:-0.581081 2:-0.630252 3:-0.893805 4:-0.964937 5:-0.97579 6:-0.963134 7:-0.931241 \n",
      "7 0:1 1:-0.216216 2:-0.294118 3:-0.761062 4:-0.844519 5:-0.860121 6:-0.877551 7:-0.882412 \n",
      "5 0:1 1:-0.216216 2:-0.327731 3:-0.858407 4:-0.860811 5:-0.895763 6:-0.910467 7:-0.897359 \n",
      "6 0:1 1:-0.202703 2:-0.277311 3:-0.831858 4:-0.837082 5:-0.823134 6:-0.893351 7:-0.880419 \n",
      "7 0:1 1:-0.189189 2:-0.243697 3:-0.849558 4:-0.777935 5:-0.749832 6:-0.880184 7:-0.869457 \n",
      "5 0:1 1:-0.162162 2:-0.176471 3:-0.840708 4:-0.826457 5:-0.836584 6:-0.885451 7:-0.86846 \n",
      "5 0:1 1:-0.135135 2:-0.193277 3:-0.840708 4:-0.787144 5:-0.809011 6:-0.826201 7:-0.850523 \n",
      "6 0:1 1:-0.121622 2:-0.210084 3:-0.80531 4:-0.768373 5:-0.748487 6:-0.881501 7:-0.838565 \n",
      "7 0:1 1:-0.121622 2:-0.176471 3:-0.840708 4:-0.802019 5:-0.841964 6:-0.840685 7:-0.843548 \n",
      "8 0:1 1:-0.108108 2:-0.142857 3:-0.831858 4:-0.75881 5:-0.761264 6:-0.832785 7:-0.832586 \n",
      "7 0:1 1:-0.108108 2:-0.210084 3:-0.840708 4:-0.80131 5:-0.850706 6:-0.803818 7:-0.840558 \n",
      "6 0:1 1:-0.108108 2:-0.176471 3:-0.814159 4:-0.786081 5:-0.805649 6:-0.840685 7:-0.842551 \n",
      "5 0:1 1:-0.0945946 2:-0.109244 3:-0.831858 4:-0.795644 5:-0.811701 6:-0.835418 7:-0.857499 \n",
      "9 0:-1 1:-0.0810811 2:-0.12605 3:-0.79646 4:-0.725518 5:-0.73033 6:-0.830151 7:-0.797708 \n",
      "6 0:1 1:-0.0540541 2:-0.0420168 3:-0.814159 4:-0.725872 5:-0.73033 6:-0.763002 7:-0.827603 \n",
      "6 0:1 1:-0.0405405 2:-0.0420168 3:-0.814159 4:-0.689393 5:-0.680565 6:-0.805135 7:-0.788739 \n",
      "5 0:1 1:-0.0135135 2:-0.0420168 3:-0.814159 4:-0.740039 5:-0.780767 6:-0.790652 7:-0.800698 \n",
      "8 0:-1 1:-0.0135135 2:-0.109244 3:-0.787611 4:-0.678059 5:-0.67384 6:-0.759052 7:-0.798705 \n",
      "9 0:1 1:-0.0135135 2:0.0420168 3:-0.80531 4:-0.685143 5:-0.714862 6:-0.766952 7:-0.776781 \n",
      "6 0:-1 1:0.0135135 2:-0.0588235 3:-0.778761 4:-0.684434 5:-0.710155 6:-0.669519 7:-0.783757 \n",
      "7 0:1 1:0.027027 2:-0.0588235 3:-0.761062 4:-0.646538 5:-0.632818 6:-0.739302 7:-0.790732 \n",
      "8 0:1 1:0.0405405 2:0.00840336 3:-0.80531 4:-0.69258 5:-0.735709 6:-0.748519 7:-0.753861 \n",
      "9 0:1 1:0.0675676 2:-0.0252101 3:-0.752212 4:-0.674517 5:-0.693342 6:-0.710336 7:-0.771799 \n",
      "6 0:1 1:0.0675676 2:-0.00840336 3:-0.778761 4:-0.695768 5:-0.745797 6:-0.694536 7:-0.768809 \n",
      "6 0:1 1:0.0675676 2:0.00840336 3:-0.787611 4:-0.740393 5:-0.831876 6:-0.781435 7:-0.730942 \n",
      "8 0:-1 1:0.0810811 2:0.0588235 3:-0.778761 4:-0.541704 5:-0.534633 6:-0.643186 7:-0.71998 \n",
      "7 0:1 1:0.0810811 2:0.0420168 3:-0.79646 4:-0.676288 5:-0.70881 6:-0.756419 7:-0.770802 \n",
      "7 1:0.0810811 2:0.0420168 3:-0.79646 4:-0.600496 5:-0.623403 6:-0.693219 7:-0.736921 \n",
      "7 0:1 1:0.0945946 2:0.0252101 3:-0.778761 4:-0.617496 5:-0.62542 6:-0.731402 7:-0.710015 \n",
      "6 0:1 1:0.108108 2:0.092437 3:-0.787611 4:-0.666726 5:-0.72226 6:-0.718236 7:-0.710015 \n",
      "8 0:-1 1:0.108108 2:0.12605 3:-0.849558 4:-0.545599 5:-0.605245 6:-0.730086 7:-0.608371 \n",
      "6 0:-1 1:0.108108 2:0.0588235 3:-0.769912 4:-0.628829 5:-0.667115 6:-0.724819 7:-0.693074 \n",
      "7 1:0.135135 2:0.092437 3:-0.787611 4:-0.595537 5:-0.64425 6:-0.662936 7:-0.715994 \n",
      "9 0:-1 1:0.162162 2:0.109244 3:-0.814159 4:-0.610058 5:-0.679892 6:-0.673469 7:-0.693074 \n",
      "9 1:0.162162 2:0.092437 3:-0.761062 4:-0.515849 5:-0.515804 6:-0.589203 7:-0.682113 \n",
      "8 0:1 1:0.189189 2:0.142857 3:-0.778761 4:-0.607579 5:-0.63887 6:-0.748519 7:-0.664175 \n",
      "8 0:-1 1:0.189189 2:0.243697 3:-0.743363 4:-0.338056 5:-0.332885 6:-0.524687 7:-0.567514 \n",
      "7 0:1 1:0.189189 2:0.159664 3:-0.778761 4:-0.602975 5:-0.665098 6:-0.673469 7:-0.664175 \n",
      "8 0:-1 1:0.202703 2:0.159664 3:-0.778761 4:-0.605454 5:-0.659718 6:-0.635286 7:-0.706029 \n",
      "8 0:-1 1:0.216216 2:0.159664 3:-0.752212 4:-0.491057 5:-0.505716 6:-0.619487 7:-0.6572 \n",
      "9 0:1 1:0.22973 2:0.260504 3:-0.769912 4:-0.502391 5:-0.535978 6:-0.628703 7:-0.626308 \n",
      "8 0:-1 1:0.22973 2:0.159664 3:-0.778761 4:-0.464849 5:-0.466039 6:-0.603687 7:-0.65421 \n",
      "9 1:0.27027 2:0.193277 3:-0.752212 4:-0.47689 5:-0.521856 6:-0.533904 7:-0.598406 \n",
      "8 1:0.283784 2:0.260504 3:-0.752212 4:-0.40641 5:-0.496974 6:-0.427255 7:-0.615346 \n",
      "9 0:-1 1:0.283784 2:0.243697 3:-0.716814 4:-0.439702 5:-0.540013 6:-0.465438 7:-0.574489 \n",
      "8 1:0.310811 2:0.260504 3:-0.734513 4:-0.381973 5:-0.392065 6:-0.577354 7:-0.564524 \n",
      "9 1:0.310811 2:0.277311 3:-0.734513 4:-0.384098 5:-0.361802 6:-0.518104 7:-0.63727 \n",
      "8 0:-1 1:0.337838 2:0.310924 3:-0.734513 4:-0.30193 5:-0.32347 6:-0.456221 7:-0.506726 \n",
      "9 0:-1 1:0.351351 2:0.378151 3:-0.734513 4:-0.236409 5:-0.201076 6:-0.457538 7:-0.52865 \n",
      "8 0:-1 1:0.351351 2:0.361345 3:-0.707965 4:-0.352931 5:-0.462677 6:-0.352205 7:-0.527653 \n",
      "9 1:0.364865 2:0.361345 3:-0.690265 4:-0.1762 5:-0.127102 6:-0.420671 7:-0.394121 \n",
      "9 1:0.364865 2:0.277311 3:-0.752212 4:-0.326368 5:-0.362475 6:-0.431205 7:-0.585451 \n",
      "8 0:-1 1:0.378378 2:0.344538 3:-0.734513 4:-0.35966 5:-0.450572 6:-0.395655 7:-0.537618 \n",
      "8 0:-1 1:0.391892 2:0.294118 3:-0.734513 4:-0.383389 5:-0.480834 6:-0.435155 7:-0.514699 \n",
      "9 1:0.391892 2:0.378151 3:-0.734513 4:-0.186117 5:-0.176866 6:-0.371955 7:-0.501744 \n",
      "8 1:0.391892 2:0.361345 3:-0.743363 4:-0.299805 5:-0.392065 6:-0.420671 7:-0.454908 \n",
      "9 1:0.405405 2:0.344538 3:-0.716814 4:-0.264742 5:-0.39341 6:-0.302172 7:-0.428999 \n",
      "8 0:-1 1:0.418919 2:0.344538 3:-0.725664 4:-0.332035 5:-0.414257 6:-0.452271 7:-0.504733 \n",
      "9 0:-1 1:0.418919 2:0.378151 3:-0.646018 4:-0.109616 5:-0.139879 6:-0.478604 7:-0.29148 \n",
      "11 1:0.432432 2:0.445378 3:-0.707965 4:-0.32743 5:-0.391392 6:-0.493088 7:-0.451918 \n",
      "9 1:0.432432 2:0.445378 3:-0.716814 4:-0.150699 5:-0.440484 6:-0.24424 7:-0.24564 \n",
      "9 1:0.432432 2:0.546218 3:-0.699115 4:-0.0883655 5:-0.194351 6:-0.225806 7:-0.342302 \n",
      "11 1:0.445946 2:0.445378 3:-0.699115 4:-0.221888 5:-0.312038 6:-0.398288 7:-0.395117 \n",
      "8 0:1 1:0.459459 2:0.411765 3:-0.769912 4:-0.40464 5:-0.526564 6:-0.497038 7:-0.50274 \n",
      "10 0:-1 1:0.472973 2:0.445378 3:-0.725664 4:-0.258367 5:-0.37996 6:-0.393022 7:-0.504733 \n",
      "8 1:0.472973 2:0.277311 3:-0.725664 4:-0.284576 5:-0.359785 6:-0.379855 7:-0.454908 \n",
      "9 0:-1 1:0.472973 2:0.428571 3:-0.707965 4:-0.241721 5:-0.353732 6:-0.382488 7:-0.381166 \n",
      "10 0:-1 1:0.486486 2:0.563025 3:-0.690265 4:-0.00230211 5:-0.071957 6:-0.15339 7:-0.312407 \n",
      "11 0:-1 1:0.486486 2:0.394958 3:-0.681416 4:-0.196742 5:-0.394755 6:-0.146807 7:-0.395117 \n",
      "9 0:-1 1:0.5 2:0.394958 3:-0.743363 4:-0.221888 5:-0.30195 6:-0.316656 7:-0.452915 \n",
      "10 1:0.5 2:0.495798 3:-0.690265 4:-0.214804 5:-0.373235 6:-0.295589 7:-0.347285 \n",
      "9 0:-1 1:0.5 2:0.344538 3:-0.734513 4:-0.199929 5:-0.354405 6:-0.278473 7:-0.395117 \n",
      "11 0:-1 1:0.5 2:0.428571 3:-0.734513 4:-0.101116 5:-0.112979 6:-0.362739 7:-0.385152 \n",
      "10 1:0.5 2:0.462185 3:-0.60177 4:-0.0550735 5:-0.0860794 6:-0.319289 7:-0.356253 \n",
      "10 1:0.513514 2:0.512605 3:-0.743363 4:-0.197804 5:-0.322125 6:-0.302172 7:-0.375187 \n",
      "8 0:-1 1:0.513514 2:0.529412 3:-0.672566 4:-0.074907 5:-0.26967 6:-0.163924 7:-0.251619 \n",
      "9 1:0.527027 2:0.546218 3:-0.637168 4:0.0848238 5:-0.108944 6:-0.120474 7:-0.200797 \n",
      "10 1:0.540541 2:0.546218 3:-0.690265 4:0.0936781 5:-0.0551446 6:-0.040158 7:-0.175884 \n",
      "11 0:-1 1:0.540541 2:0.529412 3:-0.725664 4:0.0887197 5:-0.139879 6:-0.0572745 7:-0.145989 \n",
      "11 1:0.540541 2:0.512605 3:-0.707965 4:-0.0678236 5:-0.261601 6:-0.207373 7:-0.335326 \n",
      "10 1:0.554054 2:0.647059 3:-0.690265 4:0.0784487 5:-0.207801 6:-0.142857 7:-0.0164425 \n",
      "10 0:-1 1:0.554054 2:0.546218 3:-0.690265 4:0.0370108 5:-0.0907868 6:-0.199473 7:-0.20578 \n",
      "9 1:0.554054 2:0.495798 3:-0.716814 4:-0.0221356 5:-0.0571621 6:-0.20079 7:-0.366218 \n",
      "8 0:-1 1:0.554054 2:0.445378 3:-0.752212 4:-0.169116 5:-0.362475 6:-0.360105 7:-0.574489 \n",
      "11 1:0.567568 2:0.630252 3:-0.619469 4:0.100407 5:-0.066577 6:-0.221856 7:-0.118087 \n",
      "11 0:-1 1:0.567568 2:0.529412 3:-0.619469 4:0.261909 5:0.193679 6:0.0770244 7:-0.166916 \n",
      "10 0:-1 1:0.581081 2:0.512605 3:-0.707965 4:-0.0281565 5:-0.209146 6:-0.0770244 7:-0.315396 \n",
      "10 1:0.594595 2:0.546218 3:-0.681416 4:-0.0175314 5:-0.201748 6:-0.148124 7:-0.215745 \n",
      "11 0:-1 1:0.608108 2:0.647059 3:-0.646018 4:0.204533 5:0.119032 6:-0.0164582 7:-0.18585 \n",
      "10 0:-1 1:0.608108 2:0.529412 3:-0.690265 4:0.0798654 5:-0.125757 6:0.17709 7:-0.315396 \n",
      "10 0:-1 1:0.608108 2:0.495798 3:-0.663717 4:0.0745529 5:-0.172831 6:0.0204082 7:-0.175884 \n",
      "12 1:0.635135 2:0.495798 3:-0.672566 4:0.231804 5:0.0295898 6:-0.144174 7:-0.0692576 \n",
      "11 0:-1 1:0.635135 2:0.546218 3:-0.699115 4:0.140074 5:0.130464 6:-0.195523 7:-0.215745 \n",
      "12 0:-1 1:0.662162 2:0.579832 3:-0.646018 4:0.261201 5:0.231338 6:-0.125741 7:-0.0842053 \n",
      "11 1:0.689189 2:0.663866 3:-0.699115 4:0.191429 5:0.0121049 6:-0.14154 7:-0.365222 \n",
      "11 0:-1 1:0.689189 2:0.680672 3:-0.646018 4:0.31468 5:-0.0195024 6:-0.036208 7:0.18286 \n",
      "10 0:-1 1:0.702703 2:0.697479 3:-0.707965 4:0.185054 5:0.070612 6:0.0770244 7:-0.229696 \n",
      "12 0:-1 1:0.743243 2:0.714286 3:-0.646018 4:0.489995 5:0.36651 6:-0.0454246 7:-0.0184355 \n",
      "13 0:-1 1:0.756757 2:0.747899 3:-0.575221 4:0.564016 5:0.815736 6:0.0862409 7:-0.00348779 \n",
      "10 0:-1 1:0.797297 2:0.731092 3:-0.681416 4:0.324951 5:0.224613 6:0.123107 7:-0.114101 \n",
      "11 0:-1 1:0.824324 2:0.663866 3:-0.681416 4:0.339472 5:0.265636 6:0.044108 7:-0.116094 \n",
      "5 0:1 1:-0.635135 2:-0.613445 3:-0.920354 4:-0.96777 5:-0.97579 6:-0.97235 7:-0.97708 \n",
      "5 0:1 1:-0.432432 2:-0.478992 3:-0.902655 4:-0.929874 5:-0.945528 6:-0.95655 7:-0.936223 \n",
      "6 0:1 1:-0.405405 2:-0.462185 3:-0.876106 4:-0.915707 5:-0.93813 6:-0.9605 7:-0.922272 \n",
      "6 0:1 1:-0.391892 2:-0.411765 3:-0.849558 4:-0.918541 5:-0.934095 6:-0.955234 7:-0.920279 \n",
      "6 0:1 1:-0.378378 2:-0.428571 3:-0.840708 4:-0.897645 5:-0.916611 6:-0.911784 7:-0.920279 \n",
      "5 0:1 1:-0.297297 2:-0.327731 3:-0.858407 4:-0.882415 5:-0.895091 6:-0.907834 7:-0.903338 \n",
      "5 0:1 1:-0.256757 2:-0.310924 3:-0.867257 4:-0.873915 5:-0.880296 6:-0.936801 7:-0.893373 \n",
      "6 0:1 1:-0.243243 2:-0.277311 3:-0.867257 4:-0.875686 5:-0.895091 6:-0.918367 7:-0.895366 \n",
      "7 0:1 1:-0.243243 2:-0.310924 3:-0.840708 4:-0.860811 5:-0.905178 6:-0.870968 7:-0.887394 \n",
      "8 0:1 1:-0.22973 2:-0.277311 3:-0.831858 4:-0.859749 5:-0.903161 6:-0.853851 7:-0.881415 \n",
      "7 0:1 1:-0.22973 2:-0.260504 3:-0.867257 4:-0.845228 5:-0.868863 6:-0.885451 7:-0.87145 \n",
      "7 0:1 1:-0.22973 2:-0.294118 3:-0.867257 4:-0.870728 5:-0.889711 6:-0.905201 7:-0.893373 \n",
      "7 0:1 1:-0.216216 2:-0.277311 3:-0.849558 4:-0.843811 5:-0.875588 6:-0.863068 7:-0.87145 \n",
      "6 0:1 1:-0.202703 2:-0.277311 3:-0.831858 4:-0.847353 5:-0.870881 6:-0.880184 7:-0.873443 \n",
      "7 0:1 1:-0.189189 2:-0.243697 3:-0.858407 4:-0.848061 5:-0.875588 6:-0.757735 7:-0.863478 \n",
      "7 0:1 1:-0.175676 2:-0.226891 3:-0.831858 4:-0.82929 5:-0.880968 6:-0.826201 7:-0.853513 \n",
      "6 0:1 1:-0.175676 2:-0.210084 3:-0.823009 4:-0.83354 5:-0.856086 6:-0.897301 7:-0.839562 \n",
      "8 0:1 1:-0.162162 2:-0.210084 3:-0.840708 4:-0.833894 5:-0.866846 6:-0.868334 7:-0.85152 \n",
      "7 0:1 1:-0.162162 2:-0.243697 3:-0.831858 4:-0.819373 5:-0.841291 6:-0.845951 7:-0.863478 \n",
      "8 0:1 1:-0.162162 2:-0.176471 3:-0.840708 4:-0.783248 5:-0.796234 6:-0.853851 7:-0.836572 \n",
      "7 0:1 1:-0.148649 2:-0.176471 3:-0.840708 4:-0.822915 5:-0.858104 6:-0.861751 7:-0.843548 \n",
      "7 0:1 1:-0.148649 2:-0.226891 3:-0.823009 4:-0.802373 5:-0.829859 6:-0.838051 7:-0.849527 \n",
      "7 0:1 1:-0.148649 2:-0.210084 3:-0.823009 4:-0.843811 5:-0.873571 6:-0.878868 7:-0.857499 \n",
      "8 0:1 1:-0.0945946 2:-0.176471 3:-0.840708 4:-0.786081 5:-0.827841 6:-0.814352 7:-0.812656 \n",
      "8 0:1 1:-0.0945946 2:-0.176471 3:-0.840708 4:-0.803081 5:-0.811701 6:-0.849901 7:-0.853513 \n",
      "7 0:1 1:-0.0810811 2:-0.092437 3:-0.823009 4:-0.779706 5:-0.814391 6:-0.836735 7:-0.810663 \n",
      "8 0:1 1:-0.0540541 2:-0.092437 3:-0.80531 4:-0.776873 5:-0.819771 6:-0.874918 7:-0.823617 \n",
      "7 0:1 1:-0.0540541 2:-0.12605 3:-0.858407 4:-0.786789 5:-0.825151 6:-0.847268 7:-0.813652 \n",
      "7 0:1 1:-0.027027 2:-0.0588235 3:-0.823009 4:-0.768018 5:-0.827841 6:-0.816985 7:-0.783757 \n",
      "6 0:1 1:-0.027027 2:-0.092437 3:-0.80531 4:-0.741456 5:-0.786819 6:-0.790652 7:-0.793722 \n",
      "8 0:1 1:0.0135135 2:-0.0420168 3:-0.831858 4:-0.77156 5:-0.815064 6:-0.832785 7:-0.793722 \n",
      "7 0:1 1:0.0135135 2:-0.0588235 3:-0.80531 4:-0.704268 5:-0.757902 6:-0.777485 7:-0.734928 \n",
      "10 0:1 1:0.027027 2:0.0252101 3:-0.79646 4:-0.677705 5:-0.720915 6:-0.776169 7:-0.710015 \n",
      "7 0:1 1:0.0405405 2:-0.00840336 3:-0.80531 4:-0.71808 5:-0.764627 6:-0.782752 7:-0.76283 \n",
      "7 0:1 1:0.0405405 2:0.00840336 3:-0.80531 4:-0.700018 5:-0.73033 6:-0.788018 7:-0.743896 \n",
      "9 0:1 1:0.0540541 2:0.0588235 3:-0.787611 4:-0.692226 5:-0.748487 6:-0.786702 7:-0.710015 \n",
      "8 0:1 1:0.0540541 2:-0.0252101 3:-0.80531 4:-0.723039 5:-0.755884 6:-0.807768 7:-0.763827 \n",
      "8 0:1 1:0.0675676 2:0.00840336 3:-0.778761 4:-0.647955 5:-0.718897 6:-0.741935 7:-0.694071 \n",
      "8 0:1 1:0.0810811 2:0.0252101 3:-0.743363 4:-0.553391 5:-0.621385 6:-0.640553 7:-0.624315 \n",
      "7 0:-1 1:0.0810811 2:0.0252101 3:-0.823009 4:-0.697893 5:-0.737054 6:-0.740619 7:-0.779771 \n",
      "8 0:1 1:0.0810811 2:0.0252101 3:-0.778761 4:-0.653976 5:-0.725622 6:-0.65767 7:-0.753861 \n",
      "9 0:1 1:0.0945946 2:0.0588235 3:-0.778761 4:-0.665663 5:-0.760592 6:-0.728769 7:-0.65421 \n",
      "8 0:1 1:0.0945946 2:0.0588235 3:-0.787611 4:-0.621746 5:-0.663753 6:-0.701119 7:-0.704036 \n",
      "8 0:-1 1:0.0945946 2:0.00840336 3:-0.716814 4:-0.672747 5:-0.704102 6:-0.722186 7:-0.526657 \n",
      "7 0:1 1:0.108108 2:0.0756303 3:-0.769912 4:-0.574641 5:-0.60659 6:-0.662936 7:-0.684106 \n",
      "8 0:1 1:0.121622 2:0.0756303 3:-0.79646 4:-0.674517 5:-0.726967 6:-0.752469 7:-0.717987 \n",
      "8 0:1 1:0.121622 2:0.159664 3:-0.761062 4:-0.559412 5:-0.593141 6:-0.662936 7:-0.666168 \n",
      "8 0:1 1:0.135135 2:0.0588235 3:-0.778761 4:-0.667434 5:-0.72226 6:-0.761685 7:-0.704036 \n",
      "8 0:1 1:0.135135 2:0.159664 3:-0.814159 4:-0.574996 5:-0.664425 6:-0.668203 7:-0.624315 \n",
      "8 0:1 1:0.148649 2:0.159664 3:-0.787611 4:-0.565079 5:-0.650303 6:-0.624753 7:-0.617339 \n",
      "8 0:1 1:0.148649 2:0.12605 3:-0.787611 4:-0.5796 5:-0.67115 6:-0.61422 7:-0.658196 \n",
      "9 0:1 1:0.148649 2:0.0756303 3:-0.752212 4:-0.605454 5:-0.68191 6:-0.64582 7:-0.666168 \n",
      "7 0:1 1:0.175676 2:0.142857 3:-0.769912 4:-0.574641 5:-0.623403 6:-0.624753 7:-0.68012 \n",
      "6 1:0.189189 2:0.0756303 3:-0.80531 4:-0.571808 5:-0.597176 6:-0.656353 7:-0.704036 \n",
      "9 0:1 1:0.189189 2:0.0252101 3:-0.778761 4:-0.666726 5:-0.757229 6:-0.672153 7:-0.727952 \n",
      "10 0:1 1:0.189189 2:-0.00840336 3:-0.814159 4:-0.665309 5:-0.714862 6:-0.677419 7:-0.748879 \n",
      "8 0:1 1:0.189189 2:0.142857 3:-0.778761 4:-0.531433 5:-0.570948 6:-0.632653 7:-0.664175 \n",
      "8 0:1 1:0.189189 2:0.12605 3:-0.778761 4:-0.597308 5:-0.681237 6:-0.668203 7:-0.63428 \n",
      "9 0:1 1:0.202703 2:0.193277 3:-0.743363 4:-0.543829 5:-0.602555 6:-0.581303 7:-0.674141 \n",
      "7 0:1 1:0.202703 2:0.12605 3:-0.769912 4:-0.608642 5:-0.6846 6:-0.712969 7:-0.625311 \n",
      "8 0:-1 1:0.216216 2:0.210084 3:-0.743363 4:-0.402869 5:-0.527236 6:-0.57077 7:-0.429995 \n",
      "9 0:1 1:0.216216 2:0.12605 3:-0.787611 4:-0.531078 5:-0.582381 6:-0.61422 7:-0.648231 \n",
      "8 0:1 1:0.216216 2:0.092437 3:-0.761062 4:-0.565787 5:-0.650303 6:-0.58262 7:-0.65421 \n",
      "7 0:1 1:0.216216 2:0.159664 3:-0.752212 4:-0.538162 5:-0.591123 6:-0.579987 7:-0.666168 \n",
      "9 0:-1 1:0.216216 2:0.159664 3:-0.725664 4:-0.50062 5:-0.622058 6:-0.57867 7:-0.554559 \n",
      "7 0:1 1:0.22973 2:0.226891 3:-0.787611 4:-0.578891 5:-0.6577 6:-0.63002 7:-0.650224 \n",
      "10 0:1 1:0.22973 2:0.260504 3:-0.761062 4:-0.55835 5:-0.671822 6:-0.61817 7:-0.577479 \n",
      "9 0:1 1:0.22973 2:0.159664 3:-0.743363 4:-0.608288 5:-0.741089 6:-0.65767 7:-0.61435 \n",
      "11 0:1 1:0.22973 2:0.226891 3:-0.769912 4:-0.40889 5:-0.497646 6:-0.561554 7:-0.506726 \n",
      "7 0:1 1:0.243243 2:0.159664 3:-0.769912 4:-0.536037 5:-0.62004 6:-0.57472 7:-0.65421 \n",
      "9 0:1 1:0.256757 2:0.260504 3:-0.699115 4:-0.409244 5:-0.500336 6:-0.523371 7:-0.524664 \n",
      "9 0:1 1:0.256757 2:0.243697 3:-0.752212 4:-0.475828 5:-0.570948 6:-0.63397 7:-0.504733 \n",
      "9 0:1 1:0.256757 2:0.260504 3:-0.752212 4:-0.420931 5:-0.472091 6:-0.54707 7:-0.54559 \n",
      "10 0:-1 1:0.256757 2:0.344538 3:-0.752212 4:-0.31291 5:-0.437794 6:-0.329822 7:-0.466866 \n",
      "9 0:1 1:0.256757 2:0.226891 3:-0.752212 4:-0.556933 5:-0.664425 6:-0.691903 7:-0.534629 \n",
      "10 0:1 1:0.256757 2:0.243697 3:-0.769912 4:-0.491057 5:-0.6039 6:-0.556287 7:-0.554559 \n",
      "8 0:1 1:0.256757 2:0.243697 3:-0.761062 4:-0.515495 5:-0.533961 6:-0.59447 7:-0.578475 \n",
      "9 0:1 1:0.27027 2:0.159664 3:-0.769912 4:-0.515495 5:-0.559516 6:-0.61817 7:-0.644245 \n",
      "8 0:1 1:0.27027 2:0.0756303 3:-0.787611 4:-0.616788 5:-0.68191 6:-0.697169 7:-0.659193 \n",
      "8 0:1 1:0.27027 2:0.226891 3:-0.778761 4:-0.493536 5:-0.519839 6:-0.706386 7:-0.564524 \n",
      "10 0:-1 1:0.283784 2:0.277311 3:-0.752212 4:-0.461307 5:-0.561533 6:-0.557604 7:-0.486796 \n",
      "9 0:1 1:0.283784 2:0.243697 3:-0.734513 4:-0.548787 5:-0.639543 6:-0.647136 7:-0.570503 \n",
      "12 0:1 1:0.283784 2:0.226891 3:-0.761062 4:-0.423411 5:-0.464022 6:-0.61027 7:-0.544594 \n",
      "8 0:1 1:0.283784 2:0.210084 3:-0.743363 4:-0.447848 5:-0.499664 6:-0.579987 7:-0.561535 \n",
      "8 0:1 1:0.283784 2:0.243697 3:-0.734513 4:-0.458474 5:-0.545393 6:-0.537854 7:-0.584454 \n",
      "9 0:1 1:0.297297 2:0.142857 3:-0.769912 4:-0.605808 5:-0.702757 6:-0.673469 7:-0.664175 \n",
      "8 0:1 1:0.297297 2:0.277311 3:-0.752212 4:-0.459536 5:-0.470746 6:-0.60632 7:-0.592427 \n",
      "11 0:1 1:0.297297 2:0.361345 3:-0.743363 4:-0.363556 5:-0.484196 6:-0.585253 7:-0.451918 \n",
      "10 0:1 1:0.310811 2:0.310924 3:-0.734513 4:-0.418806 5:-0.505716 6:-0.508887 7:-0.532636 \n",
      "8 0:1 1:0.310811 2:0.294118 3:-0.769912 4:-0.488932 5:-0.531944 6:-0.608953 7:-0.604385 \n",
      "10 0:-1 1:0.310811 2:0.243697 3:-0.761062 4:-0.400035 5:-0.562206 6:-0.419355 7:-0.475835 \n",
      "9 0:1 1:0.324324 2:0.226891 3:-0.725664 4:-0.47512 5:-0.584398 6:-0.511521 7:-0.542601 \n",
      "9 1:0.324324 2:0.294118 3:-0.734513 4:-0.390119 5:-0.416274 6:-0.608953 7:-0.464873 \n",
      "9 0:-1 1:0.324324 2:0.294118 3:-0.778761 4:-0.433327 5:-0.517821 6:-0.520737 7:-0.574489 \n",
      "10 0:-1 1:0.324324 2:0.260504 3:-0.734513 4:-0.412786 5:-0.430397 6:-0.544437 7:-0.566517 \n",
      "8 1:0.337838 2:0.327731 3:-0.761062 4:-0.448557 5:-0.551446 6:-0.514154 7:-0.584454 \n",
      "9 0:-1 1:0.337838 2:0.327731 3:-0.752212 4:-0.438286 5:-0.546066 6:-0.611587 7:-0.514699 \n",
      "9 0:1 1:0.337838 2:0.277311 3:-0.699115 4:-0.400744 5:-0.463349 6:-0.564187 7:-0.504733 \n",
      "10 0:1 1:0.337838 2:0.260504 3:-0.743363 4:-0.411369 5:-0.525219 6:-0.62212 7:-0.441953 \n",
      "9 0:1 1:0.337838 2:0.310924 3:-0.725664 4:-0.387285 5:-0.503026 6:-0.552337 7:-0.444943 \n",
      "12 0:1 1:0.337838 2:0.310924 3:-0.743363 4:-0.47689 5:-0.589778 6:-0.548387 7:-0.639263 \n",
      "8 0:-1 1:0.351351 2:0.344538 3:-0.707965 4:-0.387285 5:-0.494956 6:-0.526004 7:-0.468859 \n",
      "10 0:1 1:0.351351 2:0.243697 3:-0.761062 4:-0.437223 5:-0.511769 6:-0.485188 7:-0.526657 \n",
      "10 1:0.351351 2:0.394958 3:-0.725664 4:-0.210908 5:-0.316745 6:-0.374589 7:-0.325361 \n",
      "9 0:1 1:0.351351 2:0.327731 3:-0.778761 4:-0.448911 5:-0.560861 6:-0.506254 7:-0.534629 \n",
      "10 0:-1 1:0.351351 2:0.394958 3:-0.672566 4:-0.303701 5:-0.497646 6:-0.428571 7:-0.295466 \n",
      "8 1:0.351351 2:0.378151 3:-0.654867 4:-0.295555 5:-0.440484 6:-0.350889 7:-0.0662681 \n",
      "9 0:1 1:0.351351 2:0.310924 3:-0.699115 4:-0.433682 5:-0.533961 6:-0.615537 7:-0.504733 \n",
      "10 0:1 1:0.351351 2:0.327731 3:-0.761062 4:-0.429786 5:-0.515131 6:-0.537854 7:-0.496761 \n",
      "8 1:0.351351 2:0.277311 3:-0.734513 4:-0.271472 5:-0.381977 6:-0.427255 7:-0.285501 \n",
      "9 0:-1 1:0.351351 2:0.310924 3:-0.716814 4:-0.407119 5:-0.462677 6:-0.479921 7:-0.526657 \n",
      "13 0:-1 1:0.351351 2:0.294118 3:-0.716814 4:-0.320347 5:-0.351715 6:-0.564187 7:-0.454908 \n",
      "8 1:0.364865 2:0.277311 3:-0.734513 4:-0.410661 5:-0.425689 6:-0.60237 7:-0.544594 \n",
      "10 0:-1 1:0.364865 2:0.361345 3:-0.725664 4:-0.269347 5:-0.370545 6:-0.415405 7:-0.415047 \n",
      "10 0:-1 1:0.364865 2:0.260504 3:-0.769912 4:-0.436161 5:-0.510424 6:-0.545754 7:-0.547583 \n",
      "9 0:1 1:0.364865 2:0.310924 3:-0.778761 4:-0.498849 5:-0.593813 6:-0.631336 7:-0.534629 \n",
      "10 1:0.378378 2:0.310924 3:-0.752212 4:-0.354702 5:-0.422327 6:-0.420671 7:-0.498754 \n",
      "10 0:-1 1:0.391892 2:0.462185 3:-0.707965 4:-0.146449 5:-0.249496 6:-0.382488 7:-0.387145 \n",
      "10 0:1 1:0.391892 2:0.327731 3:-0.743363 4:-0.277492 5:-0.425689 6:-0.295589 7:-0.474838 \n",
      "11 0:1 1:0.391892 2:0.361345 3:-0.743363 4:-0.362848 5:-0.437794 6:-0.531271 7:-0.484803 \n",
      "8 1:0.405405 2:0.277311 3:-0.734513 4:-0.36391 5:-0.439812 6:-0.553654 7:-0.474838 \n",
      "10 0:-1 1:0.405405 2:0.327731 3:-0.752212 4:-0.407827 5:-0.468056 6:-0.490454 7:-0.570503 \n",
      "10 0:-1 1:0.405405 2:0.327731 3:-0.743363 4:-0.322118 5:-0.378615 6:-0.457538 7:-0.497758 \n",
      "8 0:1 1:0.405405 2:0.361345 3:-0.734513 4:-0.411015 5:-0.494284 6:-0.494404 7:-0.534629 \n",
      "8 1:0.418919 2:0.361345 3:-0.725664 4:-0.311847 5:-0.427034 6:-0.462804 7:-0.405082 \n",
      "9 1:0.418919 2:0.411765 3:-0.734513 4:-0.200992 5:-0.227976 6:-0.485188 7:-0.395117 \n",
      "9 0:-1 1:0.418919 2:0.428571 3:-0.707965 4:-0.352222 5:-0.445192 6:-0.483871 7:-0.45989 \n",
      "8 0:1 1:0.418919 2:0.428571 3:-0.699115 4:-0.351514 5:-0.490249 6:-0.415405 7:-0.425012 \n",
      "10 1:0.418919 2:0.428571 3:-0.681416 4:-0.247388 5:-0.396772 6:-0.354839 7:-0.355257 \n",
      "9 0:-1 1:0.418919 2:0.394958 3:-0.707965 4:-0.251284 5:-0.32347 6:-0.366689 7:-0.454908 \n",
      "10 0:-1 1:0.418919 2:0.394958 3:-0.716814 4:-0.155658 5:-0.244788 6:-0.199473 7:-0.477828 \n",
      "9 1:0.432432 2:0.344538 3:-0.743363 4:-0.309014 5:-0.37189 6:-0.533904 7:-0.454908 \n",
      "8 0:-1 1:0.432432 2:0.411765 3:-0.743363 4:-0.375243 5:-0.485541 6:-0.499671 7:-0.464873 \n",
      "9 0:1 1:0.432432 2:0.394958 3:-0.743363 4:-0.432973 5:-0.491594 6:-0.404872 7:-0.564524 \n",
      "8 1:0.432432 2:0.428571 3:-0.752212 4:-0.299451 5:-0.364492 6:-0.383805 7:-0.524664 \n",
      "9 1:0.432432 2:0.394958 3:-0.725664 4:-0.311493 5:-0.472764 6:-0.411455 7:-0.336323 \n",
      "11 1:0.432432 2:0.512605 3:-0.681416 4:0.0143439 5:-0.0215198 6:-0.306122 7:-0.143996 \n",
      "10 0:-1 1:0.445946 2:0.411765 3:-0.725664 4:-0.305118 5:-0.387357 6:-0.400922 7:-0.472845 \n",
      "10 1:0.445946 2:0.378151 3:-0.716814 4:-0.241721 5:-0.351042 6:-0.339039 7:-0.444943 \n",
      "12 1:0.445946 2:0.445378 3:-0.734513 4:-0.12272 5:-0.190989 6:-0.233706 7:-0.388142 \n",
      "9 0:-1 1:0.445946 2:0.394958 3:-0.716814 4:-0.277492 5:-0.397445 6:-0.383805 7:-0.416044 \n",
      "10 1:0.445946 2:0.411765 3:-0.716814 4:-0.211263 5:-0.485541 6:-0.414088 7:-0.247633 \n",
      "9 0:1 1:0.445946 2:0.378151 3:-0.778761 4:-0.347972 5:-0.414929 6:-0.500987 7:-0.484803 \n",
      "9 0:-1 1:0.445946 2:0.394958 3:-0.699115 4:-0.209138 5:-0.298588 6:-0.368005 7:-0.385152 \n",
      "7 1:0.445946 2:0.445378 3:-0.681416 4:-0.0950947 5:-0.229993 6:-0.249506 7:-0.295466 \n",
      "10 0:-1 1:0.459459 2:0.394958 3:-0.716814 4:-0.28068 5:-0.365165 6:-0.370639 7:-0.444943 \n",
      "9 0:-1 1:0.459459 2:0.411765 3:-0.690265 4:-0.134408 5:-0.189644 6:-0.314022 7:-0.383159 \n",
      "12 0:1 1:0.472973 2:0.445378 3:-0.681416 4:-0.183991 5:-0.337592 6:-0.327189 7:-0.375187 \n",
      "12 1:0.472973 2:0.546218 3:-0.725664 4:-0.0625111 5:-0.102219 6:-0.315339 7:-0.335326 \n",
      "13 0:-1 1:0.472973 2:0.546218 3:-0.690265 4:-0.136533 5:-0.281775 6:-0.366689 7:-0.215745 \n",
      "11 1:0.472973 2:0.630252 3:-0.707965 4:-0.194617 5:-0.329523 6:-0.360105 7:-0.29148 \n",
      "11 0:1 1:0.472973 2:0.462185 3:-0.716814 4:-0.246325 5:-0.401479 6:-0.353522 7:-0.395117 \n",
      "12 1:0.472973 2:0.428571 3:-0.681416 4:-0.136179 5:-0.218561 6:-0.291639 7:-0.379173 \n",
      "9 0:1 1:0.472973 2:0.394958 3:-0.752212 4:-0.394723 5:-0.517821 6:-0.579987 7:-0.415047 \n",
      "10 0:1 1:0.472973 2:0.327731 3:-0.761062 4:-0.34691 5:-0.519839 6:-0.404872 7:-0.412058 \n",
      "12 0:-1 1:0.472973 2:0.428571 3:-0.734513 4:-0.104657 5:-0.156019 6:-0.323239 7:-0.387145 \n",
      "9 1:0.472973 2:0.428571 3:-0.690265 4:-0.264388 5:-0.37727 6:-0.415405 7:-0.405082 \n",
      "11 0:-1 1:0.486486 2:0.462185 3:-0.707965 4:-0.210554 5:-0.342972 6:-0.312706 7:-0.340309 \n",
      "10 0:-1 1:0.486486 2:0.411765 3:-0.716814 4:-0.233221 5:-0.328178 6:-0.381172 7:-0.384155 \n",
      "9 0:-1 1:0.486486 2:0.495798 3:-0.699115 4:-0.223304 5:-0.376597 6:-0.421988 7:-0.297459 \n",
      "12 0:1 1:0.486486 2:0.394958 3:-0.725664 4:-0.154595 5:-0.136516 6:-0.460171 7:-0.376183 \n",
      "10 1:0.486486 2:0.445378 3:-0.690265 4:-0.0366566 5:-0.0914593 6:-0.312706 7:-0.264574 \n",
      "9 0:1 1:0.486486 2:0.445378 3:-0.734513 4:-0.261909 5:-0.412239 6:-0.246873 7:-0.448929 \n",
      "9 0:-1 1:0.5 2:0.512605 3:-0.699115 4:-0.228263 5:-0.380632 6:-0.300856 7:-0.405082 \n",
      "8 1:0.5 2:0.495798 3:-0.681416 4:-0.153887 5:-0.31002 6:-0.389072 7:-0.206776 \n",
      "11 0:-1 1:0.5 2:0.462185 3:-0.699115 4:-0.169471 5:-0.294553 6:-0.282423 7:-0.327354 \n",
      "10 0:-1 1:0.5 2:0.445378 3:-0.707965 4:-0.128033 5:-0.118359 6:-0.391705 7:-0.398107 \n",
      "11 0:-1 1:0.5 2:0.478992 3:-0.690265 4:-0.102178 5:-0.187626 6:-0.287689 7:-0.349278 \n",
      "13 0:1 1:0.513514 2:0.495798 3:-0.707965 4:0.0533026 5:-0.0396772 6:-0.0941409 7:-0.282511 \n",
      "12 0:-1 1:0.513514 2:0.495798 3:-0.699115 4:0.0146981 5:-0.179556 6:-0.187623 7:-0.169905 \n",
      "10 1:0.513514 2:0.462185 3:-0.690265 4:-0.120595 5:-0.220578 6:-0.17709 7:-0.395117 \n",
      "9 1:0.513514 2:0.462185 3:-0.699115 4:-0.107845 5:-0.277068 6:-0.299539 7:-0.24564 \n",
      "10 1:0.527027 2:0.512605 3:-0.707965 4:-0.134762 5:-0.299933 6:-0.291639 7:-0.285501 \n",
      "10 0:-1 1:0.527027 2:0.546218 3:-0.681416 4:-0.118116 5:-0.264963 6:-0.231073 7:-0.26856 \n",
      "11 0:-1 1:0.527027 2:0.579832 3:-0.672566 4:0.207721 5:0.0248823 6:0.105991 7:-0.119083 \n",
      "10 0:-1 1:0.540541 2:0.512605 3:-0.734513 4:-0.179387 5:-0.303295 6:-0.312706 7:-0.335326 \n",
      "12 0:-1 1:0.540541 2:0.495798 3:-0.690265 4:-0.0904905 5:-0.242098 6:-0.242923 7:-0.233682 \n",
      "14 0:-1 1:0.540541 2:0.495798 3:-0.663717 4:0.103241 5:-0.00470746 6:-0.0230415 7:-0.236672 \n",
      "9 0:-1 1:0.540541 2:0.529412 3:-0.663717 4:0.0430317 5:-0.187626 6:-0.0928242 7:-0.0463378 \n",
      "10 0:-1 1:0.540541 2:0.529412 3:-0.654867 4:-0.132991 5:-0.209818 6:-0.418038 7:-0.256602 \n",
      "9 0:-1 1:0.540541 2:0.529412 3:-0.716814 4:-0.0593235 5:-0.104909 6:-0.187623 7:-0.371201 \n",
      "9 1:0.540541 2:0.529412 3:-0.716814 4:-0.122012 5:-0.219233 6:-0.274523 7:-0.375187 \n",
      "10 0:-1 1:0.540541 2:0.495798 3:-0.690265 4:-0.054011 5:-0.256221 6:-0.190257 7:-0.175884 \n",
      "10 1:0.540541 2:0.529412 3:-0.663717 4:-0.0359483 5:-0.230666 6:-0.0480579 7:-0.285501 \n",
      "9 0:-1 1:0.540541 2:0.445378 3:-0.734513 4:-0.136179 5:-0.235373 6:-0.281106 7:-0.345291 \n",
      "12 1:0.540541 2:0.428571 3:-0.663717 4:-0.0302816 5:-0.0699395 6:-0.23634 7:-0.305431 \n",
      "9 1:0.554054 2:0.478992 3:-0.725664 4:-0.0543651 5:-0.174176 6:-0.159974 7:-0.335326 \n",
      "11 0:-1 1:0.554054 2:0.512605 3:-0.663717 4:-0.0989906 5:-0.207801 6:-0.395655 7:-0.223717 \n",
      "11 0:-1 1:0.554054 2:0.579832 3:-0.672566 4:0.0525943 5:-0.106927 6:-0.113891 7:-0.249626 \n",
      "9 0:-1 1:0.554054 2:0.529412 3:-0.716814 4:-0.0214273 5:-0.142569 6:-0.23634 7:-0.272546 \n",
      "10 0:-1 1:0.567568 2:0.663866 3:-0.681416 4:-0.0989906 5:-0.213181 6:-0.261356 7:-0.275536 \n",
      "9 1:0.567568 2:0.529412 3:-0.734513 4:-0.262617 5:-0.356422 6:-0.414088 7:-0.395117 \n",
      "12 1:0.567568 2:0.512605 3:-0.663717 4:-0.0462192 5:-0.203093 6:-0.278473 7:-0.155954 \n",
      "11 1:0.567568 2:0.512605 3:-0.654867 4:0.0189481 5:-0.0759919 6:0.000658328 7:-0.279522 \n",
      "9 0:-1 1:0.581081 2:0.495798 3:-0.707965 4:-0.0667611 5:-0.104237 6:-0.292956 7:-0.323368 \n",
      "12 1:0.581081 2:0.613445 3:-0.690265 4:0.0734903 5:-0.0450572 6:-0.178407 7:-0.175884 \n",
      "10 0:-1 1:0.581081 2:0.596639 3:-0.654867 4:0.0968656 5:-0.126429 6:-0.133641 7:-0.0164425 \n",
      "14 0:-1 1:0.581081 2:0.529412 3:-0.707965 4:0.158491 5:0.0322798 6:-0.0678078 7:-0.220727 \n",
      "11 0:-1 1:0.594595 2:0.579832 3:-0.690265 4:0.020719 5:-0.108944 6:0.0111916 7:-0.299452 \n",
      "11 0:-1 1:0.594595 2:0.512605 3:-0.716814 4:-0.0883655 5:-0.174849 6:-0.335089 7:-0.272546 \n",
      "11 1:0.594595 2:0.512605 3:-0.716814 4:-0.0865947 5:-0.152656 6:-0.231073 7:-0.365222 \n",
      "12 0:-1 1:0.594595 2:0.563025 3:-0.690265 4:-0.0292191 5:-0.186281 6:-0.158657 7:-0.215745 \n",
      "9 0:-1 1:0.594595 2:0.495798 3:-0.690265 4:-0.0823446 5:-0.184264 6:-0.174457 7:-0.375187 \n",
      "12 0:-1 1:0.608108 2:0.512605 3:-0.716814 4:-0.10997 5:-0.160054 6:-0.182357 7:-0.389138 \n",
      "11 0:-1 1:0.608108 2:0.563025 3:-0.707965 4:-0.0168231 5:-0.0450572 6:-0.246873 7:-0.405082 \n",
      "13 1:0.608108 2:0.563025 3:-0.663717 4:-0.0664069 5:-0.297243 6:-0.186307 7:-0.150972 \n",
      "10 1:0.608108 2:0.663866 3:-0.725664 4:0.107845 5:0.152656 6:-0.108624 7:-0.297459 \n",
      "11 1:0.608108 2:0.630252 3:-0.654867 4:0.145387 5:-0.00605245 6:-0.131007 7:-0.0762332 \n",
      "13 0:-1 1:0.621622 2:0.579832 3:-0.716814 4:-0.0922614 5:-0.232011 6:-0.275839 7:-0.296462 \n",
      "10 1:0.621622 2:0.529412 3:-0.654867 4:-0.0224898 5:-0.188299 6:-0.16524 7:-0.212755 \n",
      "10 0:-1 1:0.635135 2:0.563025 3:-0.654867 4:0.0281565 5:-0.205111 6:0.0283081 7:-0.180867 \n",
      "11 1:0.635135 2:0.529412 3:-0.646018 4:0.137241 5:-0.0410222 6:-0.108624 7:-0.0652715 \n",
      "11 0:-1 1:0.648649 2:0.563025 3:-0.734513 4:-0.0285107 5:-0.0349697 6:-0.229756 7:-0.365222 \n",
      "11 1:0.648649 2:0.714286 3:-0.690265 4:0.158845 5:0.0443847 6:-0.0138249 7:-0.130045 \n",
      "12 1:0.662162 2:0.663866 3:-0.646018 4:0.10997 5:-0.0773369 6:-0.0335747 7:-0.0861983 \n",
      "10 0:-1 1:0.689189 2:0.714286 3:-0.690265 4:0.313618 5:0.134499 6:0.0348914 7:0.0732436 \n",
      "15 1:0.689189 2:0.613445 3:-0.690265 4:0.254471 5:-0.0860794 6:0.262673 7:0.017439 \n",
      "11 1:0.702703 2:0.647059 3:-0.699115 4:0.117762 5:-0.135844 6:0.20079 7:-0.474838 \n",
      "11 0:-1 1:0.716216 2:0.747899 3:-0.619469 4:0.42164 5:0.329523 6:0.17709 7:-0.00249128 \n",
      "12 1:0.716216 2:0.731092 3:-0.654867 4:0.401452 5:0.333557 6:0.295589 7:-0.0463378 \n",
      "10 1:0.716216 2:0.630252 3:-0.637168 4:0.118116 5:0.0773369 6:-0.245556 7:-0.136024 \n",
      "11 0:-1 1:0.716216 2:0.697479 3:-0.610619 4:0.42589 5:0.238063 6:0.194207 7:0.125062 \n",
      "10 0:-1 1:0.743243 2:0.731092 3:-0.646018 4:0.293076 5:0.234701 6:-0.0375247 7:-0.0363727 \n",
      "13 0:-1 1:0.743243 2:0.663866 3:-0.637168 4:0.503807 5:0.539341 6:0.163924 7:0.0154459 \n",
      "7 1:0.743243 2:0.579832 3:-0.681416 4:0.0221356 5:-0.152656 6:-0.154707 7:-0.136024 \n",
      "14 1:0.756757 2:0.714286 3:-0.628319 4:0.516203 5:0.383995 6:0.281106 7:-0.000498256 \n",
      "11 1:0.77027 2:0.697479 3:-0.663717 4:0.374535 5:0.073302 6:0.366689 7:0.124066 \n",
      "11 0:-1 1:0.783784 2:0.798319 3:-0.619469 4:0.236055 5:-0.0228648 6:0.0599078 7:0.107125 \n",
      "12 1:0.797297 2:0.714286 3:-0.637168 4:0.499557 5:0.297243 6:0.364055 7:-0.0423518 \n",
      "11 1:0.824324 2:0.714286 3:-0.619469 4:0.371348 5:0.0390047 6:0.269256 7:0.143 \n",
      "12 0:-1 1:0.824324 2:0.815126 3:-0.637168 4:0.571454 5:0.455279 6:0.107307 7:0.252616 \n",
      "12 0:-1 1:0.878378 2:0.89916 3:-0.654867 4:0.780414 5:0.498991 6:0.687953 7:0.276532 \n",
      "12 0:-1 1:0.891892 2:0.932773 3:-0.557522 4:0.967416 5:0.812374 6:1 7:0.148979 \n",
      "5 0:1 1:-0.459459 2:-0.596639 3:-0.840708 4:-0.837436 5:-0.872226 6:-0.851218 7:-0.862481 \n",
      "6 0:1 1:-0.189189 2:-0.361345 3:-0.823009 4:-0.72233 5:-0.778077 6:-0.761685 7:-0.778774 \n",
      "7 1:-0.189189 2:-0.277311 3:-0.761062 4:-0.578537 5:-0.635508 6:-0.656353 7:-0.669158 \n",
      "7 0:-1 1:-0.148649 2:-0.243697 3:-0.778761 4:-0.601913 5:-0.593141 6:-0.749835 7:-0.717987 \n",
      "8 0:1 1:-0.027027 2:-0.176471 3:-0.787611 4:-0.578891 5:-0.652993 6:-0.635286 7:-0.675137 \n",
      "7 0:-1 2:-0.109244 3:-0.787611 4:-0.708164 5:-0.733692 6:-0.764319 7:-0.769806 \n",
      "6 0:1 1:0.027027 2:-0.0588235 3:-0.814159 4:-0.702497 5:-0.693342 6:-0.773535 7:-0.803687 \n",
      "8 0:1 1:0.027027 2:-0.092437 3:-0.761062 4:-0.420577 5:-0.462677 6:-0.549704 7:-0.582461 \n",
      "8 0:1 1:0.027027 2:-0.0252101 3:-0.80531 4:-0.693997 5:-0.722932 6:-0.776169 7:-0.760837 \n",
      "10 0:1 1:0.0540541 2:-0.092437 3:-0.752212 4:-0.462015 5:-0.514459 6:-0.597103 7:-0.586447 \n",
      "7 0:-1 1:0.0540541 2:0.0252101 3:-0.79646 4:-0.590933 5:-0.604573 6:-0.63397 7:-0.763827 \n",
      "8 0:1 1:0.108108 2:0.0420168 3:-0.814159 4:-0.632725 5:-0.739072 6:-0.677419 7:-0.640259 \n",
      "9 0:-1 1:0.108108 2:0.0588235 3:-0.725664 4:-0.315743 5:-0.437794 6:-0.354839 7:-0.531639 \n",
      "9 0:1 1:0.108108 2:-0.0252101 3:-0.716814 4:-0.385869 5:-0.586416 6:-0.514154 7:-0.367215 \n",
      "8 1:0.121622 2:0.00840336 3:-0.716814 4:-0.378431 5:-0.532616 6:-0.435155 7:-0.43996 \n",
      "9 0:-1 1:0.148649 2:0.0588235 3:-0.734513 4:-0.249513 5:-0.33692 6:-0.414088 7:-0.413054 \n",
      "9 0:-1 1:0.189189 2:-0.00840336 3:-0.725664 4:-0.347972 5:-0.438467 6:-0.479921 7:-0.458894 \n",
      "8 0:-1 1:0.189189 2:0.142857 3:-0.761062 4:-0.288118 5:-0.36651 6:-0.344305 7:-0.500747 \n",
      "10 0:-1 1:0.216216 2:0.0420168 3:-0.699115 4:-0.321055 5:-0.412239 6:-0.415405 7:-0.452915 \n",
      "7 0:-1 1:0.216216 2:0.092437 3:-0.778761 4:-0.540995 5:-0.593813 6:-0.593153 7:-0.686099 \n",
      "8 0:-1 1:0.22973 2:0.193277 3:-0.752212 4:-0.466974 5:-0.531271 6:-0.549704 7:-0.583458 \n",
      "8 1:0.243243 2:0.243697 3:-0.761062 4:-0.455286 5:-0.494956 6:-0.523371 7:-0.645242 \n",
      "8 0:1 1:0.243243 2:0.109244 3:-0.681416 4:-0.23393 5:-0.334902 6:-0.396972 7:-0.39711 \n",
      "8 0:1 1:0.27027 2:0.226891 3:-0.707965 4:-0.368514 5:-0.431742 6:-0.423305 7:-0.54858 \n",
      "10 1:0.27027 2:0.210084 3:-0.646018 4:-0.03949 5:-0.238736 6:-0.163924 7:-0.199801 \n",
      "9 1:0.27027 2:0.109244 3:-0.734513 4:-0.209138 5:-0.271688 6:-0.357472 7:-0.435974 \n",
      "10 1:0.283784 2:0.092437 3:-0.707965 4:-0.147866 5:-0.271015 6:-0.227123 7:-0.336323 \n",
      "10 0:-1 1:0.283784 2:0.226891 3:-0.716814 4:-0.051886 5:-0.150639 6:-0.182357 7:-0.317389 \n",
      "9 0:-1 1:0.337838 2:0.344538 3:-0.690265 4:-0.278909 5:-0.355077 6:-0.436471 7:-0.425012 \n",
      "10 0:-1 1:0.351351 2:0.294118 3:-0.672566 4:-0.275367 5:-0.318763 6:-0.410138 7:-0.507723 \n",
      "10 0:1 1:0.351351 2:0.327731 3:-0.769912 4:-0.424473 5:-0.459314 6:-0.549704 7:-0.578475 \n",
      "10 1:0.364865 2:0.260504 3:-0.699115 4:0.0469276 5:-0.122394 6:-0.148124 7:-0.174888 \n",
      "10 0:-1 1:0.378378 2:0.344538 3:-0.743363 4:-0.326368 5:-0.470746 6:-0.294273 7:-0.488789 \n",
      "8 0:1 1:0.378378 2:0.327731 3:-0.734513 4:-0.369931 5:-0.466711 6:-0.465438 7:-0.498754 \n",
      "11 0:-1 1:0.418919 2:0.478992 3:-0.690265 4:-0.0802196 5:-0.168124 6:-0.253456 7:-0.348281 \n",
      "9 0:-1 1:0.418919 2:0.378151 3:-0.707965 4:-0.266159 5:-0.332213 6:-0.420671 7:-0.50274 \n",
      "9 0:-1 1:0.432432 2:0.411765 3:-0.690265 4:-0.11422 5:-0.251513 6:-0.19684 7:-0.319382 \n",
      "10 0:-1 1:0.432432 2:0.411765 3:-0.734513 4:-0.186825 5:-0.227976 6:-0.390388 7:-0.411061 \n",
      "11 1:0.445946 2:0.411765 3:-0.734513 4:-0.212679 5:-0.302623 6:-0.323239 7:-0.404086 \n",
      "10 1:0.459459 2:0.344538 3:-0.743363 4:-0.211263 5:-0.322798 6:-0.374589 7:-0.375187 \n",
      "10 0:-1 1:0.472973 2:0.394958 3:-0.743363 4:-0.231804 5:-0.314055 6:-0.286373 7:-0.491779 \n",
      "9 0:-1 1:0.486486 2:0.478992 3:-0.690265 4:-0.113157 5:-0.219233 6:-0.24819 7:-0.368211 \n",
      "11 0:-1 1:0.486486 2:0.462185 3:-0.672566 4:-0.173366 5:-0.291863 6:-0.333772 7:-0.317389 \n",
      "11 0:-1 1:0.513514 2:0.478992 3:-0.654867 4:-0.171241 5:-0.402824 6:-0.18104 7:-0.310414 \n",
      "10 1:0.513514 2:0.411765 3:-0.734513 4:-0.162387 5:-0.284465 6:-0.19289 7:-0.423019 \n",
      "8 1:0.527027 2:0.411765 3:-0.752212 4:-0.241721 5:-0.342972 6:-0.396972 7:-0.385152 \n",
      "9 0:-1 1:0.540541 2:0.495798 3:-0.716814 4:-0.022844 5:-0.0975118 6:-0.142857 7:-0.375187 \n",
      "11 0:-1 1:0.554054 2:0.579832 3:-0.663717 4:0.140783 5:0.0437122 6:-0.0309414 7:-0.212755 \n",
      "10 0:-1 1:0.554054 2:0.445378 3:-0.716814 4:0.230742 5:-0.232683 6:-0.267939 7:-0.390135 \n",
      "9 1:0.567568 2:0.563025 3:-0.646018 4:0.0947406 5:-0.0423672 6:-0.174457 7:-0.0742402 \n",
      "10 0:-1 1:0.567568 2:0.647059 3:-0.663717 4:0.00761466 5:-0.150639 6:-0.124424 7:-0.249626 \n",
      "11 1:0.594595 2:0.546218 3:-0.672566 4:-0.051886 5:-0.248823 6:-0.229756 7:-0.131041 \n",
      "10 1:0.621622 2:0.596639 3:-0.690265 4:0.0231982 5:-0.0901143 6:-0.132324 7:-0.227703 \n",
      "9 1:0.648649 2:0.613445 3:-0.690265 4:0.120949 5:-0.0363147 6:-0.00724161 7:-0.16293 \n",
      "10 1:0.675676 2:0.663866 3:-0.672566 4:0.187887 5:0.0813719 6:0.0559579 7:-0.212755 \n",
      "11 0:-1 1:0.675676 2:0.596639 3:-0.663717 4:0.221179 5:0.0242098 6:0.146807 7:-0.0961634 \n",
      "13 1:0.702703 2:0.647059 3:-0.681416 4:0.0890738 5:-0.184264 6:-0.0335747 7:-0.0772297 \n",
      "9 1:0.743243 2:0.663866 3:-0.654867 4:0.466974 5:0.439812 6:0.121791 7:-0.00348779 \n",
      "11 0:-1 1:0.743243 2:0.697479 3:-0.681416 4:0.122366 5:-0.071957 6:-0.0138249 7:-0.121076 \n",
      "11 0:-1 1:0.77027 2:0.747899 3:-0.628319 4:0.46414 5:0.247478 6:0.0757077 7:0.278525 \n",
      "5 0:1 1:-0.702703 2:-0.731092 3:-0.929204 4:-0.982291 5:-0.987223 6:-0.986833 7:-0.985052 \n",
      "4 0:1 1:-0.554054 2:-0.579832 3:-0.902655 4:-0.962104 5:-0.969738 6:-0.967084 7:-0.967115 \n",
      "7 0:1 1:-0.364865 2:-0.462185 3:-0.867257 4:-0.911103 5:-0.925353 6:-0.928901 7:-0.931241 \n",
      "6 0:1 1:-0.283784 2:-0.310924 3:-0.849558 4:-0.867894 5:-0.891728 6:-0.913101 7:-0.883408 \n",
      "7 0:1 1:-0.256757 2:-0.294118 3:-0.858407 4:-0.859749 5:-0.880296 6:-0.890718 7:-0.883408 \n",
      "6 0:1 1:-0.216216 2:-0.277311 3:-0.849558 4:-0.861874 5:-0.891728 6:-0.915734 7:-0.873443 \n",
      "7 0:1 1:-0.216216 2:-0.260504 3:-0.849558 4:-0.843457 5:-0.869536 6:-0.902567 7:-0.853513 \n",
      "6 0:1 1:-0.216216 2:-0.277311 3:-0.867257 4:-0.844519 5:-0.873571 6:-0.884134 7:-0.863478 \n",
      "8 0:1 1:-0.148649 2:-0.142857 3:-0.814159 4:-0.812644 5:-0.841964 6:-0.863068 7:-0.841555 \n",
      "6 0:1 1:-0.108108 2:-0.176471 3:-0.840708 4:-0.810873 5:-0.862811 6:-0.824885 7:-0.783757 \n",
      "7 0:1 1:-0.0945946 2:-0.12605 3:-0.831858 4:-0.802727 5:-0.848016 6:-0.910467 7:-0.783757 \n",
      "7 0:1 1:-0.0945946 2:-0.0588235 3:-0.814159 4:-0.76731 5:-0.812374 6:-0.832785 7:-0.793722 \n",
      "6 0:1 1:-0.0810811 2:-0.142857 3:-0.840708 4:-0.802019 5:-0.833894 6:-0.839368 7:-0.833582 \n",
      "6 0:1 1:-0.0810811 2:-0.142857 3:-0.823009 4:-0.802727 5:-0.848016 6:-0.852535 7:-0.80867 \n",
      "8 0:1 1:-0.0810811 2:-0.142857 3:-0.831858 4:-0.781123 5:-0.850034 6:-0.836735 7:-0.773792 \n",
      "11 0:1 1:-0.0675676 2:-0.092437 3:-0.823009 4:-0.740747 5:-0.776059 6:-0.836735 7:-0.776781 \n",
      "8 0:1 1:-0.0405405 2:-0.0420168 3:-0.823009 4:-0.760227 5:-0.813719 6:-0.826201 7:-0.763827 \n",
      "7 0:1 1:-0.027027 2:-0.0588235 3:-0.823009 4:-0.77156 5:-0.819771 6:-0.794602 7:-0.807673 \n",
      "7 0:1 1:-0.027027 2:-0.0756303 3:-0.80531 4:-0.732247 5:-0.797579 6:-0.752469 7:-0.783757 \n",
      "8 0:1 1:-0.027027 2:-0.0756303 3:-0.814159 4:-0.764123 5:-0.791527 6:-0.855168 7:-0.793722 \n",
      "7 0:1 1:-0.027027 2:-0.0252101 3:-0.787611 4:-0.773685 5:-0.826496 6:-0.853851 7:-0.766816 \n",
      "8 0:1 2:-0.0756303 3:-0.80531 4:-0.747831 5:-0.796234 6:-0.824885 7:-0.766816 \n",
      "7 0:1 1:0.0540541 2:0.0588235 3:-0.80531 4:-0.686205 5:-0.781439 6:-0.748519 7:-0.672147 \n",
      "8 0:1 1:0.0675676 2:0.0756303 3:-0.787611 4:-0.656455 5:-0.737727 6:-0.740619 7:-0.733931 \n",
      "9 0:1 1:0.0810811 2:-0.0420168 3:-0.814159 4:-0.680184 5:-0.728312 6:-0.790652 7:-0.711011 \n",
      "7 0:1 1:0.108108 2:0.109244 3:-0.769912 4:-0.599079 5:-0.664425 6:-0.532587 7:-0.696064 \n",
      "8 0:1 1:0.108108 2:0.0252101 3:-0.787611 4:-0.636267 5:-0.669805 6:-0.731402 7:-0.710015 \n",
      "9 0:1 1:0.108108 2:0.0588235 3:-0.79646 4:-0.677705 5:-0.747814 6:-0.747202 7:-0.704036 \n",
      "8 0:1 1:0.135135 2:0.092437 3:-0.761062 4:-0.640517 5:-0.723605 6:-0.694536 7:-0.674141 \n",
      "8 0:1 1:0.135135 2:0.092437 3:-0.743363 4:-0.647246 5:-0.725622 6:-0.611587 7:-0.703039 \n",
      "8 0:1 1:0.135135 2:0.0756303 3:-0.752212 4:-0.651496 5:-0.757902 6:-0.744569 7:-0.622322 \n",
      "9 0:1 1:0.148649 2:0.092437 3:-0.80531 4:-0.604392 5:-0.624748 6:-0.722186 7:-0.704036 \n",
      "11 0:1 1:0.162162 2:0.176471 3:-0.769912 4:-0.577121 5:-0.699395 6:-0.691903 7:-0.554559 \n",
      "8 0:1 1:0.162162 2:0.159664 3:-0.743363 4:-0.502391 5:-0.552118 6:-0.62607 7:-0.590433 \n",
      "10 1:0.175676 2:0.159664 3:-0.787611 4:-0.505224 5:-0.534633 6:-0.710336 7:-0.61435 \n",
      "10 0:1 1:0.189189 2:0.210084 3:-0.761062 4:-0.496724 5:-0.618023 6:-0.601053 7:-0.514699 \n",
      "10 0:1 1:0.189189 2:0.226891 3:-0.734513 4:-0.525058 5:-0.6577 6:-0.64977 7:-0.534629 \n",
      "8 0:-1 1:0.189189 2:0.109244 3:-0.80531 4:-0.591642 5:-0.661063 6:-0.579987 7:-0.723966 \n",
      "8 0:1 1:0.202703 2:0.193277 3:-0.80531 4:-0.634142 5:-0.710155 6:-0.760369 7:-0.636273 \n",
      "9 0:1 1:0.202703 2:0.210084 3:-0.752212 4:-0.549849 5:-0.587088 6:-0.64977 7:-0.668161 \n",
      "9 0:1 1:0.202703 2:0.142857 3:-0.778761 4:-0.590225 5:-0.672495 6:-0.616853 7:-0.674141 \n",
      "7 0:1 1:0.202703 2:0.092437 3:-0.79646 4:-0.530724 5:-0.559516 6:-0.553654 7:-0.718984 \n",
      "10 0:1 1:0.202703 2:0.109244 3:-0.79646 4:-0.589871 5:-0.6577 6:-0.59052 7:-0.717987 \n",
      "9 0:1 1:0.216216 2:0.210084 3:-0.787611 4:-0.579246 5:-0.624075 6:-0.685319 7:-0.665172 \n",
      "8 0:1 1:0.216216 2:0.176471 3:-0.743363 4:-0.508057 5:-0.591796 6:-0.597103 7:-0.584454 \n",
      "9 0:1 1:0.216216 2:0.159664 3:-0.743363 4:-0.569683 5:-0.667787 6:-0.58262 7:-0.65421 \n",
      "8 0:1 1:0.22973 2:0.260504 3:-0.752212 4:-0.52187 5:-0.600538 6:-0.747202 7:-0.544594 \n",
      "9 0:1 1:0.22973 2:0.260504 3:-0.716814 4:-0.488224 5:-0.569603 6:-0.665569 7:-0.524664 \n",
      "9 0:1 1:0.22973 2:0.142857 3:-0.769912 4:-0.594121 5:-0.669132 6:-0.698486 7:-0.638266 \n",
      "9 0:1 1:0.22973 2:0.176471 3:-0.787611 4:-0.553745 5:-0.63618 6:-0.611587 7:-0.629297 \n",
      "9 0:1 1:0.243243 2:0.344538 3:-0.752212 4:-0.292013 5:-0.2885 6:-0.536537 7:-0.516692 \n",
      "9 1:0.256757 2:0.243697 3:-0.716814 4:-0.331681 5:-0.507061 6:-0.473338 7:-0.415047 \n",
      "9 0:1 1:0.256757 2:0.142857 3:-0.761062 4:-0.537099 5:-0.637525 6:-0.593153 7:-0.620329 \n",
      "8 0:1 1:0.256757 2:0.12605 3:-0.778761 4:-0.55835 5:-0.661735 6:-0.585253 7:-0.624315 \n",
      "10 0:1 1:0.27027 2:0.243697 3:-0.752212 4:-0.424473 5:-0.591123 6:-0.393022 7:-0.516692 \n",
      "9 0:1 1:0.27027 2:0.260504 3:-0.752212 4:-0.514787 5:-0.64963 6:-0.631336 7:-0.504733 \n",
      "10 0:1 1:0.283784 2:0.277311 3:-0.752212 4:-0.435098 5:-0.604573 6:-0.499671 7:-0.52865 \n",
      "6 0:1 1:0.283784 2:0.327731 3:-0.769912 4:-0.431911 5:-0.547411 6:-0.631336 7:-0.544594 \n",
      "8 0:-1 1:0.297297 2:0.277311 3:-0.752212 4:-0.470515 5:-0.542703 6:-0.568137 7:-0.576482 \n",
      "8 0:-1 1:0.297297 2:0.193277 3:-0.778761 4:-0.577121 5:-0.685945 6:-0.615537 7:-0.616343 \n",
      "7 0:-1 1:0.297297 2:0.159664 3:-0.769912 4:-0.500266 5:-0.554808 6:-0.583937 7:-0.644245 \n",
      "9 0:1 1:0.297297 2:0.327731 3:-0.690265 4:-0.478661 5:-0.592468 6:-0.539171 7:-0.564524 \n",
      "9 0:-1 1:0.297297 2:0.344538 3:-0.761062 4:-0.408536 5:-0.487559 6:-0.55102 7:-0.534629 \n",
      "9 0:1 1:0.310811 2:0.310924 3:-0.707965 4:-0.412077 5:-0.536651 6:-0.529954 7:-0.446936 \n",
      "11 1:0.324324 2:0.310924 3:-0.778761 4:-0.41314 5:-0.579691 6:-0.531271 7:-0.544594 \n",
      "7 0:-1 1:0.324324 2:0.210084 3:-0.778761 4:-0.528953 5:-0.595158 6:-0.59447 7:-0.63428 \n",
      "11 0:-1 1:0.324324 2:0.344538 3:-0.725664 4:-0.338764 5:-0.435104 6:-0.519421 7:-0.484803 \n",
      "9 0:1 1:0.324324 2:0.277311 3:-0.743363 4:-0.403223 5:-0.466711 6:-0.585253 7:-0.494768 \n",
      "9 0:-1 1:0.324324 2:0.327731 3:-0.716814 4:-0.367452 5:-0.443174 6:-0.487821 7:-0.512706 \n",
      "10 0:1 1:0.324324 2:0.361345 3:-0.725664 4:-0.384098 5:-0.496301 6:-0.435155 7:-0.504733 \n",
      "11 0:-1 1:0.337838 2:0.361345 3:-0.725664 4:-0.292722 5:-0.39072 6:-0.461488 7:-0.474838 \n",
      "11 0:-1 1:0.337838 2:0.344538 3:-0.725664 4:-0.412077 5:-0.519166 6:-0.54312 7:-0.450922 \n",
      "9 0:-1 1:0.337838 2:0.294118 3:-0.690265 4:-0.334514 5:-0.489576 6:-0.399605 7:-0.438964 \n",
      "13 0:-1 1:0.337838 2:0.210084 3:-0.769912 4:-0.378077 5:-0.426362 6:-0.486504 7:-0.52865 \n",
      "9 1:0.337838 2:0.294118 3:-0.787611 4:-0.432619 5:-0.487559 6:-0.599737 7:-0.536622 \n",
      "9 0:-1 1:0.351351 2:0.327731 3:-0.769912 4:-0.445369 5:-0.573638 6:-0.493088 7:-0.55157 \n",
      "9 0:-1 1:0.351351 2:0.327731 3:-0.725664 4:-0.309722 5:-0.335575 6:-0.436471 7:-0.534629 \n",
      "10 0:-1 1:0.351351 2:0.277311 3:-0.761062 4:-0.298743 5:-0.420309 6:-0.415405 7:-0.526657 \n",
      "11 0:-1 1:0.351351 2:0.344538 3:-0.725664 4:-0.283868 5:-0.371217 6:-0.452271 7:-0.415047 \n",
      "10 0:-1 1:0.351351 2:0.310924 3:-0.743363 4:-0.38091 5:-0.490921 6:-0.576037 7:-0.464873 \n",
      "9 1:0.351351 2:0.378151 3:-0.690265 4:-0.22295 5:-0.364492 6:-0.469388 7:-0.305431 \n",
      "10 0:1 1:0.351351 2:0.327731 3:-0.761062 4:-0.384098 5:-0.3961 6:-0.57472 7:-0.554559 \n",
      "11 0:1 1:0.351351 2:0.327731 3:-0.761062 4:-0.41739 5:-0.547411 6:-0.444371 7:-0.526657 \n",
      "9 1:0.351351 2:0.260504 3:-0.725664 4:-0.437932 5:-0.532616 6:-0.494404 7:-0.564524 \n",
      "9 0:-1 1:0.351351 2:0.411765 3:-0.743363 4:-0.394369 5:-0.508406 6:-0.545754 7:-0.466866 \n",
      "9 1:0.364865 2:0.327731 3:-0.654867 4:-0.415973 5:-0.458642 6:-0.545754 7:-0.554559 \n",
      "10 1:0.364865 2:0.495798 3:-0.707965 4:-0.346202 5:-0.503699 6:-0.514154 7:-0.404086 \n",
      "10 0:-1 1:0.364865 2:0.294118 3:-0.734513 4:-0.260138 5:-0.30464 6:-0.425938 7:-0.445939 \n",
      "10 0:1 1:0.364865 2:0.294118 3:-0.743363 4:-0.441473 5:-0.527236 6:-0.568137 7:-0.520678 \n",
      "9 0:-1 1:0.364865 2:0.294118 3:-0.716814 4:-0.413848 5:-0.548756 6:-0.473338 7:-0.507723 \n",
      "10 0:-1 1:0.405405 2:0.344538 3:-0.734513 4:-0.373827 5:-0.420982 6:-0.472021 7:-0.558545 \n",
      "11 1:0.418919 2:0.394958 3:-0.761062 4:-0.314326 5:-0.375252 6:-0.486504 7:-0.476831 \n",
      "11 0:-1 1:0.418919 2:0.361345 3:-0.699115 4:-0.16522 5:-0.38803 6:-0.113891 7:-0.347285 \n",
      "10 0:-1 1:0.418919 2:0.411765 3:-0.734513 4:-0.300159 5:-0.482179 6:-0.423305 7:-0.384155 \n",
      "11 1:0.418919 2:0.378151 3:-0.716814 4:-0.198867 5:-0.37458 6:-0.241606 7:-0.409068 \n",
      "10 0:1 1:0.432432 2:0.462185 3:-0.707965 4:-0.242784 5:-0.35306 6:-0.491771 7:-0.301445 \n",
      "9 1:0.432432 2:0.344538 3:-0.743363 4:-0.390827 5:-0.552118 6:-0.478604 7:-0.405082 \n",
      "10 0:-1 1:0.432432 2:0.394958 3:-0.681416 4:-0.211263 5:-0.357095 6:-0.325872 7:-0.363229 \n",
      "8 0:-1 1:0.445946 2:0.428571 3:-0.752212 4:-0.271117 5:-0.412912 6:-0.312706 7:-0.464873 \n",
      "10 1:0.445946 2:0.361345 3:-0.743363 4:-0.209138 5:-0.35844 6:-0.225806 7:-0.408072 \n",
      "8 1:0.445946 2:0.361345 3:-0.725664 4:-0.323535 5:-0.429052 6:-0.481238 7:-0.474838 \n",
      "11 1:0.445946 2:0.394958 3:-0.707965 4:-0.166637 5:-0.240081 6:-0.267939 7:-0.41704 \n",
      "11 0:-1 1:0.459459 2:0.394958 3:-0.743363 4:-0.272888 5:-0.404842 6:-0.257406 7:-0.434978 \n",
      "9 0:-1 1:0.459459 2:0.394958 3:-0.734513 4:-0.231096 5:-0.332213 6:-0.25609 7:-0.467862 \n",
      "12 1:0.459459 2:0.478992 3:-0.716814 4:-0.112449 5:-0.219233 6:-0.159974 7:-0.360239 \n",
      "10 0:-1 1:0.459459 2:0.478992 3:-0.646018 4:-0.137949 5:-0.242771 6:-0.403555 7:-0.2287 \n",
      "11 0:-1 1:0.472973 2:0.462185 3:-0.716814 4:-0.268284 5:-0.409549 6:-0.336406 7:-0.434978 \n",
      "9 0:-1 1:0.472973 2:0.462185 3:-0.734513 4:-0.15495 5:-0.381977 6:-0.206057 7:-0.295466 \n",
      "11 1:0.472973 2:0.478992 3:-0.699115 4:-0.249159 5:-0.501009 6:-0.440421 7:-0.325361 \n",
      "15 0:-1 1:0.472973 2:0.478992 3:-0.654867 4:0.0713653 5:-0.222596 6:-0.0901909 7:0.0323866 \n",
      "9 0:-1 1:0.472973 2:0.394958 3:-0.734513 4:-0.0741987 5:-0.211836 6:0.158657 7:-0.355257 \n",
      "12 0:-1 1:0.472973 2:0.445378 3:-0.725664 4:-0.27218 5:-0.429724 6:-0.391705 7:-0.335326 \n",
      "11 0:-1 1:0.486486 2:0.478992 3:-0.725664 4:-0.258721 5:-0.346335 6:-0.443055 7:-0.362232 \n",
      "9 0:-1 1:0.486486 2:0.546218 3:-0.699115 4:-0.0586152 5:-0.231338 6:-0.210007 7:-0.283508 \n",
      "10 0:-1 1:0.486486 2:0.512605 3:-0.672566 4:-0.182221 5:-0.30195 6:-0.368005 7:-0.298455 \n",
      "11 1:0.486486 2:0.310924 3:-0.716814 4:-0.229325 5:-0.38265 6:-0.22054 7:-0.39711 \n",
      "11 1:0.486486 2:0.563025 3:-0.681416 4:-0.0423234 5:-0.349697 6:-0.0770244 7:-0.255605 \n",
      "11 1:0.486486 2:0.394958 3:-0.743363 4:-0.304409 5:-0.362475 6:-0.474654 7:-0.474838 \n",
      "9 0:-1 1:0.5 2:0.462185 3:-0.725664 4:-0.11422 5:-0.154001 6:-0.353522 7:-0.427005 \n",
      "11 1:0.513514 2:0.445378 3:-0.707965 4:-0.102178 5:-0.243443 6:-0.194207 7:-0.326358 \n",
      "11 1:0.513514 2:0.563025 3:-0.707965 4:-0.051886 5:-0.320108 6:-0.221856 7:-0.181863 \n",
      "11 1:0.513514 2:0.512605 3:-0.725664 4:-0.0880113 5:-0.202421 6:-0.174457 7:-0.315396 \n",
      "12 0:-1 1:0.513514 2:0.579832 3:-0.716814 4:-0.15495 5:-0.270343 6:-0.353522 7:-0.335326 \n",
      "12 0:-1 1:0.513514 2:0.495798 3:-0.707965 4:-0.099699 5:-0.122394 6:-0.440421 7:-0.275536 \n",
      "10 0:-1 1:0.513514 2:0.546218 3:-0.707965 4:-0.130866 5:-0.321453 6:-0.217907 7:-0.298455 \n",
      "11 0:-1 1:0.527027 2:0.596639 3:-0.707965 4:-0.158845 5:-0.360457 6:-0.211323 7:-0.305431 \n",
      "10 1:0.527027 2:0.428571 3:-0.743363 4:-0.211971 5:-0.31809 6:-0.369322 7:-0.325361 \n",
      "10 1:0.527027 2:0.546218 3:-0.707965 4:-0.0724278 5:-0.336247 6:-0.328506 7:-0.18585 \n",
      "10 0:1 1:0.527027 2:0.462185 3:-0.761062 4:-0.222242 5:-0.34499 6:-0.341672 7:-0.42003 \n",
      "9 0:-1 1:0.527027 2:0.462185 3:-0.725664 4:-0.202054 5:-0.359785 6:-0.292956 7:-0.325361 \n",
      "11 1:0.527027 2:0.445378 3:-0.672566 4:0.00407296 5:-0.0954943 6:-0.0888743 7:-0.35426 \n",
      "12 1:0.540541 2:0.529412 3:-0.681416 4:0.145741 5:0.0497646 6:-0.15339 7:-0.0712506 \n",
      "11 0:-1 1:0.540541 2:0.462185 3:-0.690265 4:-0.0664069 5:-0.123739 6:-0.375905 7:-0.328351 \n",
      "16 1:0.540541 2:0.563025 3:-0.628319 4:0.0989906 5:-0.172831 6:-0.0388413 7:-0.058296 \n",
      "10 0:1 1:0.554054 2:0.563025 3:-0.734513 4:-0.124491 5:-0.262273 6:-0.221856 7:-0.344295 \n",
      "13 1:0.554054 2:0.529412 3:-0.725664 4:-0.1592 5:-0.351715 6:-0.269256 7:-0.276532 \n",
      "10 1:0.554054 2:0.529412 3:-0.672566 4:-0.0274482 5:-0.287155 6:0.00987492 7:-0.208769 \n",
      "11 1:0.567568 2:0.546218 3:-0.681416 4:-0.0012396 5:-0.168124 6:-0.346939 7:-0.0124564 \n",
      "10 1:0.567568 2:0.579832 3:-0.690265 4:-0.0465734 5:-0.213853 6:-0.315339 7:-0.217738 \n",
      "9 0:-1 1:0.567568 2:0.563025 3:-0.699115 4:-0.190721 5:-0.2885 6:-0.414088 7:-0.309417 \n",
      "10 1:0.581081 2:0.613445 3:-0.637168 4:0.0196565 5:-0.204438 6:-0.270573 7:-0.0264076 \n",
      "11 0:-1 1:0.581081 2:0.529412 3:-0.690265 4:-0.138658 5:-0.321453 6:-0.203423 7:-0.265571 \n",
      "10 1:0.594595 2:0.495798 3:-0.734513 4:-0.117762 5:-0.379287 6:-0.223173 7:-0.286497 \n",
      "12 0:-1 1:0.594595 2:0.546218 3:-0.646018 4:-0.102178 5:-0.313383 6:-0.296906 7:-0.134031 \n",
      "12 0:-1 1:0.594595 2:0.579832 3:-0.681416 4:0.0108022 5:-0.0981843 6:-0.237656 7:-0.20578 \n",
      "12 1:0.608108 2:0.596639 3:-0.637168 4:-0.00867717 5:-0.136516 6:-0.352205 7:-0.173891 \n",
      "11 0:-1 1:0.621622 2:0.546218 3:-0.734513 4:-0.0720737 5:-0.253531 6:-0.25214 7:-0.18286 \n",
      "11 1:0.621622 2:0.529412 3:-0.672566 4:0.0419692 5:-0.154674 6:-0.20474 7:-0.156951 \n",
      "12 0:-1 1:0.635135 2:0.630252 3:-0.663717 4:0.14822 5:-0.0376597 6:-0.0691244 7:-0.0632785 \n",
      "10 0:-1 1:0.635135 2:0.630252 3:-0.725664 4:0.085178 5:-0.0988568 6:-0.00329164 7:-0.237668 \n",
      "10 0:-1 1:0.648649 2:0.613445 3:-0.725664 4:-0.020719 5:-0.111634 6:-0.436471 7:-0.190832 \n",
      "11 0:-1 1:0.662162 2:0.663866 3:-0.681416 4:0.196742 5:-0.106254 6:0.0572745 7:-0.00647733 \n",
      "13 0:-1 1:0.675676 2:0.647059 3:-0.672566 4:0.111741 5:-0.107599 6:0.00855826 7:-0.105132 \n",
      "13 1:0.689189 2:0.747899 3:-0.637168 4:0.254471 5:-0.187626 6:0.175774 7:0.0692576 \n",
      "12 0:-1 1:0.689189 2:0.663866 3:-0.690265 4:0.0189481 5:-0.118359 6:-0.215273 7:-0.255605 \n",
      "13 0:-1 1:0.689189 2:0.663866 3:-0.654867 4:0.149283 5:-0.0934768 6:-0.0875576 7:0.0632785 \n",
      "10 1:0.702703 2:0.613445 3:-0.610619 4:0.320347 5:0.248151 6:0.00855826 7:-0.127055 \n",
      "12 1:0.743243 2:0.747899 3:-0.681416 4:0.181866 5:-0.0168124 6:-0.0520079 7:-0.0044843 \n",
      "11 0:-1 1:0.743243 2:0.714286 3:-0.663717 4:0.47264 5:0.453262 6:0.132324 7:-0.000498256 \n",
      "11 1:0.756757 2:0.731092 3:-0.637168 4:0.145741 5:-0.000672495 6:-0.171824 7:-0.0303936 \n",
      "11 1:0.824324 2:0.663866 3:-0.654867 4:0.296618 5:0.114997 6:-0.0375247 7:-0.126059 \n",
      "13 0:-1 1:0.851351 2:0.848739 3:-0.619469 4:0.537808 5:0.0759919 6:0.292956 7:0.284504 \n",
      "4 0:1 1:-0.837838 2:-0.747899 3:-0.929204 4:-0.980875 5:-0.984533 6:-0.9842 7:-0.987045 \n",
      "4 0:1 1:-0.77027 2:-0.815126 3:-0.955752 4:-0.987604 5:-0.991258 6:-0.98815 7:-0.991031 \n",
      "5 0:1 1:-0.635135 2:-0.680672 3:-0.902655 4:-0.968479 5:-0.97848 6:-0.969717 7:-0.973094 \n",
      "6 0:1 1:-0.445946 2:-0.478992 3:-0.867257 4:-0.91677 5:-0.930061 6:-0.931534 7:-0.943199 \n",
      "7 0:1 1:-0.445946 2:-0.512605 3:-0.884956 4:-0.93802 5:-0.952925 6:-0.9526 7:-0.943199 \n",
      "6 0:1 1:-0.432432 2:-0.462185 3:-0.893805 4:-0.935187 5:-0.95965 6:-0.94075 7:-0.943199 \n",
      "5 0:1 1:-0.418919 2:-0.478992 3:-0.876106 4:-0.922437 5:-0.936785 6:-0.947334 7:-0.943199 \n",
      "6 0:1 1:-0.418919 2:-0.478992 3:-0.893805 4:-0.91677 5:-0.926026 6:-0.939434 7:-0.943199 \n",
      "6 0:1 1:-0.418919 2:-0.478992 3:-0.884956 4:-0.932708 5:-0.950908 6:-0.943384 7:-0.943199 \n",
      "6 0:1 1:-0.337838 2:-0.378151 3:-0.876106 4:-0.907207 5:-0.922663 6:-0.934167 7:-0.921276 \n",
      "7 0:1 1:-0.324324 2:-0.344538 3:-0.876106 4:-0.877811 5:-0.883658 6:-0.907834 7:-0.923269 \n",
      "6 0:1 1:-0.297297 2:-0.344538 3:-0.858407 4:-0.881353 5:-0.907868 6:-0.885451 7:-0.904335 \n",
      "7 0:1 1:-0.256757 2:-0.394958 3:-0.858407 4:-0.880999 5:-0.903833 6:-0.878868 7:-0.904335 \n",
      "6 0:1 1:-0.256757 2:-0.344538 3:-0.876106 4:-0.887728 5:-0.905178 6:-0.913101 7:-0.91131 \n",
      "9 0:1 1:-0.243243 2:-0.277311 3:-0.814159 4:-0.809456 5:-0.809684 6:-0.863068 7:-0.856502 \n",
      "6 0:1 1:-0.22973 2:-0.277311 3:-0.849558 4:-0.846644 5:-0.858104 6:-0.901251 7:-0.879422 \n",
      "7 0:1 1:-0.22973 2:-0.277311 3:-0.849558 4:-0.862582 5:-0.879623 6:-0.911784 7:-0.897359 \n",
      "6 0:1 1:-0.189189 2:-0.243697 3:-0.858407 4:-0.841332 5:-0.860121 6:-0.877551 7:-0.873443 \n",
      "7 0:1 1:-0.189189 2:-0.260504 3:-0.849558 4:-0.845582 5:-0.854741 6:-0.869651 7:-0.882412 \n",
      "7 0:1 1:-0.135135 2:-0.210084 3:-0.831858 4:-0.788914 5:-0.788837 6:-0.822251 7:-0.847534 \n",
      "7 0:1 1:-0.108108 2:-0.344538 3:-0.840708 4:-0.797769 5:-0.829186 6:-0.835418 7:-0.842551 \n",
      "8 0:1 1:-0.0810811 2:-0.092437 3:-0.80531 4:-0.777581 5:-0.815064 6:-0.791968 7:-0.818635 \n",
      "7 0:1 1:-0.0540541 2:-0.12605 3:-0.831858 4:-0.741101 5:-0.750504 6:-0.823568 7:-0.806677 \n",
      "8 0:1 1:-0.0405405 2:-0.109244 3:-0.80531 4:-0.741101 5:-0.776059 6:-0.732719 7:-0.793722 \n",
      "7 0:1 1:-0.027027 2:-0.092437 3:-0.787611 4:-0.756331 5:-0.787492 6:-0.780118 7:-0.813652 \n",
      "8 0:-1 1:0.0135135 2:-0.0756303 3:-0.814159 4:-0.650434 5:-0.65501 6:-0.785385 7:-0.745889 \n",
      "7 0:1 1:0.0405405 2:-0.00840336 3:-0.80531 4:-0.670267 5:-0.715535 6:-0.740619 7:-0.728949 \n",
      "10 0:-1 1:0.0675676 2:0.0420168 3:-0.761062 4:-0.631663 5:-0.67922 6:-0.599737 7:-0.714001 \n",
      "10 0:1 1:0.0675676 2:0.0756303 3:-0.814159 4:-0.689038 5:-0.776732 6:-0.773535 7:-0.714001 \n",
      "8 0:1 1:0.0810811 2:0.0420168 3:-0.787611 4:-0.634142 5:-0.640888 6:-0.712969 7:-0.730942 \n",
      "9 0:-1 1:0.162162 2:0.12605 3:-0.787611 4:-0.53887 5:-0.555481 6:-0.636603 7:-0.670154 \n",
      "12 0:-1 1:0.162162 2:0.142857 3:-0.761062 4:-0.582433 5:-0.613988 6:-0.655036 7:-0.63428 \n",
      "10 0:-1 1:0.162162 2:0.109244 3:-0.79646 4:-0.659642 5:-0.718897 6:-0.728769 7:-0.69706 \n",
      "9 0:1 1:0.175676 2:0.344538 3:-0.761062 4:-0.515849 5:-0.61466 6:-0.595787 7:-0.597409 \n",
      "9 0:-1 1:0.189189 2:0.159664 3:-0.752212 4:-0.552683 5:-0.613988 6:-0.619487 7:-0.668161 \n",
      "9 0:-1 1:0.216216 2:0.193277 3:-0.769912 4:-0.514432 5:-0.539341 6:-0.607637 7:-0.651221 \n",
      "9 1:0.22973 2:0.260504 3:-0.734513 4:-0.476536 5:-0.564223 6:-0.512837 7:-0.612357 \n",
      "9 1:0.22973 2:0.176471 3:-0.769912 4:-0.551266 5:-0.64694 6:-0.589203 7:-0.63428 \n",
      "11 0:-1 1:0.27027 2:0.294118 3:-0.752212 4:-0.406765 5:-0.522529 6:-0.499671 7:-0.527653 \n",
      "10 1:0.283784 2:0.394958 3:-0.734513 4:-0.349389 5:-0.488904 6:-0.360105 7:-0.469856 \n",
      "9 1:0.310811 2:0.193277 3:-0.716814 4:-0.419515 5:-0.541358 6:-0.516787 7:-0.498754 \n",
      "9 0:-1 1:0.324324 2:0.310924 3:-0.743363 4:-0.345847 5:-0.416947 6:-0.443055 7:-0.509716 \n",
      "10 1:0.337838 2:0.277311 3:-0.734513 4:-0.413848 5:-0.480161 6:-0.59052 7:-0.514699 \n",
      "9 0:-1 1:0.364865 2:0.361345 3:-0.716814 4:-0.248451 5:-0.311365 6:-0.289006 7:-0.479821 \n",
      "10 0:-1 1:0.391892 2:0.378151 3:-0.707965 4:-0.211617 5:-0.306658 6:-0.282423 7:-0.454908 \n",
      "10 1:0.418919 2:0.327731 3:-0.752212 4:-0.408536 5:-0.503699 6:-0.535221 7:-0.519681 \n",
      "12 0:-1 1:0.432432 2:0.310924 3:-0.752212 4:-0.305826 5:-0.423672 6:-0.452271 7:-0.415047 \n",
      "10 0:-1 1:0.445946 2:0.462185 3:-0.716814 4:-0.213742 5:-0.375925 6:-0.400922 7:-0.323368 \n",
      "14 1:0.486486 2:0.546218 3:-0.681416 4:-0.0462192 5:-0.294553 6:-0.337722 7:-0.220727 \n",
      "9 0:-1 1:0.581081 2:0.546218 3:-0.654867 4:0.107491 5:-0.0134499 6:-0.0717577 7:-0.233682 \n",
      "5 0:1 1:-0.513514 2:-0.546218 3:-0.893805 4:-0.940499 5:-0.947545 6:-0.9526 7:-0.953164 \n",
      "5 0:1 1:-0.472973 2:-0.529412 3:-0.884956 4:-0.925978 5:-0.937458 6:-0.942067 7:-0.946188 \n",
      "5 0:1 1:-0.445946 2:-0.462185 3:-0.858407 4:-0.907916 5:-0.904506 6:-0.943384 7:-0.93722 \n",
      "6 0:1 1:-0.432432 2:-0.462185 3:-0.876106 4:-0.92527 5:-0.932751 6:-0.942067 7:-0.949178 \n",
      "8 0:1 1:-0.337838 2:-0.327731 3:-0.849558 4:-0.877811 5:-0.904506 6:-0.914417 7:-0.889387 \n",
      "8 0:1 1:-0.324324 2:-0.378151 3:-0.876106 4:-0.893749 5:-0.925353 6:-0.921001 7:-0.895366 \n",
      "8 0:1 1:-0.162162 2:-0.243697 3:-0.823009 4:-0.806269 5:-0.825824 6:-0.840685 7:-0.858495 \n",
      "7 0:1 1:-0.135135 2:-0.193277 3:-0.823009 4:-0.793873 5:-0.813046 6:-0.838051 7:-0.839562 \n",
      "7 1:-0.121622 2:-0.159664 3:-0.716814 4:-0.740747 5:-0.768662 6:-0.815668 7:-0.793722 \n",
      "8 0:1 1:-0.108108 2:-0.142857 3:-0.840708 4:-0.780414 5:-0.815736 6:-0.843318 7:-0.829596 \n",
      "7 0:1 1:-0.0810811 2:-0.159664 3:-0.787611 4:-0.763414 5:-0.779422 6:-0.801185 7:-0.842551 \n",
      "6 0:1 1:-0.0675676 2:-0.12605 3:-0.79646 4:-0.749956 5:-0.746469 6:-0.830151 7:-0.829596 \n",
      "9 0:1 1:-0.0135135 2:-0.159664 3:-0.79646 4:-0.732956 5:-0.783457 6:-0.761685 7:-0.783757 \n",
      "8 0:1 2:-0.109244 3:-0.787611 4:-0.733664 5:-0.796907 6:-0.784068 7:-0.763827 \n",
      "8 0:-1 1:0.0135135 2:-0.00840336 3:-0.769912 4:-0.671684 5:-0.72226 6:-0.726136 7:-0.733931 \n",
      "8 1:0.027027 2:0.00840336 3:1 4:-0.580662 5:-0.554808 6:-0.695853 7:-0.736921 \n",
      "8 0:-1 1:0.0405405 2:-0.0252101 3:-0.787611 4:-0.651851 5:-0.67384 6:-0.691903 7:-0.73991 \n",
      "7 0:-1 1:0.0405405 2:-0.0252101 3:-0.80531 4:-0.675934 5:-0.685272 6:-0.768269 7:-0.771799 \n",
      "7 0:-1 1:0.0540541 2:0.0252101 3:-0.80531 4:-0.650434 5:-0.642905 6:-0.777485 7:-0.761834 \n",
      "9 0:1 1:0.0540541 2:0.00840336 3:-0.840708 4:-0.695059 5:-0.731675 6:-0.806452 7:-0.748879 \n",
      "7 1:0.0810811 2:0.092437 3:-0.752212 4:-0.51337 5:-0.575656 6:-0.655036 7:-0.613353 \n",
      "8 0:1 1:0.0945946 2:-0.00840336 3:-0.761062 4:-0.614308 5:-0.63349 6:-0.739302 7:-0.688092 \n",
      "8 0:-1 1:0.108108 2:0.12605 3:-0.761062 4:-0.56437 5:-0.665098 6:-0.647136 7:-0.67713 \n",
      "8 0:1 1:0.121622 2:0.0588235 3:-0.80531 4:-0.620329 5:-0.636853 6:-0.728769 7:-0.725959 \n",
      "9 0:-1 1:0.148649 2:0.12605 3:-0.761062 4:-0.447848 5:-0.515804 6:-0.58657 7:-0.527653 \n",
      "8 1:0.148649 2:0.092437 3:-0.752212 4:-0.551266 5:-0.628783 6:-0.624753 7:-0.647235 \n",
      "7 0:-1 1:0.162162 2:0.109244 3:-0.769912 4:-0.545599 5:-0.579691 6:-0.608953 7:-0.701046 \n",
      "8 0:-1 1:0.216216 2:0.109244 3:-0.823009 4:-0.6391 5:-0.670477 6:-0.736669 7:-0.713004 \n",
      "8 0:-1 1:0.243243 2:0.226891 3:-0.778761 4:-0.478661 5:-0.523874 6:-0.502304 7:-0.645242 \n",
      "8 1:0.243243 2:0.226891 3:-0.769912 4:-0.506287 5:-0.581036 6:-0.589203 7:-0.597409 \n",
      "10 1:0.256757 2:0.109244 3:-0.752212 4:-0.459182 5:-0.562206 6:-0.695853 7:-0.531639 \n",
      "9 1:0.256757 2:0.226891 3:-0.769912 4:-0.469807 5:-0.506389 6:-0.560237 7:-0.635277 \n",
      "9 1:0.27027 2:0.260504 3:-0.716814 4:-0.403577 5:-0.470746 6:-0.512837 7:-0.542601 \n",
      "9 0:-1 1:0.283784 2:0.193277 3:-0.769912 4:-0.384806 5:-0.402152 6:-0.444371 7:-0.578475 \n",
      "11 0:1 1:0.283784 2:0.226891 3:-0.79646 4:-0.528245 5:-0.607935 6:-0.640553 7:-0.586447 \n",
      "9 1:0.324324 2:0.294118 3:-0.761062 4:-0.413494 5:-0.472764 6:-0.544437 7:-0.52865 \n",
      "8 0:-1 1:0.364865 2:0.327731 3:-0.787611 4:-0.386223 5:-0.439139 6:-0.612903 7:-0.483807 \n",
      "7 1:0.364865 2:0.277311 3:-0.734513 4:-0.407119 5:-0.532616 6:-0.456221 7:-0.620329 \n",
      "11 1:0.378378 2:0.445378 3:-0.734513 4:-0.237117 5:-0.443847 6:-0.444371 7:-0.293473 \n",
      "9 0:-1 1:0.405405 2:0.378151 3:-0.734513 4:-0.350452 5:-0.418292 6:-0.536537 7:-0.480817 \n",
      "10 1:0.418919 2:0.394958 3:-0.663717 4:-0.197804 5:-0.33961 6:-0.317972 7:-0.330344 \n",
      "8 1:0.445946 2:0.260504 3:-0.752212 4:-0.357535 5:-0.412239 6:-0.474654 7:-0.564524 \n",
      "8 0:-1 1:0.445946 2:0.428571 3:-0.707965 4:-0.120595 5:-0.251513 6:-0.296906 7:-0.261584 \n",
      "9 1:0.472973 2:0.462185 3:-0.716814 4:-0.253409 5:-0.338265 6:-0.358789 7:-0.45989 \n",
      "9 0:-1 1:0.540541 2:0.478992 3:-0.734513 4:-0.144679 5:-0.190316 6:-0.415405 7:-0.327354 \n",
      "8 0:-1 1:0.554054 2:0.495798 3:-0.752212 4:-0.124491 5:-0.172159 6:-0.381172 7:-0.365222 \n",
      "10 1:0.594595 2:0.579832 3:-0.628319 4:0.163095 5:0.0988568 6:-0.107307 7:-0.150972 \n",
      "11 0:-1 1:0.648649 2:0.663866 3:-0.646018 4:0.254117 5:0.0921318 6:0.0177749 7:-0.0264076 \n",
      "12 1:0.662162 2:0.630252 3:-0.654867 4:-0.11422 5:-0.0195024 6:0.0454246 7:-0.0822123 \n",
      "10 1:0.702703 2:0.731092 3:-0.672566 4:0.245971 5:0.00336247 6:-0.0204082 7:-0.0303936 \n",
      "10 1:0.716216 2:0.495798 3:-0.734513 4:-0.0688861 5:-0.0820444 6:-0.260039 7:-0.444943 \n",
      "11 0:-1 1:0.743243 2:0.781513 3:-0.610619 4:0.354347 5:0.229993 6:0.178407 7:-0.0483308 \n",
      "12 1:0.743243 2:0.747899 3:-0.619469 4:0.486099 5:0.150639 6:0.269256 7:0.196811 \n",
      "10 1:0.77027 2:0.680672 3:-0.681416 4:0.195325 5:-0.119704 6:-0.483871 7:-0.019432 \n",
      "17 0:-1 1:0.891892 2:0.731092 3:-0.610619 4:0.437932 5:-0.0127774 6:0.250823 7:0.309417 \n",
      "8 1:0.162162 2:0.12605 3:-0.79646 4:-0.533912 5:-0.591796 6:-0.591837 7:-0.65421 \n",
      "10 0:-1 1:0.22973 2:0.243697 3:-0.769912 4:-0.473349 5:-0.598521 6:-0.644503 7:-0.514699 \n",
      "11 1:0.162162 2:0.109244 3:-0.79646 4:-0.565079 5:-0.674512 6:-0.718236 7:-0.584454 \n",
      "7 0:1 1:-0.108108 2:-0.159664 3:-0.840708 4:-0.80131 5:-0.848016 6:-0.849901 7:-0.813652 \n",
      "7 0:-1 1:-0.0810811 2:-0.176471 3:-0.823009 4:-0.763768 5:-0.793544 6:-0.820935 7:-0.813652 \n",
      "11 0:-1 1:0.148649 2:0.12605 3:-0.743363 4:-0.540287 5:-0.634163 6:-0.65372 7:-0.564524 \n",
      "7 0:-1 1:-0.0540541 2:-0.0756303 3:-0.858407 4:-0.745706 5:-0.821116 6:-0.784068 7:-0.753861 \n",
      "8 0:-1 1:0.0675676 2:-0.00840336 3:-0.823009 4:-0.663184 5:-0.747814 6:-0.768269 7:-0.65421 \n",
      "10 1:-0.121622 2:-0.142857 3:-0.79646 4:-0.755977 5:-0.802959 6:-0.818302 7:-0.773792 \n",
      "10 0:1 1:-0.202703 2:-0.210084 3:-0.823009 4:-0.824332 5:-0.863484 6:-0.868334 7:-0.833582 \n",
      "19 0:-1 1:0.148649 2:0.092437 3:-0.725664 4:-0.533912 5:-0.64425 6:-0.64187 7:-0.574489 \n",
      "9 0:1 1:-0.0945946 2:-0.142857 3:-0.80531 4:-0.778289 5:-0.834566 6:-0.785385 7:-0.813652 \n",
      "9 0:-1 1:-0.189189 2:-0.210084 3:-0.823009 4:-0.805915 5:-0.843309 6:-0.852535 7:-0.833582 \n",
      "11 1:0.121622 2:0.109244 3:-0.778761 4:-0.619267 5:-0.70881 6:-0.664253 7:-0.674141 \n",
      "10 0:-1 1:0.378378 2:0.428571 3:-0.672566 4:-0.264742 5:-0.417619 6:-0.303489 7:-0.434978 \n",
      "14 0:-1 1:0.405405 2:0.344538 3:-0.725664 4:-0.264034 5:-0.441829 6:-0.447005 7:-0.275536 \n",
      "15 1:0.621622 2:0.663866 3:-0.681416 4:0.194617 5:-0.245461 6:-0.0256748 7:0.192825 \n",
      "27 0:-1 1:0.594595 2:0.613445 3:-0.60177 4:0.545245 5:0.0121049 6:0.0283081 7:0.760837 \n",
      "13 0:-1 1:0.472973 2:0.462185 3:-0.699115 4:-0.14397 5:-0.303968 6:-0.328506 7:-0.335326 \n",
      "5 0:1 1:-0.324324 2:-0.344538 3:-0.902655 4:-0.883832 5:-0.899126 6:-0.867018 7:-0.913303 \n",
      "9 0:1 1:0.027027 2:0.00840336 3:-0.858407 4:-0.681247 5:-0.710155 6:-0.739302 7:-0.753861 \n",
      "8 0:-1 1:0.216216 2:0.176471 3:-0.769912 4:-0.492474 5:-0.562206 6:-0.481238 7:-0.65421 \n",
      "7 0:1 1:-0.162162 2:-0.210084 3:-0.840708 4:-0.837082 5:-0.886348 6:-0.870968 7:-0.843548 \n",
      "4 0:1 1:-0.851351 2:-0.865546 3:-0.938053 4:-0.993979 5:-0.99462 6:-0.9842 7:-0.996014 \n",
      "3 0:1 1:-0.716216 2:-0.747899 3:-0.920354 4:-0.981937 5:-0.984533 6:-0.974984 7:-0.985052 \n",
      "6 0:1 1:-0.364865 2:-0.428571 3:-0.911504 4:-0.899061 5:-0.910558 6:-0.899934 7:-0.913303 \n",
      "7 1:-0.189189 2:-0.210084 3:-0.858407 4:-0.801665 5:-0.812374 6:-0.810402 7:-0.843548 \n",
      "8 1:0.0945946 2:0.092437 3:-0.787611 4:-0.570746 5:-0.637525 6:-0.631336 7:-0.63428 \n",
      "9 0:1 1:0.027027 2:0.0588235 3:-0.778761 4:-0.694705 5:-0.731002 6:-0.668203 7:-0.714001 \n",
      "7 0:-1 1:-0.0540541 2:-0.092437 3:-0.823009 4:-0.768018 5:-0.817754 6:-0.810402 7:-0.783757 \n",
      "9 0:1 1:0.0810811 2:0.0252101 3:-0.80531 4:-0.678767 5:-0.763282 6:-0.747202 7:-0.714001 \n",
      "11 1:-0.027027 2:-0.00840336 3:-0.787611 4:-0.676642 5:-0.743107 6:-0.737986 7:-0.743896 \n",
      "6 1:-0.418919 2:-0.478992 3:-0.867257 4:-0.806623 5:-0.849361 6:-0.823568 7:-0.933234 \n",
      "7 0:-1 1:-0.162162 2:-0.193277 3:-0.831858 4:-0.764123 5:-0.803631 6:-0.753785 7:-0.823617 \n",
      "8 0:-1 1:0.0675676 2:0.0756303 3:-0.79646 4:-0.699309 5:-0.774714 6:-0.802502 7:-0.704036 \n",
      "10 1:0.148649 2:0.159664 3:-0.778761 4:-0.593058 5:-0.67922 6:-0.669519 7:-0.63428 \n",
      "7 0:1 1:-0.121622 2:-0.142857 3:-0.823009 4:-0.911457 5:-0.858776 6:-0.814352 7:-0.833582 \n",
      "12 0:-1 1:0.472973 2:0.529412 3:-0.690265 4:-0.186471 5:-0.412912 6:-0.404872 7:-0.20578 \n",
      "12 0:-1 1:0.405405 2:0.394958 3:-0.734513 4:-0.369931 5:-0.518494 6:-0.447005 7:-0.514699 \n",
      "9 0:-1 1:0.378378 2:0.344538 3:-0.752212 4:-0.314326 5:-0.37996 6:-0.514154 7:-0.415047 \n",
      "6 0:-1 1:-0.337838 2:-0.378151 3:-0.858407 4:-0.873915 5:-0.893746 6:-0.899934 7:-0.893373 \n",
      "10 1:0.202703 2:0.193277 3:-0.778761 4:-0.506641 5:-0.605245 6:-0.573404 7:-0.574489 \n",
      "9 0:-1 1:-0.0135135 2:-0.00840336 3:-0.80531 4:-0.676642 5:-0.732347 6:-0.768269 7:-0.743896 \n",
      "8 1:-0.0135135 2:-0.0756303 3:-0.79646 4:-0.717726 5:-0.809011 6:-0.703752 7:-0.763827 \n",
      "12 0:-1 1:0.324324 2:0.243697 3:-0.823009 4:-0.495307 5:-0.590451 6:-0.564187 7:-0.644245 \n",
      "9 1:0.310811 2:0.243697 3:-0.778761 4:-0.341243 5:-0.515804 6:-0.440421 7:-0.335326 \n",
      "10 1:0.391892 2:0.344538 3:-0.690265 4:-0.31716 5:-0.475454 6:-0.354839 7:-0.385152 \n",
      "16 1:0.337838 2:0.378151 3:-0.681416 4:-0.29343 5:-0.456624 6:-0.271889 7:-0.415047 \n",
      "14 0:-1 1:0.635135 2:0.596639 3:-0.637168 4:0.058261 5:-0.217888 6:-0.113891 7:-0.0762332 \n",
      "10 1:0.0135135 2:0.0252101 3:-0.778761 4:-0.642642 5:-0.702757 6:-0.724819 7:-0.684106 \n",
      "7 0:1 1:-0.337838 2:-0.378151 3:-0.867257 4:-0.87852 5:-0.899126 6:-0.907834 7:-0.903338 \n",
      "9 0:1 1:0.0405405 2:-0.00840336 3:-0.80531 4:-0.721976 5:-0.774714 6:-0.773535 7:-0.753861 \n",
      "10 0:-1 1:0.0675676 2:0.0588235 3:-0.814159 4:-0.670976 5:-0.728985 6:-0.734036 7:-0.694071 \n",
      "9 0:-1 1:0.027027 2:-0.00840336 3:-0.814159 4:-0.717372 5:-0.789509 6:-0.782752 7:-0.733931 \n",
      "9 1:-0.0810811 2:-0.092437 3:-0.79646 4:-0.756685 5:-0.812374 6:-0.799868 7:-0.783757 \n",
      "11 0:-1 1:0.0540541 2:-0.00840336 3:-0.787611 4:-0.632725 5:-0.73033 6:-0.573404 7:-0.63428 \n",
      "12 0:-1 1:0.0405405 2:0.0756303 3:-0.761062 4:-0.651851 5:-0.751177 6:-0.778802 7:-0.664175 \n",
      "8 0:-1 1:-0.0810811 2:-0.142857 3:-0.840708 4:-0.77156 5:-0.825824 6:-0.807768 7:-0.773792 \n",
      "7 0:-1 1:-0.472973 2:-0.529412 3:-0.876106 4:-0.926333 5:-0.938803 6:-0.953917 7:-0.931241 \n",
      "10 0:-1 2:0.00840336 3:-0.80531 4:-0.688684 5:-0.758574 6:-0.728769 7:-0.703039 \n",
      "18 1:0.810811 2:0.781513 3:-0.663717 4:0.391181 5:0.133154 6:0.14944 7:0.163926 \n",
      "8 1:-0.121622 2:-0.176471 3:-0.79646 4:-0.787144 5:-0.821789 6:-0.878868 7:-0.816642 \n",
      "5 0:1 1:-0.445946 2:-0.512605 3:-0.867257 4:-0.914645 5:-0.928043 6:-0.971034 7:-0.933234 \n",
      "10 0:-1 1:0.283784 2:0.294118 3:-0.761062 4:-0.378785 5:-0.506389 6:-0.449638 7:-0.474838 \n",
      "16 0:-1 1:0.364865 2:0.361345 3:-0.707965 4:-0.131928 5:-0.365165 6:-0.483871 7:-0.136024 \n",
      "12 0:-1 1:0.445946 2:0.495798 3:-0.707965 4:-0.100761 5:-0.340282 6:-0.514154 7:-0.0264076 \n",
      "17 0:-1 1:0.472973 2:0.478992 3:-0.690265 4:0.277847 5:-0.136516 6:-0.136274 7:0.441953 \n",
      "14 0:-1 1:0.310811 2:0.226891 3:-0.654867 4:-0.428723 5:-0.594486 6:-0.528637 7:-0.434978 \n",
      "12 1:0.527027 2:0.529412 3:-0.646018 4:-0.0164689 5:-0.180901 6:-0.128374 7:-0.18585 \n",
      "19 0:-1 1:0.662162 2:0.663866 3:-0.646018 4:0.306534 5:-0.0168124 6:0.241606 7:0.133034 \n",
      "17 1:0.72973 2:0.714286 3:-0.575221 4:0.556579 5:-0.0268998 6:0.223173 7:0.760837 \n",
      "11 1:0.716216 2:0.714286 3:-0.654867 4:0.285638 5:0.0544721 6:0.294273 7:-0.0264076 \n",
      "9 1:0.283784 2:0.394958 3:-0.734513 4:-0.366035 5:-0.494284 6:-0.516787 7:-0.425012 \n",
      "5 0:-1 1:-0.189189 2:-0.159664 3:-0.840708 4:-0.77156 5:-0.813719 6:-0.852535 7:-0.813652 \n",
      "9 1:0.445946 2:0.327731 3:-0.716814 4:-0.196742 5:-0.444519 6:-0.182357 7:-0.405082 \n",
      "6 0:1 1:-0.175676 2:-0.243697 3:-0.849558 4:-0.807685 5:-0.846671 6:-0.840685 7:-0.833582 \n",
      "5 1:-0.202703 2:-0.260504 3:-0.849558 4:-0.831061 5:-0.861466 6:-0.860434 7:-0.863478 \n",
      "6 0:-1 1:-0.297297 2:-0.394958 3:-0.849558 4:-0.891978 5:-0.912576 6:-0.910467 7:-0.913303 \n",
      "4 0:1 1:-0.756757 2:-0.798319 3:-0.973451 4:-0.991146 5:-0.993948 6:-0.993417 7:-0.993024 \n",
      "7 0:-1 1:-0.432432 2:-0.478992 3:-0.867257 4:-0.917478 5:-0.927371 6:-0.926267 7:-0.923269 \n",
      "6 0:1 1:-0.689189 2:-0.747899 3:-0.946903 4:-0.980521 5:-0.980498 6:-0.961817 7:-0.983059 \n",
      "5 0:1 1:-0.621622 2:-0.680672 3:-0.946903 4:-0.974146 5:-0.985878 6:-0.98815 7:-0.983059 \n",
      "10 0:-1 1:0.405405 2:0.378151 3:-0.778761 4:-0.435452 5:-0.564896 6:-0.474654 7:-0.544594 \n",
      "13 1:0.540541 2:0.495798 3:-0.699115 4:-0.162387 5:-0.355077 6:-0.279789 7:-0.295466 \n",
      "12 0:-1 1:0.351351 2:0.327731 3:-0.672566 4:-0.346202 5:-0.541358 6:-0.482554 7:-0.305431 \n",
      "20 1:0.337838 2:0.327731 3:-0.699115 4:-0.223659 5:-0.444519 6:-0.508887 7:-0.195815 \n",
      "14 1:0.364865 2:0.327731 3:-0.584071 4:-0.242784 5:-0.597848 6:-0.458855 7:-0.215745 \n",
      "15 1:0.405405 2:0.428571 3:-0.646018 4:-0.310784 5:-0.519839 6:-0.465438 7:-0.325361 \n",
      "14 1:0.405405 2:0.394958 3:-0.557522 4:-0.0926155 5:-0.37996 6:-0.349572 7:-0.116094 \n",
      "21 1:0.486486 2:0.226891 3:-0.707965 4:-0.25093 5:-0.519839 6:-0.56682 7:-0.116094 \n",
      "16 0:-1 1:0.243243 2:0.226891 3:-0.707965 4:-0.350097 5:-0.550101 6:-0.478604 7:-0.484803 \n",
      "13 0:-1 1:0.283784 2:0.260504 3:-0.716814 4:-0.343014 5:-0.574983 6:-0.544437 7:-0.295466 \n",
      "6 0:-1 1:0.135135 2:0.159664 3:-0.725664 4:-0.428723 5:-0.685945 6:-0.697169 7:-0.305431 \n",
      "6 0:1 1:-0.337838 2:-0.394958 3:-0.858407 4:-0.896228 5:-0.915266 6:-0.919684 7:-0.913303 \n",
      "9 0:-1 2:-0.0420168 3:-0.787611 4:-0.684434 5:-0.741762 6:-0.728769 7:-0.743896 \n",
      "9 1:0.202703 2:0.159664 3:-0.778761 4:-0.515141 5:-0.604573 6:-0.549704 7:-0.63428 \n",
      "14 0:-1 1:0.135135 2:0.109244 3:-0.761062 4:-0.552683 5:-0.732347 6:-0.678736 7:-0.484803 \n",
      "12 0:-1 1:0.0675676 2:0.0588235 3:-0.761062 4:-0.613954 5:-0.702757 6:-0.652403 7:-0.664175 \n",
      "10 1:0.121622 2:0.0588235 3:-0.752212 4:-0.587037 5:-0.674512 6:-0.698486 7:-0.61435 \n",
      "11 0:-1 1:0.364865 2:0.394958 3:-0.707965 4:-0.344785 5:-0.568931 6:-0.478604 7:-0.375187 \n",
      "15 0:-1 1:0.540541 2:0.478992 3:-0.672566 4:0.0564902 5:-0.293208 6:-0.267939 7:-0.0961634 \n",
      "14 1:0.351351 2:0.445378 3:-0.707965 4:-0.264388 5:-0.437794 6:-0.306122 7:-0.405082 \n",
      "6 0:1 1:-0.621622 2:-0.613445 3:-0.902655 4:-0.958562 5:-0.973773 6:-0.96445 7:-0.963129 \n",
      "13 0:1 1:-0.0405405 2:-0.092437 3:-0.80531 4:-0.741101 5:-0.819099 6:-0.755102 7:-0.763827 \n",
      "5 0:1 1:-0.5 2:-0.462185 3:-0.858407 4:-0.931291 5:-0.95158 6:-0.934167 7:-0.913303 \n",
      "11 0:1 1:-0.202703 2:-0.243697 3:-0.840708 4:-0.836373 5:-0.879623 6:-0.857801 7:-0.863478 \n",
      "10 0:1 1:-0.108108 2:-0.159664 3:-0.814159 4:-0.744643 5:-0.790854 6:-0.815668 7:-0.753861 \n",
      "6 0:1 1:-0.472973 2:-0.546218 3:-0.858407 4:-0.944041 5:-0.965703 6:-0.949967 7:-0.943199 \n",
      "21 1:0.635135 2:0.663866 3:-0.646018 4:0.129095 5:-0.295225 6:0.0717577 7:0.16293 \n",
      "13 1:0.554054 2:0.546218 3:-0.654867 4:-0.00938551 5:-0.302623 6:-0.0533246 7:-0.126059 \n",
      "25 1:0.540541 2:0.462185 3:-0.619469 4:-0.00548964 5:-0.427707 6:-0.399605 7:0.0134529 \n",
      "19 0:-1 1:0.337838 2:0.176471 3:-0.716814 4:-0.346556 5:-0.537996 6:-0.425938 7:-0.415047 \n",
      "18 0:-1 1:0.459459 2:0.428571 3:-0.663717 4:-0.0380733 5:-0.287828 6:-0.375905 7:-0.0662681 \n",
      "7 0:-1 1:-0.0675676 2:-0.0252101 3:-0.814159 4:-0.69683 5:-0.765972 6:-0.748519 7:-0.743896 \n",
      "6 0:1 1:-0.459459 2:-0.445378 3:-0.858407 4:-0.904728 5:-0.925353 6:-0.926267 7:-0.919283 \n",
      "5 1:-0.418919 2:-0.428571 3:-0.867257 4:-0.902249 5:-0.932078 6:-0.939434 7:-0.923269 \n",
      "8 0:-1 1:-0.0675676 2:-0.0420168 3:-0.79646 4:-0.702851 5:-0.765972 6:-0.756419 7:-0.733931 \n",
      "16 1:0.486486 2:0.579832 3:-0.619469 4:0.115282 5:-0.313383 6:-0.317972 7:0.322372 \n",
      "27 1:0.283784 2:0.378151 3:-0.681416 4:-0.142554 5:-0.564896 6:-0.461488 7:0.0433483 \n",
      "18 0:-1 1:0.581081 2:0.512605 3:-0.646018 4:0.153533 5:-0.347007 6:-0.219223 7:0.212755 \n",
      "17 0:-1 1:0.324324 2:0.394958 3:-0.654867 4:-0.192492 5:-0.480834 6:-0.321922 7:-0.305431 \n",
      "13 1:0.405405 2:0.478992 3:-0.584071 4:-0.0338233 5:-0.320108 6:-0.424621 7:0.0333832 \n",
      "17 0:-1 1:0.5 2:0.529412 3:-0.59292 4:0.0887197 5:-0.243443 6:-0.260039 7:0.133034 \n",
      "8 1:-0.0405405 2:-0.092437 3:-0.787611 4:-0.686205 5:-0.779422 6:-0.739302 7:-0.694071 \n",
      "10 1:0.027027 2:-0.00840336 3:-0.752212 4:-0.595892 5:-0.737054 6:-0.652403 7:-0.65421 \n",
      "7 0:1 1:-0.310811 2:-0.310924 3:-0.858407 4:-0.866832 5:-0.898453 6:-0.899934 7:-0.873443 \n",
      "13 1:0.189189 2:0.210084 3:-0.769912 4:-0.460244 5:-0.630128 6:-0.485188 7:-0.504733 \n",
      "14 0:-1 1:0.135135 2:0.12605 3:-0.734513 4:-0.397202 5:-0.559516 6:-0.503621 7:-0.464873 \n",
      "13 1:0.108108 2:0.0756303 3:-0.743363 4:-0.584558 5:-0.680565 6:-0.697169 7:-0.624315 \n",
      "8 1:0.243243 2:0.361345 3:-0.743363 4:-0.443598 5:-0.544721 6:-0.473338 7:-0.604385 \n",
      "17 0:-1 1:0.364865 2:0.378151 3:-0.690265 4:-0.268284 5:-0.462004 6:-0.510204 7:-0.235675 \n",
      "13 1:0.486486 2:0.579832 3:-0.654867 4:-0.04374 5:-0.395427 6:-0.357472 7:0.0533134 \n",
      "14 1:0.297297 2:0.344538 3:-0.681416 4:-0.322826 5:-0.603228 6:-0.487821 7:-0.22571 \n",
      "9 1:0.283784 2:0.243697 3:-0.743363 4:-0.436869 5:-0.601883 6:-0.60632 7:-0.474838 \n",
      "13 0:-1 1:0.391892 2:0.411765 3:-0.725664 4:-0.394369 5:-0.522529 6:-0.54312 7:-0.444943 \n",
      "7 0:1 1:-0.243243 2:-0.243697 3:-0.80531 4:-0.843103 5:-0.891728 6:-0.863068 7:-0.843548 \n",
      "7 0:1 1:-0.459459 2:-0.512605 3:-0.867257 4:-0.940499 5:-0.960323 6:-0.951284 7:-0.943199 \n",
      "12 1:0.162162 2:0.12605 3:-0.690265 4:-0.511245 5:-0.642233 6:-0.60632 7:-0.574489 \n",
      "7 0:-1 1:-0.202703 2:-0.243697 3:-0.831858 4:-0.843811 5:-0.893073 6:-0.867018 7:-0.853513 \n",
      "13 0:-1 1:0.297297 2:0.260504 3:-0.707965 4:-0.464849 5:-0.63349 6:-0.57077 7:-0.454908 \n",
      "9 1:0.162162 2:0.159664 3:-0.707965 4:-0.485036 5:-0.64156 6:-0.593153 7:-0.504733 \n",
      "9 1:0.310811 2:0.310924 3:-0.681416 4:-0.361785 5:-0.520511 6:-0.462804 7:-0.415047 \n",
      "17 0:-1 1:0.405405 2:0.411765 3:-0.699115 4:-0.224721 5:-0.437794 6:-0.398288 7:-0.305431 \n",
      "14 1:0.337838 2:0.327731 3:-0.707965 4:-0.361785 5:-0.556826 6:-0.515471 7:-0.415047 \n",
      "13 0:-1 1:0.418919 2:0.428571 3:-0.690265 4:-0.130866 5:-0.446537 6:-0.281106 7:-0.175884 \n",
      "15 1:0.310811 2:0.277311 3:-0.672566 4:-0.217992 5:-0.433759 6:-0.360105 7:-0.345291 \n",
      "17 0:-1 1:0.378378 2:0.378151 3:-0.663717 4:-0.17195 5:-0.476126 6:-0.381172 7:-0.20578 \n",
      "8 0:1 1:0.0405405 2:-0.0588235 3:-0.80531 4:-0.686913 5:-0.698722 6:-0.805135 7:-0.783757 \n",
      "8 1:0.0405405 2:0.0252101 3:-0.79646 4:-0.664601 5:-0.718225 6:-0.724819 7:-0.684106 \n",
      "12 0:-1 1:-0.0810811 2:-0.12605 3:-0.778761 4:-0.72658 5:-0.909886 6:-0.764319 7:-0.753861 \n",
      "11 1:-0.027027 2:-0.109244 3:-0.787611 4:-0.73331 5:-0.796907 6:-0.760369 7:-0.753861 \n",
      "7 1:0.0810811 2:0.092437 3:-0.761062 4:-0.657163 5:-0.767989 6:-0.816985 7:-0.63428 \n",
      "10 0:-1 1:0.0540541 2:0.0252101 3:-0.769912 4:-0.628475 5:-0.718225 6:-0.689269 7:-0.674141 \n",
      "11 0:1 1:-0.243243 2:-0.243697 3:-0.823009 4:-0.840269 5:-0.875588 6:-0.881501 7:-0.833582 \n",
      "9 0:-1 1:0.0405405 2:0.0756303 3:-0.752212 4:-0.639809 5:-0.743107 6:-0.726136 7:-0.59442 \n",
      "10 1:-0.175676 2:-0.092437 3:-0.80531 4:-0.781477 5:-0.839946 6:-0.806452 7:-0.793722 \n",
      "9 1:0.0675676 2:0.0420168 3:-0.787611 4:-0.616788 5:-0.69267 6:-0.607637 7:-0.704036 \n",
      "6 0:-1 1:-0.22973 2:-0.277311 3:-0.840708 4:-0.843811 5:-0.889711 6:-0.861751 7:-0.853513 \n",
      "12 1:0.378378 2:0.344538 3:-0.707965 4:-0.294493 5:-0.537323 6:-0.344305 7:-0.375187 \n",
      "11 0:-1 1:0.567568 2:0.798319 3:-0.646018 4:0.0933239 5:-0.121722 6:-0.00987492 7:-0.175884 \n",
      "13 0:-1 1:0.418919 2:0.445378 3:-0.690265 4:-0.103595 5:-0.329523 6:-0.260039 7:-0.24564 \n",
      "14 1:0.337838 2:0.361345 3:-0.699115 4:-0.222242 5:-0.446537 6:-0.420671 7:-0.24564 \n",
      "11 1:0.540541 2:0.495798 3:-0.646018 4:0.010448 5:-0.141896 6:-0.198157 7:-0.285501 \n",
      "14 0:-1 1:0.554054 2:0.478992 3:-0.681416 4:0.268638 5:0.0753194 6:-0.108624 7:0.0533134 \n",
      "12 0:-1 1:0.175676 2:0.142857 3:-0.743363 4:-0.563308 5:-0.710827 6:-0.636603 7:-0.524664 \n",
      "8 0:-1 1:0.202703 2:0.092437 3:-0.761062 4:-0.588808 5:-0.664425 6:-0.589203 7:-0.65421 \n",
      "13 0:-1 1:0.135135 2:0.210084 3:-0.707965 4:-0.471224 5:-0.646268 6:-0.648453 7:-0.434978 \n",
      "8 0:-1 1:-0.0405405 2:-0.0588235 3:-0.79646 4:-0.71383 5:-0.778077 6:-0.755102 7:-0.733931 \n",
      "13 1:0.391892 2:0.378151 3:-0.716814 4:-0.221888 5:-0.32078 6:-0.336406 7:-0.415047 \n",
      "10 0:-1 1:0.283784 2:0.361345 3:-0.690265 4:-0.385869 5:-0.577001 6:-0.520737 7:-0.365222 \n",
      "11 0:-1 1:0.378378 2:0.260504 3:-0.716814 4:-0.324951 5:-0.513786 6:-0.537854 7:-0.464873 \n",
      "17 1:0.364865 2:0.344538 3:-0.716814 4:-0.348681 5:-0.581708 6:-0.485188 7:-0.405082 \n",
      "13 1:0.472973 2:0.529412 3:-0.734513 4:0.0299274 5:-0.219906 6:-0.24424 7:-0.365222 \n",
      "14 0:1 1:0.391892 2:0.327731 3:-0.716814 4:-0.368868 5:-0.632145 6:-0.425938 7:-0.315396 \n",
      "13 1:0.743243 2:0.747899 3:-0.619469 4:0.57535 5:0.203093 6:0.0651745 7:0.232686 \n",
      "14 1:0.513514 2:0.529412 3:-0.690265 4:-0.142554 5:-0.229993 6:-0.314022 7:-0.285501 \n",
      "15 1:0.445946 2:0.428571 3:-0.690265 4:-0.245263 5:-0.475454 6:-0.432521 7:-0.165919 \n",
      "13 1:0.27027 2:0.310924 3:-0.690265 4:-0.397556 5:-0.535306 6:-0.503621 7:-0.415047 \n",
      "12 0:-1 1:0.337838 2:0.327731 3:-0.716814 4:-0.391181 5:-0.500336 6:-0.428571 7:-0.494768 \n",
      "18 1:0.418919 2:0.411765 3:-0.681416 4:-0.178325 5:-0.314055 6:-0.296906 7:-0.365222 \n",
      "14 1:0.202703 2:0.193277 3:-0.699115 4:-0.384806 5:-0.498991 6:-0.424621 7:-0.504733 \n",
      "15 0:-1 1:0.513514 2:0.529412 3:-0.628319 4:0.130512 5:-0.122394 6:-0.254773 7:0.152965 \n",
      "13 1:0.608108 2:0.563025 3:-0.734513 4:-0.00548964 5:-0.303295 6:-0.0849243 7:-0.265571 \n",
      "15 0:-1 1:0.675676 2:0.731092 3:-0.646018 4:0.43864 5:0.00874243 6:0.119157 7:0.362232 \n",
      "20 0:-1 1:0.567568 2:0.579832 3:-0.672566 4:-0.109616 5:-0.346335 6:-0.418038 7:-0.116094 \n",
      "14 1:0.472973 2:0.428571 3:-0.59292 4:-0.226846 5:-0.459314 6:-0.356155 7:-0.295466 \n",
      "19 1:0.418919 2:0.411765 3:-0.681416 4:-0.16522 5:-0.416947 6:-0.349572 7:-0.155954 \n",
      "9 0:-1 1:0.175676 2:0.176471 3:-0.769912 4:-0.493182 5:-0.500336 6:-0.585253 7:-0.664175 \n",
      "10 0:-1 1:0.216216 2:0.176471 3:-0.761062 4:-0.464849 5:-0.556826 6:-0.432521 7:-0.61435 \n",
      "9 0:-1 1:-0.0135135 2:0.0756303 3:-0.769912 4:-0.656455 5:-0.697377 6:-0.747202 7:-0.694071 \n",
      "8 0:1 1:0.108108 2:0.210084 3:-0.752212 4:-0.597308 5:-0.665098 6:-0.648453 7:-0.63428 \n",
      "10 1:0.135135 2:0.109244 3:-0.769912 4:-0.512307 5:-0.581036 6:-0.529954 7:-0.65421 \n",
      "7 0:1 1:-0.027027 2:-0.0252101 3:-0.787611 4:-0.684434 5:-0.704102 6:-0.706386 7:-0.753861 \n",
      "9 0:1 1:-0.108108 2:-0.12605 3:-0.814159 4:-0.755622 5:-0.785474 6:-0.794602 7:-0.803687 \n",
      "9 0:1 1:-0.0675676 2:-0.0756303 3:-0.823009 4:-0.752081 5:-0.781439 6:-0.766952 7:-0.803687 \n",
      "9 1:0.148649 2:0.142857 3:-0.734513 4:-0.495307 5:-0.566241 6:-0.545754 7:-0.61435 \n",
      "7 1:-0.162162 2:-0.159664 3:-0.814159 4:-0.766602 5:-0.817754 6:-0.805135 7:-0.803687 \n",
      "6 0:1 1:-0.310811 2:-0.294118 3:-0.840708 4:-0.873915 5:-0.909886 6:-0.906517 7:-0.883408 \n",
      "10 1:0.364865 2:0.411765 3:-0.725664 4:-0.311493 5:-0.422327 6:-0.395655 7:-0.434978 \n",
      "6 0:1 1:-0.324324 2:-0.277311 3:-0.823009 4:-0.870374 5:-0.893746 6:-0.886768 7:-0.873443 \n",
      "10 0:-1 1:0.0810811 2:0.0756303 3:-0.787611 4:-0.602621 5:-0.661735 6:-0.684003 7:-0.63428 \n",
      "9 1:-0.175676 2:-0.176471 3:-0.840708 4:-0.773685 5:-0.793544 6:-0.803818 7:-0.813652 \n",
      "6 0:1 1:-0.283784 2:-0.310924 3:-0.840708 4:-0.874624 5:-0.899126 6:-0.863068 7:-0.893373 \n",
      "12 0:-1 1:0.216216 2:0.243697 3:-0.787611 4:-0.504162 5:-0.552791 6:-0.615537 7:-0.564524 \n",
      "10 1:0.202703 2:0.210084 3:-0.743363 4:-0.431557 5:-0.554136 6:-0.54707 7:-0.434978 \n",
      "13 1:0.243243 2:0.327731 3:-0.761062 4:-0.429432 5:-0.568258 6:-0.524687 7:-0.504733 \n",
      "8 0:-1 1:0.0810811 2:0.0252101 3:-0.787611 4:-0.591996 5:-0.621385 6:-0.685319 7:-0.664175 \n",
      "7 0:1 1:-0.0810811 2:-0.092437 3:-0.823009 4:-0.728706 5:-0.776732 6:-0.790652 7:-0.753861 \n",
      "8 0:1 1:0.135135 2:0.109244 3:-0.778761 4:-0.587037 5:-0.6308 6:-0.676103 7:-0.674141 \n",
      "10 1:0.0945946 2:0.176471 3:-0.769912 4:-0.549849 5:-0.628783 6:-0.620803 7:-0.584454 \n",
      "10 1:0.202703 2:0.243697 3:-0.734513 4:-0.425536 5:-0.483524 6:-0.470704 7:-0.544594 \n",
      "9 0:-1 1:0.0405405 2:0.0756303 3:-0.769912 4:-0.595183 5:-0.664425 6:-0.687953 7:-0.61435 \n",
      "11 1:0.364865 2:0.344538 3:-0.787611 4:-0.335576 5:-0.464694 6:-0.324556 7:-0.474838 \n",
      "11 0:-1 1:0.391892 2:0.462185 3:-0.761062 4:-0.287409 5:-0.433759 6:-0.410138 7:-0.434978 \n",
      "23 1:0.283784 2:0.210084 3:-0.761062 4:-0.452453 5:-0.595158 6:-0.529954 7:-0.484803 \n",
      "12 1:0.554054 2:0.495798 3:-0.707965 4:-0.190721 5:-0.349025 6:-0.427255 7:-0.275536 \n",
      "16 1:0.0540541 2:0.0756303 3:-0.761062 4:-0.576412 5:-0.702085 6:-0.661619 7:-0.544594 \n",
      "11 0:-1 1:0.027027 2:0.00840336 3:-0.769912 4:-0.636621 5:-0.732347 6:-0.665569 7:-0.65421 \n",
      "13 0:-1 1:0.0675676 2:0.0756303 3:-0.769912 4:-0.590933 5:-0.712845 6:-0.569454 7:-0.61435 \n",
      "13 1:-0.027027 2:-0.00840336 3:-0.80531 4:-0.729414 5:-0.809011 6:-0.736669 7:-0.753861 \n",
      "7 0:-1 1:-0.256757 2:-0.294118 3:-0.80531 4:-0.791394 5:-0.817754 6:-0.835418 7:-0.833582 \n",
      "8 0:1 1:-0.351351 2:-0.378151 3:-0.876106 4:-0.904374 5:-0.928043 6:-0.918367 7:-0.923269 \n",
      "16 0:-1 1:0.405405 2:0.394958 3:-0.743363 4:-0.299451 5:-0.458642 6:-0.605003 7:-0.325361 \n",
      "14 1:0.364865 2:0.411765 3:-0.761062 4:-0.346202 5:-0.475454 6:-0.56682 7:-0.454908 \n",
      "17 0:-1 1:0.351351 2:0.277311 3:-0.734513 4:-0.431202 5:-0.607263 6:-0.573404 7:-0.464873 \n",
      "13 0:-1 1:0.243243 2:0.277311 3:-0.725664 4:-0.369931 5:-0.542031 6:-0.535221 7:-0.504733 \n",
      "13 0:-1 1:0.189189 2:0.226891 3:-0.752212 4:-0.456703 5:-0.664425 6:-0.595787 7:-0.425012 \n",
      "12 1:0.162162 2:0.109244 3:-0.761062 4:-0.563308 5:-0.663753 6:-0.691903 7:-0.604385 \n",
      "15 1:0.162162 2:0.142857 3:-0.743363 4:-0.539933 5:-0.63887 6:-0.59842 7:-0.59442 \n",
      "10 0:1 1:-0.121622 2:-0.142857 3:-0.823009 4:-0.797769 5:-0.847344 6:-0.834101 7:-0.813652 \n",
      "14 0:-1 1:0.121622 2:0.142857 3:-0.761062 4:-0.608642 5:-0.714862 6:-0.757735 7:-0.574489 \n",
      "12 0:-1 1:0.22973 2:0.277311 3:-0.761062 4:-0.479724 5:-0.560861 6:-0.655036 7:-0.564524 \n",
      "8 0:1 1:-0.135135 2:-0.092437 3:-0.814159 4:-0.784664 5:-0.852051 6:-0.807768 7:-0.813652 \n",
      "17 1:0.594595 2:0.613445 3:-0.663717 4:0.058261 5:-0.224613 6:-0.260039 7:-0.0563029 \n",
      "10 1:-0.0810811 2:-0.159664 3:-0.814159 4:-0.74606 5:-0.839946 6:-0.785385 7:-0.803687 \n",
      "11 0:-1 1:-0.0405405 2:-0.0252101 3:-0.79646 4:-0.785727 5:-0.876933 6:-0.856485 7:-0.763827 \n",
      "13 0:-1 1:0.0810811 2:0.142857 3:-0.761062 4:-0.582079 5:-0.669805 6:-0.568137 7:-0.604385 \n",
      "15 1:0.216216 2:0.243697 3:-0.743363 4:-0.435098 5:-0.551446 6:-0.450955 7:-0.524664 \n",
      "15 0:1 1:0.0945946 2:0.12605 3:-0.743363 4:-0.588808 5:-0.68998 6:-0.682686 7:-0.494768 \n",
      "9 0:1 1:-0.0675676 2:-0.0252101 3:-0.79646 4:-0.758102 5:-0.797579 6:-0.791968 7:-0.773792 \n",
      "15 0:-1 1:0.391892 2:0.361345 3:-0.725664 4:-0.35966 5:-0.561533 6:-0.61027 7:-0.335326 \n",
      "9 1:0.189189 2:0.226891 3:-0.761062 4:-0.555516 5:-0.62273 6:-0.666886 7:-0.574489 \n",
      "13 0:-1 1:0.675676 2:0.663866 3:-0.610619 4:0.0975739 5:-0.240081 6:0.00855826 7:-0.116094 \n",
      "23 1:0.959459 2:0.932773 3:-0.654867 4:0.787852 5:0.253531 6:0.552337 7:0.232686 \n",
      "23 0:-1 1:0.445946 2:0.462185 3:-0.734513 4:-0.220117 5:-0.429724 6:-0.468071 7:-0.285501 \n",
      "18 1:0.324324 2:0.428571 3:-0.690265 4:-0.323535 5:-0.478816 6:-0.435155 7:-0.454908 \n",
      "11 0:-1 1:0.310811 2:0.344538 3:-0.707965 4:-0.392244 5:-0.461332 6:-0.55497 7:-0.514699 \n",
      "17 0:-1 1:0.567568 2:0.445378 3:-0.654867 4:0.146095 5:-0.157364 6:-0.0585912 7:-0.0363727 \n",
      "17 0:-1 1:0.527027 2:0.563025 3:-0.646018 4:-0.0047813 5:-0.240081 6:-0.20079 7:-0.0961634 \n",
      "11 1:0.391892 2:0.394958 3:-0.699115 4:-0.36391 5:-0.523874 6:-0.499671 7:-0.504733 \n",
      "7 0:1 1:-0.364865 2:-0.378151 3:-0.840708 4:-0.898353 5:-0.919973 6:-0.918367 7:-0.913303 \n",
      "6 0:1 1:-0.513514 2:-0.563025 3:-0.876106 4:-0.948291 5:-0.963685 6:-0.953917 7:-0.953164 \n",
      "6 0:1 1:-0.743243 2:-0.764706 3:-0.902655 4:-0.984771 5:-0.98924 6:-0.986833 7:-0.987045 \n",
      "21 0:-1 1:0.608108 2:0.663866 3:-0.699115 4:-0.118116 5:-0.36651 6:-0.354839 7:-0.20578 \n",
      "17 1:0.716216 2:0.714286 3:-0.654867 4:0.221534 5:-0.143241 6:-0.115207 7:0.123069 \n",
      "13 1:0.310811 2:0.260504 3:-0.778761 4:-0.432973 5:-0.580363 6:-0.549704 7:-0.478824 \n",
      "11 0:-1 1:0.162162 2:0.159664 3:-0.769912 4:-0.460244 5:-0.593141 6:-0.503621 7:-0.569507 \n",
      "16 0:-1 1:0.216216 2:0.260504 3:-0.707965 4:-0.389056 5:-0.495629 6:-0.489138 7:-0.501744 \n",
      "9 1:0.0135135 2:0.0252101 3:-0.814159 4:-0.667434 5:-0.72764 6:-0.755102 7:-0.706029 \n",
      "12 1:0.189189 2:0.277311 3:-0.699115 4:-0.554454 5:-0.629455 6:-0.709019 7:-0.572496 \n",
      "19 0:-1 1:0.391892 2:0.411765 3:-0.716814 4:-0.331681 5:-0.488231 6:-0.516787 7:-0.464873 \n",
      "18 0:-1 1:0.689189 2:0.596639 3:-0.663717 4:-0.0674695 5:-0.264291 6:-0.387755 7:-0.165919 \n",
      "17 1:0.743243 2:0.697479 3:-0.690265 4:0.221534 5:-0.144586 6:-0.102041 7:0.0433483 \n",
      "11 0:-1 1:0.513514 2:0.478992 3:-0.734513 4:-0.2357 5:-0.352387 6:-0.364055 7:-0.385152 \n",
      "13 0:-1 1:0.297297 2:0.294118 3:-0.761062 4:-0.362139 5:-0.489576 6:-0.447005 7:-0.444943 \n",
      "13 0:-1 1:0.351351 2:0.394958 3:-0.734513 4:-0.192846 5:-0.394082 6:-0.464121 7:-0.20578 \n",
      "17 0:-1 1:0.378378 2:0.344538 3:-0.778761 4:-0.273951 5:-0.475454 6:-0.443055 7:-0.504733 \n",
      "20 1:0.445946 2:0.445378 3:-0.628319 4:-0.0490526 5:-0.281775 6:-0.420671 7:0.023418 \n",
      "13 1:0.540541 2:0.579832 3:-0.646018 4:0.024969 5:-0.193006 6:-0.325872 7:0.00348779 \n",
      "11 1:0.27027 2:0.294118 3:-0.690265 4:-0.452807 5:-0.599866 6:-0.507571 7:-0.474838 \n",
      "12 0:-1 1:0.283784 2:0.327731 3:-0.725664 4:-0.442182 5:-0.540013 6:-0.58262 7:-0.504733 \n",
      "18 1:0.581081 2:0.579832 3:-0.637168 4:-0.0334691 5:-0.328178 6:-0.235023 7:-0.18585 \n",
      "18 0:-1 1:0.337838 2:0.411765 3:-0.654867 4:-0.27218 5:-0.377942 6:-0.499671 7:-0.395117 \n",
      "15 1:0.418919 2:0.394958 3:-0.646018 4:-0.271117 5:-0.474109 6:-0.465438 7:-0.425012 \n",
      "12 1:0.5 2:0.512605 3:-0.707965 4:-0.247034 5:-0.383322 6:-0.432521 7:-0.375187 \n",
      "19 0:-1 1:0.675676 2:0.731092 3:-0.59292 4:0.333806 5:0.164089 6:0.144174 7:-0.00647733 \n",
      "15 0:-1 1:0.554054 2:0.647059 3:-0.716814 4:-0.121303 5:-0.346335 6:-0.221856 7:-0.0463378 \n",
      "16 1:0.743243 2:0.815126 3:-0.60177 4:0.393306 5:0.0806994 6:0.112574 7:0.312407 \n",
      "12 0:1 1:0.310811 2:0.294118 3:-0.699115 4:-0.332389 5:-0.524546 6:-0.428571 7:-0.405082 \n",
      "8 0:1 1:-0.0675676 2:-0.092437 3:-0.79646 4:-0.750664 5:-0.782784 6:-0.832785 7:-0.793722 \n",
      "3 0:-1 1:-0.716216 2:-0.764706 3:-0.911504 4:-0.985125 5:-0.989913 6:-0.986833 7:-0.983059 \n",
      "12 1:-0.108108 2:-0.092437 3:-0.80531 4:-0.748185 5:-0.806321 6:-0.810402 7:-0.783757 \n",
      "12 1:0.148649 2:0.176471 3:-0.734513 4:-0.578891 5:-0.661063 6:-0.669519 7:-0.63428 \n",
      "12 0:1 1:-0.027027 2:-0.0588235 3:-0.80531 4:-0.730122 5:-0.792199 6:-0.823568 7:-0.733931 \n",
      "10 0:-1 1:-0.283784 2:-0.260504 3:-0.840708 4:-0.855144 5:-0.903833 6:-0.888084 7:-0.863478 \n",
      "8 1:-0.0405405 2:-0.0420168 3:-0.80531 4:-0.730831 5:-0.794217 6:-0.749835 7:-0.78575 \n",
      "12 0:1 1:0.243243 2:0.193277 3:-0.725664 4:-0.5541 5:-0.632145 6:-0.628703 7:-0.641256 \n",
      "12 0:1 1:-0.0810811 2:-0.092437 3:-0.79646 4:-0.768727 5:-0.812374 6:-0.867018 7:-0.791729 \n",
      "10 1:-0.22973 2:-0.294118 3:-0.840708 4:-0.848061 5:-0.872226 6:-0.903884 7:-0.856502 \n",
      "5 0:-1 1:-0.72973 2:-0.731092 3:-0.929204 4:-0.979812 5:-0.98655 6:-0.981567 7:-0.983059 \n",
      "5 0:-1 1:-0.783784 2:-0.798319 3:-0.955752 4:-0.984417 5:-0.98924 6:-0.98815 7:-0.988042 \n",
      "11 0:1 1:0.216216 2:0.260504 3:-0.734513 4:-0.479724 5:-0.567586 6:-0.577354 7:-0.574489 \n",
      "16 1:0.216216 2:0.12605 3:-0.761062 4:-0.576058 5:-0.696705 6:-0.656353 7:-0.584454 \n",
      "11 1:-0.0135135 2:-0.0252101 3:-0.814159 4:-0.697893 5:-0.779422 6:-0.782752 7:-0.73991 \n",
      "12 1:0.0135135 2:-0.0252101 3:-0.79646 4:-0.65008 5:-0.745124 6:-0.693219 7:-0.723966 \n",
      "14 1:0.108108 2:0.0420168 3:-0.752212 4:-0.5626 5:-0.65232 6:-0.620803 7:-0.650224 \n",
      "11 0:1 1:0.0675676 2:-0.00840336 3:-0.761062 4:-0.599787 5:-0.68998 6:-0.615537 7:-0.699053 \n",
      "10 0:1 1:0.189189 2:0.0756303 3:-0.752212 4:-0.540641 5:-0.66577 6:-0.63002 7:-0.561535 \n",
      "11 0:-1 1:-0.0675676 2:-0.0420168 3:-0.778761 4:-0.683018 5:-0.779422 6:-0.705069 7:-0.715994 \n",
      "16 1:0.027027 2:-0.00840336 3:-0.778761 4:-0.683726 5:-0.788164 6:-0.732719 7:-0.736921 \n",
      "10 0:-1 1:-0.202703 2:-0.210084 3:-0.840708 4:-0.830707 5:-0.853396 6:-0.882818 7:-0.865471 \n",
      "10 0:-1 1:-0.310811 2:-0.344538 3:-0.840708 4:-0.861874 5:-0.887021 6:-0.893351 7:-0.882412 \n",
      "7 0:1 1:-0.391892 2:-0.445378 3:-0.840708 4:-0.900478 5:-0.924681 6:-0.913101 7:-0.917289 \n",
      "14 0:1 1:0.486486 2:0.361345 3:-0.716814 4:-0.123428 5:-0.261601 6:-0.282423 7:-0.24564 \n",
      "14 0:1 1:0.445946 2:0.411765 3:-0.699115 4:-0.265805 5:-0.404842 6:-0.366689 7:-0.365222 \n",
      "14 0:1 1:0.486486 2:0.378151 3:-0.725664 4:-0.31291 5:-0.457969 6:-0.515471 7:-0.305431 \n",
      "17 0:1 1:0.513514 2:0.512605 3:-0.663717 4:-0.058261 5:-0.220578 6:-0.337722 7:-0.136024 \n",
      "14 0:1 1:0.148649 2:0.109244 3:-0.725664 4:-0.461661 5:-0.490921 6:-0.577354 7:-0.624315 \n",
      "17 1:0.22973 2:0.260504 3:-0.699115 4:-0.452453 5:-0.530599 6:-0.601053 7:-0.534629 \n",
      "13 0:1 2:-0.0756303 3:-0.823009 4:-0.691872 5:-0.782112 6:-0.802502 7:-0.664175 \n",
      "12 1:0.378378 2:0.210084 3:-0.725664 4:-0.506641 5:-0.597848 6:-0.616853 7:-0.61435 \n",
      "16 0:1 1:-0.0135135 2:0.00840336 3:-0.707965 4:-0.693288 5:-0.787492 6:-0.724819 7:-0.723966 \n",
      "10 0:-1 1:-0.418919 2:-0.428571 3:-0.858407 4:-0.909687 5:-0.929388 6:-0.932851 7:-0.913303 \n",
      "15 0:1 1:0.297297 2:0.344538 3:-0.699115 4:-0.403931 5:-0.585743 6:-0.499671 7:-0.405082 \n",
      "15 0:1 1:0.567568 2:0.546218 3:-0.743363 4:-0.115991 5:-0.293208 6:-0.25609 7:-0.375187 \n",
      "10 1:0.364865 2:0.361345 3:-0.672566 4:-0.281034 5:-0.528581 6:-0.474654 7:-0.365222 \n",
      "12 0:1 1:0.486486 2:0.260504 3:-0.690265 4:-0.00194794 5:-0.232011 6:-0.219223 7:-0.215745 \n",
      "15 0:1 1:0.472973 2:0.445378 3:-0.699115 4:-0.145741 5:-0.355077 6:-0.199473 7:-0.345291 \n",
      "8 1:0.527027 2:0.495798 3:-0.734513 4:-0.243138 5:-0.502354 6:-0.289006 7:-0.285501 \n",
      "10 1:0.162162 2:0.0756303 3:-0.79646 4:-0.58385 5:-0.64694 6:-0.685319 7:-0.670154 \n",
      "9 0:1 1:0.148649 2:0.142857 3:-0.787611 4:-0.621038 5:-0.710155 6:-0.715602 7:-0.647235 \n",
      "7 0:-1 1:-0.364865 2:-0.361345 3:-0.831858 4:-0.895166 5:-0.930733 6:-0.911784 7:-0.907324 \n",
      "12 1:0.162162 2:0.092437 3:-0.743363 4:-0.540287 5:-0.60659 6:-0.500987 7:-0.664175 \n",
      "9 0:1 1:-0.0675676 2:-0.159664 3:-0.80531 4:-0.803081 5:-0.874916 6:-0.794602 7:-0.812656 \n",
      "10 0:-1 1:-0.121622 2:-0.12605 3:-0.814159 4:-0.798123 5:-0.848689 6:-0.903884 7:-0.777778 \n",
      "18 0:-1 1:-0.0540541 2:-0.12605 3:-0.778761 4:-0.751727 5:-0.848689 6:-0.852535 7:-0.743896 \n",
      "11 0:-1 1:-0.364865 2:-0.394958 3:-0.893805 4:-0.916416 5:-0.945528 6:-0.914417 7:-0.923269 \n",
      "18 1:0.0540541 2:-0.00840336 3:-0.769912 4:-0.651496 5:-0.739744 6:-0.730086 7:-0.694071 \n",
      "10 1:0.0540541 2:0.0252101 3:-0.787611 4:-0.663892 5:-0.743107 6:-0.705069 7:-0.684106 \n",
      "13 0:-1 1:-0.256757 2:-0.327731 3:-0.849558 4:-0.849478 5:-0.866846 6:-0.878868 7:-0.883408 \n",
      "15 0:1 1:0.202703 2:0.210084 3:-0.716814 4:-0.579954 5:-0.718225 6:-0.627386 7:-0.484803 \n",
      "12 1:0.0810811 2:0.0420168 3:-0.769912 4:-0.661059 5:-0.745124 6:-0.701119 7:-0.709018 \n",
      "10 1:-0.0945946 2:-0.12605 3:-0.80531 4:-0.774039 5:-0.832549 6:-0.828835 7:-0.813652 \n",
      "6 0:-1 1:-0.5 2:-0.512605 3:-0.884956 4:-0.933416 5:-0.942165 6:-0.930217 7:-0.943199 \n",
      "13 0:1 1:0.351351 2:0.327731 3:-0.699115 4:-0.341597 5:-0.519839 6:-0.436471 7:-0.484803 \n",
      "14 0:1 1:0.324324 2:0.277311 3:-0.725664 4:-0.447494 5:-0.63618 6:-0.55892 7:-0.434978 \n",
      "6 0:-1 1:-0.5 2:-0.546218 3:-0.867257 4:-0.934478 5:-0.941493 6:-0.94865 7:-0.943199 \n",
      "10 1:0.22973 2:0.109244 3:-0.778761 4:-0.527183 5:-0.612643 6:-0.603687 7:-0.644245 \n",
      "9 0:-1 1:-0.283784 2:-0.327731 3:-0.831858 4:-0.85054 5:-0.892401 6:-0.911784 7:-0.863478 \n",
      "11 0:1 1:0.202703 2:0.092437 3:-0.752212 4:-0.629538 5:-0.762609 6:-0.698486 7:-0.63428 \n",
      "18 1:0.513514 2:0.495798 3:-0.681416 4:-0.0720737 5:-0.289845 6:-0.346939 7:-0.0363727 \n",
      "11 1:0.445946 2:0.445378 3:-0.707965 4:-0.23145 5:-0.429052 6:-0.390388 7:-0.24564 \n",
      "16 1:0.581081 2:0.546218 3:-0.681416 4:0.0773862 5:-0.27505 6:-0.115207 7:0.103139 \n",
      "16 0:1 1:0.513514 2:0.495798 3:-0.681416 4:-0.0671153 5:-0.263618 6:-0.23239 7:-0.0264076 \n",
      "14 1:0.0540541 2:0.092437 3:-0.761062 4:-0.591287 5:-0.721587 6:-0.712969 7:-0.564524 \n",
      "23 0:-1 1:0.189189 2:0.159664 3:-0.716814 4:-0.422348 5:-0.66308 6:-0.59052 7:-0.405082 \n",
      "9 0:1 1:-0.297297 2:-0.378151 3:-0.831858 4:-0.880999 5:-0.917956 6:-0.898618 7:-0.893373 \n",
      "16 1:0.189189 2:0.159664 3:-0.699115 4:-0.437577 5:-0.654338 6:-0.539171 7:-0.444943 \n",
      "9 1:-0.27027 2:-0.327731 3:-0.823009 4:-0.861874 5:-0.905851 6:-0.867018 7:-0.883408 \n",
      "13 0:-1 1:0.0540541 2:0.00840336 3:-0.778761 4:-0.629184 5:-0.728985 6:-0.64582 7:-0.714001 \n",
      "10 0:-1 1:0.256757 2:0.210084 3:-0.699115 4:-0.378785 5:-0.545393 6:-0.453588 7:-0.494768 \n",
      "9 0:-1 1:0.0810811 2:0.00840336 3:-0.778761 4:-0.673809 5:-0.751177 6:-0.719552 7:-0.714001 \n",
      "11 2:-0.0588235 3:-0.752212 4:-0.678059 5:-0.761264 6:-0.701119 7:-0.723966 \n",
      "9 0:-1 1:0.148649 2:0.00840336 3:-0.752212 4:-0.627413 5:-0.715535 6:-0.608953 7:-0.723966 \n",
      "12 0:-1 1:0.148649 2:0.092437 3:-0.761062 4:-0.5881 5:-0.69267 6:-0.668203 7:-0.644245 \n",
      "14 1:0.283784 2:0.277311 3:-0.699115 4:-0.375243 5:-0.61466 6:-0.568137 7:-0.444943 \n",
      "6 0:1 1:-0.459459 2:-0.495798 3:-0.858407 4:-0.933416 5:-0.952925 6:-0.9526 7:-0.943199 \n",
      "7 1:-0.256757 2:-0.294118 3:-0.840708 4:-0.870019 5:-0.901143 6:-0.892034 7:-0.883408 \n",
      "9 1:-0.202703 2:-0.226891 3:-0.814159 4:-0.810165 5:-0.850034 6:-0.847268 7:-0.836572 \n",
      "10 1:-0.0675676 2:-0.0756303 3:-0.778761 4:-0.673455 5:-0.751177 6:-0.711652 7:-0.714001 \n",
      "10 0:-1 1:-0.256757 2:-0.310924 3:-0.840708 4:-0.861165 5:-0.903833 6:-0.853851 7:-0.883408 \n",
      "9 0:-1 1:-0.135135 2:-0.159664 3:-0.814159 4:-0.801665 5:-0.870208 6:-0.830151 7:-0.811659 \n",
      "8 0:1 1:-0.324324 2:-0.512605 3:-0.858407 4:-0.930937 5:-0.948218 6:-0.942067 7:-0.939213 \n",
      "5 0:1 1:-0.459459 2:-0.512605 3:-0.884956 4:-0.936249 5:-0.949563 6:-0.939434 7:-0.949178 \n",
      "6 0:1 1:-0.567568 2:-0.613445 3:-0.884956 4:-0.957145 5:-0.97041 6:-0.96445 7:-0.959143 \n",
      "5 0:1 1:-0.527027 2:-0.579832 3:-0.893805 4:-0.949708 5:-0.963685 6:-0.95655 7:-0.958146 \n",
      "5 0:1 1:-0.527027 2:-0.563025 3:-0.884956 4:-0.951124 5:-0.96503 6:-0.9526 7:-0.958146 \n",
      "4 0:1 1:-0.662162 2:-0.697479 3:-0.911504 4:-0.975916 5:-0.984533 6:-0.98025 7:-0.981066 \n",
      "20 1:0.378378 2:0.394958 3:-0.699115 4:-0.22295 5:-0.466711 6:-0.389072 7:-0.289487 \n",
      "14 0:-1 2:-0.00840336 3:-0.752212 4:-0.583141 5:-0.728985 6:-0.585253 7:-0.624315 \n",
      "13 1:0.148649 2:0.109244 3:-0.769912 4:-0.457411 5:-0.648285 6:-0.751152 7:-0.464873 \n",
      "8 0:-1 1:-0.0135135 2:-0.092437 3:-0.858407 4:-0.708872 5:-0.807666 6:-0.734036 7:-0.743896 \n",
      "17 0:-1 1:0.189189 2:0.176471 3:-0.752212 4:-0.398973 5:-0.581708 6:-0.616853 7:-0.375187 \n",
      "11 1:0.202703 2:0.176471 3:-0.752212 4:-0.511599 5:-0.630128 6:-0.640553 7:-0.574489 \n",
      "11 0:-1 1:0.148649 2:0.12605 3:-0.769912 4:-0.499203 5:-0.631473 6:-0.55892 7:-0.644245 \n",
      "9 0:-1 1:-0.0540541 2:-0.092437 3:-0.787611 4:-0.735435 5:-0.810356 6:-0.720869 7:-0.793722 \n",
      "16 0:-1 1:0.175676 2:0.210084 3:-0.752212 4:-0.42164 5:-0.594486 6:-0.433838 7:-0.534629 \n",
      "8 1:-0.202703 2:-0.260504 3:-0.858407 4:-0.840623 5:-0.876261 6:-0.836735 7:-0.863478 \n",
      "13 0:-1 1:0.256757 2:0.210084 3:-0.769912 4:-0.41739 5:-0.635508 6:-0.406188 7:-0.524664 \n",
      "11 0:-1 1:0.459459 2:0.411765 3:-0.699115 4:-0.163804 5:-0.36382 6:-0.238973 7:-0.524664 \n",
      "20 0:-1 1:0.324324 2:0.294118 3:-0.690265 4:-0.206658 5:-0.472764 6:-0.474654 7:-0.255605 \n",
      "14 0:-1 1:0.540541 2:0.546218 3:-0.690265 4:0.140074 5:-0.0941493 6:0.00987492 7:-0.235675 \n",
      "14 1:0.459459 2:0.394958 3:-0.690265 4:-0.0816363 5:-0.310693 6:-0.0980908 7:-0.365222 \n",
      "14 0:-1 1:0.432432 2:0.462185 3:-0.743363 4:-0.0805738 5:-0.305985 6:-0.136274 7:-0.385152 \n",
      "12 1:0.391892 2:0.344538 3:-0.707965 4:-0.179033 5:-0.490249 6:-0.354839 7:-0.444943 \n",
      "18 0:-1 1:0.540541 2:0.445378 3:-0.725664 4:0.0533026 5:-0.205783 6:-0.179724 7:-0.24564 \n",
      "13 0:-1 1:0.337838 2:0.226891 3:-0.725664 4:-0.287409 5:-0.494284 6:-0.493088 7:-0.325361 \n",
      "8 1:0.0675676 2:0.00840336 3:-0.681416 4:-0.689038 5:-0.796234 6:-0.694536 7:-0.733931 \n",
      "14 1:0.148649 2:0.294118 3:-0.725664 4:-0.475828 5:-0.728985 6:-0.473338 7:-0.581465 \n",
      "13 1:0.202703 2:0.243697 3:-0.743363 4:-0.505578 5:-0.722932 6:-0.499671 7:-0.524664 \n",
      "8 0:-1 1:-0.148649 2:-0.226891 3:-0.831858 4:-0.809456 5:-0.853396 6:-0.843318 7:-0.843548 \n",
      "12 0:-1 1:0.202703 2:0.159664 3:-0.707965 4:-0.394723 5:-0.632145 6:-0.472021 7:-0.584454 \n",
      "14 1:0.256757 2:0.210084 3:-0.690265 4:-0.365681 5:-0.631473 6:-0.366689 7:-0.454908 \n",
      "14 0:-1 1:0.0405405 2:0.0252101 3:-0.761062 4:-0.568975 5:-0.738399 6:-0.719552 7:-0.534629 \n",
      "8 0:1 1:-0.243243 2:-0.310924 3:-0.840708 4:-0.865061 5:-0.897781 6:-0.901251 7:-0.873443 \n",
      "13 1:0.121622 2:0.159664 3:-0.743363 4:-0.531433 5:-0.718897 6:-0.660303 7:-0.501744 \n",
      "11 1:0.5 2:0.529412 3:-0.672566 4:-0.126616 5:-0.313383 6:-0.082291 7:-0.392128 \n",
      "14 0:-1 1:0.148649 2:0.109244 3:-0.743363 4:-0.462015 5:-0.670477 6:-0.487821 7:-0.596413 \n",
      "15 0:-1 1:0.121622 2:0.12605 3:-0.761062 4:-0.582079 5:-0.675857 6:-0.748519 7:-0.63727 \n",
      "7 0:-1 1:-0.0135135 2:-0.092437 3:-0.79646 4:-0.725164 5:-0.782112 6:-0.772219 7:-0.777778 \n",
      "10 1:0.189189 2:0.142857 3:-0.707965 4:-0.465557 5:-0.745124 6:-0.553654 7:-0.364225 \n",
      "11 1:0.0810811 2:0.092437 3:-0.743363 4:-0.597662 5:-0.776732 6:-0.690586 7:-0.630294 \n",
      "8 0:1 1:-0.0675676 2:-0.142857 3:-0.823009 4:-0.798477 5:-0.846671 6:-0.807768 7:-0.833582 \n",
      "9 0:-1 1:-0.121622 2:-0.159664 3:-0.769912 4:-0.793519 5:-0.872226 6:-0.823568 7:-0.793722 \n",
      "12 0:-1 1:0.0135135 2:0.0252101 3:-0.716814 4:-0.599787 5:-0.767317 6:-0.673469 7:-0.554559 \n",
      "15 1:0.202703 2:0.159664 3:-0.769912 4:-0.559058 5:-0.712172 6:-0.457538 7:-0.664175 \n",
      "12 0:-1 1:0.162162 2:0.159664 3:-0.725664 4:-0.405348 5:-0.63618 6:-0.533904 7:-0.434978 \n",
      "15 0:-1 1:0.135135 2:0.159664 3:-0.752212 4:-0.450682 5:-0.73033 6:-0.527321 7:-0.504733 \n",
      "19 0:-1 1:0.256757 2:0.193277 3:-0.743363 4:-0.300868 5:-0.62273 6:-0.440421 7:-0.295466 \n",
      "12 1:0.0945946 2:0.12605 3:-0.778761 4:-0.512307 5:-0.706792 6:-0.593153 7:-0.604385 \n",
      "9 1:-0.310811 2:-0.310924 3:-0.858407 4:-0.859749 5:-0.917283 6:-0.869651 7:-0.863478 \n",
      "5 0:1 1:-0.432432 2:-0.478992 3:-0.876106 4:-0.924208 5:-0.942165 6:-0.931534 7:-0.93722 \n",
      "6 0:1 1:-0.391892 2:-0.411765 3:-0.867257 4:-0.911457 5:-0.931406 6:-0.922317 7:-0.93423 \n",
      "6 0:1 1:-0.364865 2:-0.378151 3:-0.814159 4:-0.79706 5:-0.842636 6:-0.830151 7:-0.837569 \n",
      "6 0:1 1:-0.283784 2:-0.327731 3:-0.867257 4:-0.873915 5:-0.901143 6:-0.895984 7:-0.898356 \n",
      "7 0:1 1:-0.189189 2:-0.176471 3:-0.867257 4:-0.899416 5:-0.921991 6:-0.922317 7:-0.915296 \n",
      "7 0:1 1:-0.0810811 2:-0.092437 3:-0.823009 4:-0.670976 5:-0.694015 6:-0.720869 7:-0.775785 \n",
      "6 0:1 1:-0.0810811 2:-0.12605 3:-0.814159 4:-0.767664 5:-0.812374 6:-0.815668 7:-0.813652 \n",
      "6 0:1 1:-0.0810811 2:-0.12605 3:-0.840708 4:-0.744643 5:-0.765972 6:-0.781435 7:-0.817638 \n",
      "8 0:1 1:-0.0675676 2:-0.109244 3:-0.823009 4:-0.760581 5:-0.766644 6:-0.869651 7:-0.814649 \n",
      "6 0:1 1:-0.0540541 2:-0.142857 3:-0.814159 4:-0.742872 5:-0.787492 6:-0.784068 7:-0.793722 \n",
      "8 0:-1 1:0.0540541 2:0.0756303 3:-0.80531 4:-0.647246 5:-0.718897 6:-0.703752 7:-0.703039 \n",
      "8 1:0.0540541 2:-0.00840336 3:-0.761062 4:-0.557641 5:-0.652993 6:-0.620803 7:-0.65421 \n",
      "8 0:1 1:0.0675676 2:0.0588235 3:-0.80531 4:-0.607933 5:-0.665098 6:-0.698486 7:-0.678127 \n",
      "8 1:0.0675676 2:0.0756303 3:-0.787611 4:-0.57535 5:-0.629455 6:-0.61817 7:-0.733931 \n",
      "10 0:1 1:0.0810811 2:0.0420168 3:-0.787611 4:-0.625996 5:-0.664425 6:-0.744569 7:-0.679123 \n",
      "7 0:-1 1:0.0945946 2:0.0588235 3:-0.761062 4:-0.5541 5:-0.537996 6:-0.734036 7:-0.682113 \n",
      "8 0:-1 1:0.148649 2:0.159664 3:-0.769912 4:-0.454932 5:-0.503699 6:-0.579987 7:-0.582461 \n",
      "7 0:1 1:0.162162 2:0.12605 3:-0.672566 4:-0.567558 5:-0.642233 6:-0.627386 7:-0.660189 \n",
      "9 0:-1 1:0.216216 2:0.243697 3:-0.663717 4:-0.383744 5:-0.379287 6:-0.54707 7:-0.606378 \n",
      "9 0:-1 1:0.256757 2:0.226891 3:-0.787611 4:-0.426598 5:-0.474109 6:-0.61817 7:-0.557549 \n",
      "11 0:-1 1:0.27027 2:0.327731 3:-0.734513 4:-0.378431 5:-0.480834 6:-0.60632 7:-0.479821 \n",
      "9 1:0.324324 2:0.294118 3:-0.734513 4:-0.305118 5:-0.399462 6:-0.381172 7:-0.507723 \n",
      "11 0:-1 1:0.364865 2:0.361345 3:-0.681416 4:-0.190367 5:-0.35575 6:-0.271889 7:-0.355257 \n",
      "9 0:-1 1:0.391892 2:0.344538 3:-0.716814 4:-0.229325 5:-0.32885 6:-0.418038 7:-0.421026 \n",
      "9 0:-1 1:0.391892 2:0.428571 3:-0.716814 4:-0.107491 5:-0.236718 6:-0.283739 7:-0.335326 \n",
      "10 0:-1 1:0.405405 2:0.462185 3:-0.672566 4:-0.162033 5:-0.35306 6:-0.470704 7:-0.283508 \n",
      "9 1:0.418919 2:0.411765 3:-0.761062 4:0.0189481 5:-0.209818 6:-0.498354 7:-0.370204 \n",
      "8 1:0.418919 2:0.495798 3:-0.725664 4:-0.0579069 5:-0.162744 6:-0.254773 7:-0.305431 \n",
      "9 1:0.418919 2:0.445378 3:-0.707965 4:-0.193554 5:-0.211836 6:-0.428571 7:-0.428999 \n",
      "10 0:-1 1:0.432432 2:0.411765 3:-0.690265 4:-0.150699 5:-0.275723 6:-0.277156 7:-0.387145 \n",
      "10 1:0.486486 2:0.462185 3:-0.725664 4:-0.0593235 5:-0.103564 6:-0.319289 7:-0.345291 \n",
      "10 0:-1 1:0.5 2:0.495798 3:-0.672566 4:-0.0366566 5:-0.223268 6:-0.178407 7:-0.237668 \n",
      "12 0:-1 1:0.527027 2:0.781513 3:-0.654867 4:0.16522 5:-0.0295898 6:-0.129691 7:-0.064275 \n",
      "11 1:0.527027 2:0.495798 3:-0.681416 4:0.0607402 5:-0.203766 6:-0.174457 7:-0.143996 \n",
      "12 1:0.567568 2:0.647059 3:-0.707965 4:0.147866 5:-0.119704 6:-0.213957 7:0.019432 \n",
      "11 0:1 1:0.581081 2:0.579832 3:-0.619469 4:0.26368 5:-0.0968393 6:-0.0493746 7:-0.192825 \n",
      "15 0:-1 1:0.581081 2:0.613445 3:-0.646018 4:0.267222 5:-0.0154674 6:-0.163924 7:0.0732436 \n",
      "13 1:0.621622 2:0.680672 3:-0.637168 4:0.362139 5:-0.0423672 6:-0.0585912 7:-0.099153 \n",
      "13 1:0.621622 2:0.663866 3:-0.690265 4:0.194971 5:-0.067922 6:-0.0243581 7:-0.058296 \n",
      "9 1:0.662162 2:0.663866 3:-0.681416 4:0.173721 5:0.170814 6:-0.302172 7:-0.127055 \n",
      "14 1:0.675676 2:0.596639 3:-0.646018 4:0.448911 5:0.00739744 6:0.103357 7:0.211759 \n",
      "10 1:0.689189 2:0.579832 3:-0.663717 4:0.132991 5:-0.0504371 6:-0.040158 7:-0.145989 \n",
      "10 1:0.77027 2:0.731092 3:-0.707965 4:0.426952 5:0.435777 6:0.0994075 7:-0.136024 \n",
      "4 0:1 1:-0.648649 2:-0.680672 3:-0.884956 4:-0.973083 5:-0.974445 6:-0.97235 7:-0.97708 \n",
      "4 0:1 1:-0.594595 2:-0.613445 3:-0.876106 4:-0.961395 5:-0.969065 6:-0.967084 7:-0.971101 \n",
      "5 0:1 1:-0.581081 2:-0.579832 3:-0.911504 4:-0.956083 5:-0.972428 6:-0.965767 7:-0.963129 \n",
      "4 0:1 1:-0.459459 2:-0.529412 3:-0.876106 4:-0.939437 5:-0.954943 6:-0.943384 7:-0.952167 \n",
      "5 0:1 1:-0.445946 2:-0.478992 3:-0.902655 4:-0.926333 5:-0.945528 6:-0.931534 7:-0.941206 \n",
      "5 0:1 1:-0.445946 2:-0.445378 3:-0.858407 4:-0.90827 5:-0.912576 6:-0.938117 7:-0.943199 \n",
      "6 0:1 1:-0.405405 2:-0.445378 3:-0.876106 4:-0.912166 5:-0.932078 6:-0.928901 7:-0.933234 \n",
      "6 0:1 1:-0.364865 2:-0.428571 3:-0.867257 4:-0.891624 5:-0.913921 6:-0.903884 7:-0.930244 \n",
      "7 0:1 1:-0.351351 2:-0.394958 3:-0.876106 4:-0.895874 5:-0.923336 6:-0.915734 7:-0.909317 \n",
      "7 0:1 1:-0.283784 2:-0.294118 3:-0.876106 4:-0.870374 5:-0.917283 6:-0.897301 7:-0.863478 \n",
      "6 0:1 1:-0.202703 2:-0.210084 3:-0.858407 4:-0.821144 5:-0.856086 6:-0.852535 7:-0.863478 \n",
      "6 0:1 1:-0.175676 2:-0.226891 3:-0.849558 4:-0.83354 5:-0.846671 6:-0.894668 7:-0.863478 \n",
      "7 0:1 1:-0.148649 2:-0.193277 3:-0.823009 4:-0.80379 5:-0.845999 6:-0.845951 7:-0.843548 \n",
      "6 0:1 1:-0.108108 2:-0.142857 3:-0.884956 4:-0.774393 5:-0.789509 6:-0.827518 7:-0.827603 \n",
      "7 0:1 1:-0.0810811 2:-0.092437 3:-0.823009 4:-0.765185 5:-0.806994 6:-0.813035 7:-0.813652 \n",
      "7 0:1 1:-0.0135135 2:-0.0588235 3:-0.80531 4:-0.726226 5:-0.765972 6:-0.781435 7:-0.781764 \n",
      "6 0:1 1:-0.0135135 2:-0.0252101 3:-0.79646 4:-0.615371 5:-0.639543 6:-0.709019 7:-0.7429 \n",
      "6 0:1 1:-0.0135135 2:-0.092437 3:-0.823009 4:-0.706393 5:-0.752522 6:-0.773535 7:-0.783757 \n",
      "7 0:1 1:-0.0135135 2:0.00840336 3:-0.787611 4:-0.650788 5:-0.690652 6:-0.711652 7:-0.753861 \n",
      "6 0:1 1:0.0135135 2:-0.00840336 3:-0.778761 4:-0.663184 5:-0.70074 6:-0.766952 7:-0.767813 \n",
      "8 0:1 1:0.0135135 2:-0.00840336 3:-0.787611 4:-0.669913 5:-0.731675 6:-0.720869 7:-0.738914 \n",
      "7 1:0.027027 2:-0.00840336 3:-0.787611 4:-0.678767 5:-0.739744 6:-0.726136 7:-0.728949 \n",
      "6 1:0.0405405 2:-0.00840336 3:-0.79646 4:-0.67558 5:-0.728985 6:-0.707702 7:-0.770802 \n",
      "7 0:1 1:0.0405405 2:-0.0252101 3:-0.787611 4:-0.707101 5:-0.735037 6:-0.768269 7:-0.789736 \n",
      "6 0:1 1:0.0405405 2:-0.0252101 3:-0.79646 4:-0.702851 5:-0.746469 6:-0.732719 7:-0.781764 \n",
      "7 0:1 1:0.0540541 2:0.00840336 3:-0.80531 4:-0.665663 5:-0.691997 6:-0.736669 7:-0.763827 \n",
      "9 0:-1 1:0.0540541 2:-0.0420168 3:-0.814159 4:-0.657163 5:-0.690652 6:-0.728769 7:-0.758844 \n",
      "7 0:1 1:0.0810811 2:0.109244 3:-0.80531 4:-0.595183 5:-0.583053 6:-0.731402 7:-0.731938 \n",
      "8 0:1 1:0.0810811 2:0.00840336 3:-0.814159 4:-0.669913 5:-0.731002 6:-0.707702 7:-0.763827 \n",
      "8 0:-1 1:0.0945946 2:0.0588235 3:-0.823009 4:-0.637684 5:-0.674512 6:-0.734036 7:-0.733931 \n",
      "7 0:-1 1:0.148649 2:0.0756303 3:-0.743363 4:-0.561183 5:-0.632818 6:-0.564187 7:-0.707025 \n",
      "7 0:1 1:0.148649 2:0.092437 3:-0.80531 4:-0.651496 5:-0.708137 6:-0.764319 7:-0.738914 \n",
      "7 0:1 1:0.162162 2:0.109244 3:-0.787611 4:-0.576058 5:-0.679892 6:-0.627386 7:-0.63428 \n",
      "9 0:-1 1:0.189189 2:0.142857 3:-0.787611 4:-0.543829 5:-0.618023 6:-0.64187 7:-0.660189 \n",
      "7 0:-1 1:0.216216 2:0.210084 3:-0.761062 4:-0.43864 5:-0.471419 6:-0.503621 7:-0.600399 \n",
      "8 0:-1 1:0.216216 2:0.243697 3:-0.778761 4:-0.426244 5:-0.458642 6:-0.552337 7:-0.61435 \n",
      "6 1:0.22973 2:0.226891 3:-0.699115 4:-0.414911 5:-0.449899 6:-0.453588 7:-0.703039 \n",
      "6 0:-1 1:0.22973 2:0.193277 3:-0.752212 4:-0.519037 5:-0.585071 6:-0.628703 7:-0.63727 \n",
      "8 1:0.22973 2:0.176471 3:-0.734513 4:-0.371702 5:-0.455952 6:-0.402238 7:-0.574489 \n",
      "9 0:-1 1:0.256757 2:0.277311 3:-0.752212 4:-0.48114 5:-0.557498 6:-0.581303 7:-0.578475 \n",
      "8 1:0.283784 2:0.243697 3:-0.778761 4:-0.318576 5:-0.264963 6:-0.58262 7:-0.574489 \n",
      "7 1:0.297297 2:0.243697 3:-0.752212 4:-0.319285 5:-0.409549 6:-0.411455 7:-0.524664 \n",
      "9 1:0.337838 2:0.310924 3:-0.734513 4:-0.296618 5:-0.32347 6:-0.514154 7:-0.503737 \n",
      "8 1:0.337838 2:0.277311 3:-0.752212 4:-0.393306 5:-0.476126 6:-0.485188 7:-0.54559 \n",
      "7 0:-1 1:0.351351 2:0.327731 3:-0.725664 4:-0.32991 5:-0.424344 6:-0.458855 7:-0.486796 \n",
      "8 1:0.364865 2:0.310924 3:-0.743363 4:-0.37241 5:-0.449899 6:-0.523371 7:-0.519681 \n",
      "8 1:0.378378 2:0.327731 3:-0.716814 4:-0.360723 5:-0.456624 6:-0.418038 7:-0.537618 \n",
      "7 0:-1 1:0.391892 2:0.378151 3:-0.752212 4:-0.260492 5:-0.369872 6:-0.308756 7:-0.478824 \n",
      "9 1:0.405405 2:0.394958 3:-0.725664 4:-0.167345 5:-0.27236 6:-0.292956 7:-0.385152 \n",
      "9 1:0.405405 2:0.378151 3:-0.734513 4:-0.238888 5:-0.340955 6:-0.421988 7:-0.430992 \n",
      "11 1:0.405405 2:0.378151 3:-0.734513 4:-0.275013 5:-0.447209 6:-0.278473 7:-0.427005 \n",
      "8 1:0.418919 2:0.361345 3:-0.743363 4:-0.340889 5:-0.465367 6:-0.410138 7:-0.508719 \n",
      "8 1:0.418919 2:0.361345 3:-0.734513 4:-0.126616 5:-0.190989 6:-0.279789 7:-0.425012 \n",
      "9 0:-1 1:0.418919 2:0.361345 3:-0.734513 4:-0.118116 5:-0.283793 6:-0.281106 7:-0.425012 \n",
      "8 0:-1 1:0.445946 2:0.428571 3:-0.734513 4:-0.187179 5:-0.242771 6:-0.279789 7:-0.476831 \n",
      "10 1:0.459459 2:0.445378 3:-0.716814 4:-0.181512 5:-0.328178 6:-0.344305 7:-0.375187 \n",
      "10 1:0.459459 2:0.495798 3:-0.707965 4:-0.0614486 5:-0.194351 6:-0.207373 7:-0.295466 \n",
      "8 0:-1 1:0.459459 2:0.394958 3:-0.725664 4:-0.151408 5:-0.317418 6:-0.158657 7:-0.421026 \n",
      "9 1:0.472973 2:0.529412 3:-0.690265 4:-0.10147 5:-0.273033 6:-0.150757 7:-0.360239 \n",
      "10 1:0.472973 2:0.445378 3:-0.690265 4:-0.140429 5:-0.268325 6:-0.335089 7:-0.315396 \n",
      "9 1:0.472973 2:0.411765 3:-0.716814 4:-0.0632194 5:-0.0780094 6:-0.387755 7:-0.350274 \n",
      "10 0:-1 1:0.486486 2:0.428571 3:-0.699115 4:-0.0412608 5:-0.0988568 6:-0.295589 7:-0.328351 \n",
      "9 1:0.486486 2:0.462185 3:-0.707965 4:-0.203117 5:-0.359785 6:-0.378539 7:-0.368211 \n",
      "8 1:0.486486 2:0.462185 3:-0.690265 4:-0.216929 5:-0.398117 6:-0.431205 7:-0.286497 \n",
      "11 1:0.5 2:0.478992 3:-0.646018 4:0.008323 5:-0.114997 6:-0.116524 7:-0.24564 \n",
      "9 1:0.5 2:0.478992 3:-0.743363 4:-0.18895 5:-0.267653 6:-0.300856 7:-0.428002 \n",
      "8 0:-1 1:0.5 2:0.428571 3:-0.707965 4:-0.0904905 5:-0.188971 6:-0.287689 7:-0.305431 \n",
      "11 1:0.513514 2:0.478992 3:-0.681416 4:0.129095 5:-0.171486 6:-0.166557 7:-0.265571 \n",
      "11 1:0.513514 2:0.478992 3:-0.654867 4:-0.0826988 5:-0.253531 6:-0.215273 7:-0.265571 \n",
      "9 0:-1 1:0.540541 2:0.462185 3:-0.716814 4:-0.115282 5:-0.281103 6:-0.120474 7:-0.372197 \n",
      "12 0:-1 1:0.540541 2:0.495798 3:-0.690265 4:0.068532 5:-0.0954943 6:-0.0125082 7:-0.250623 \n",
      "9 1:0.554054 2:0.495798 3:-0.672566 4:0.0196565 5:-0.00470746 6:-0.223173 7:-0.323368 \n",
      "9 0:-1 1:0.608108 2:0.563025 3:-0.663717 4:0.1592 5:0.0901143 6:-0.0296248 7:-0.223717 \n",
      "13 1:0.662162 2:0.647059 3:-0.637168 4:0.367806 5:0.0551446 6:0.128374 7:-0.0104634 \n",
      "8 0:-1 1:0.662162 2:0.630252 3:-0.672566 4:0.209846 5:0.0376597 6:0.0138249 7:-0.141006 \n",
      "10 1:0.675676 2:0.663866 3:-0.725664 4:0.308659 5:0.0302623 6:0.162607 7:-0.170902 \n",
      "12 0:-1 1:0.675676 2:0.579832 3:-0.690265 4:0.232513 5:-0.065232 6:0.0230415 7:0.00348779 \n",
      "13 1:0.689189 2:0.747899 3:-0.637168 4:0.271826 5:-0.0201748 6:0.0348914 7:0.0264076 \n",
      "10 1:0.702703 2:0.697479 3:-0.637168 4:0.685143 5:0.332213 6:0.316656 7:0.240658 \n",
      "12 0:-1 1:0.864865 2:0.781513 3:-0.681416 4:0.697184 5:0.515804 6:0.346939 7:0.060289 \n",
      "10 0:-1 1:0.878378 2:0.831933 3:-0.619469 4:0.553037 5:0.412912 6:0.267939 7:0.160937 \n",
      "4 0:1 1:-0.608108 2:-0.647059 3:-0.911504 4:-0.966708 5:-0.972428 6:-0.974984 7:-0.973094 \n",
      "5 0:1 1:-0.459459 2:-0.495798 3:-0.876106 4:-0.926687 5:-0.335575 6:-0.951284 7:-0.940209 \n",
      "5 0:1 1:-0.418919 2:-0.478992 3:-0.893805 4:-0.927395 5:-0.945528 6:-0.943384 7:-0.933234 \n",
      "6 0:1 1:-0.310811 2:-0.378151 3:-0.867257 4:-0.885957 5:-0.901143 6:-0.914417 7:-0.907324 \n",
      "6 0:1 1:-0.243243 2:-0.226891 3:-0.831858 4:-0.840269 5:-0.872898 6:-0.876234 7:-0.860488 \n",
      "6 0:1 1:-0.189189 2:-0.210084 3:-0.823009 4:-0.84629 5:-0.876933 6:-0.901251 7:-0.853513 \n",
      "8 0:1 1:-0.0810811 2:-0.12605 3:-0.823009 4:-0.743226 5:-0.763954 6:-0.791968 7:-0.813652 \n",
      "6 0:1 1:-0.0540541 2:-0.0756303 3:-0.79646 4:-0.770143 5:-0.824479 6:-0.798552 7:-0.797708 \n",
      "7 0:1 1:-0.0540541 2:-0.0420168 3:-0.823009 4:-0.752435 5:-0.782784 6:-0.785385 7:-0.815645 \n",
      "7 0:1 1:-0.0405405 2:-0.109244 3:-0.823009 4:-0.755977 5:-0.781439 6:-0.790652 7:-0.823617 \n",
      "7 0:1 1:-0.0135135 2:-0.0420168 3:-0.823009 4:-0.713122 5:-0.720242 6:-0.807768 7:-0.797708 \n",
      "7 0:1 1:-0.0135135 2:-0.0588235 3:-0.79646 4:-0.702851 5:-0.768662 6:-0.799868 7:-0.777778 \n",
      "7 0:1 1:0.0405405 2:-0.0252101 3:-0.80531 4:-0.735435 5:-0.796234 6:-0.848585 7:-0.753861 \n",
      "6 0:1 1:0.0405405 2:0.0588235 3:-0.787611 4:-0.623517 5:-0.645595 6:-0.716919 7:-0.734928 \n",
      "7 0:1 1:0.0540541 2:0.00840336 3:-0.814159 4:-0.68833 5:-0.720915 6:-0.744569 7:-0.766816 \n",
      "7 0:1 1:0.0810811 2:0.0420168 3:-0.823009 4:-0.90827 5:-0.728985 6:-0.770902 7:-0.757848 \n",
      "7 0:1 1:0.0810811 2:0.0756303 3:-0.79646 4:-0.632725 5:-0.687962 6:-0.687953 7:-0.713004 \n",
      "8 0:1 1:0.108108 2:0.0756303 3:-0.769912 4:-0.60935 5:-0.643578 6:-0.706386 7:-0.690085 \n",
      "8 0:1 1:0.121622 2:0.0756303 3:-0.778761 4:-0.615725 5:-0.626093 6:-0.698486 7:-0.743896 \n",
      "8 0:-1 1:0.121622 2:0.092437 3:-0.80531 4:-0.608996 5:-0.60659 6:-0.736669 7:-0.704036 \n",
      "7 0:1 1:0.135135 2:0.092437 3:-0.787611 4:-0.638746 5:-0.687962 6:-0.684003 7:-0.731938 \n",
      "8 0:1 1:0.148649 2:0.12605 3:-0.778761 4:-0.588454 5:-0.605918 6:-0.65372 7:-0.683109 \n",
      "9 0:-1 1:0.148649 2:0.092437 3:-0.787611 4:-0.593058 5:-0.634163 6:-0.64582 7:-0.714001 \n",
      "7 0:-1 1:0.162162 2:0.159664 3:-0.761062 4:-0.489286 5:-0.494284 6:-0.608953 7:-0.648231 \n",
      "8 0:1 1:0.175676 2:0.142857 3:-0.725664 4:-0.619267 5:-0.669805 6:-0.715602 7:-0.670154 \n",
      "8 0:1 1:0.175676 2:0.109244 3:-0.734513 4:-0.558704 5:-0.585071 6:-0.687953 7:-0.659193 \n",
      "9 0:1 1:0.189189 2:0.159664 3:-0.778761 4:-0.581725 5:-0.644923 6:-0.691903 7:-0.668161 \n",
      "9 0:1 1:0.202703 2:0.142857 3:-0.761062 4:-0.553037 5:-0.599866 6:-0.660303 7:-0.65421 \n",
      "9 1:0.27027 2:0.260504 3:-0.752212 4:-0.412077 5:-0.415602 6:-0.553654 7:-0.602392 \n",
      "7 0:-1 1:0.27027 2:0.226891 3:-0.743363 4:-0.450328 5:-0.497646 6:-0.59447 7:-0.59442 \n",
      "7 0:-1 1:0.27027 2:0.226891 3:-0.787611 4:-0.444307 5:-0.459314 6:-0.514154 7:-0.664175 \n",
      "9 1:0.27027 2:0.159664 3:-0.752212 4:-0.450328 5:-0.506389 6:-0.435155 7:-0.644245 \n",
      "8 0:1 1:0.283784 2:0.226891 3:-0.769912 4:-0.550912 5:-0.605918 6:-0.62212 7:-0.653214 \n",
      "8 1:0.283784 2:0.294118 3:-0.761062 4:-0.403931 5:-0.417619 6:-0.475971 7:-0.63428 \n",
      "8 0:1 1:0.297297 2:0.243697 3:-0.769912 4:-0.542412 5:-0.62004 6:-0.651086 7:-0.583458 \n",
      "9 0:-1 1:0.324324 2:0.260504 3:-0.769912 4:-0.446078 5:-0.531271 6:-0.504937 7:-0.578475 \n",
      "9 1:0.337838 2:0.327731 3:-0.681416 4:-0.358243 5:-0.461332 6:-0.429888 7:-0.494768 \n",
      "9 0:-1 1:0.337838 2:0.327731 3:-0.761062 4:-0.278909 5:-0.26698 6:-0.464121 7:-0.504733 \n",
      "9 1:0.337838 2:0.260504 3:-0.716814 4:-0.426952 5:-0.480161 6:-0.58262 7:-0.547583 \n",
      "8 1:0.351351 2:0.428571 3:-0.734513 4:-0.366035 5:-0.431742 6:-0.499671 7:-0.508719 \n",
      "8 0:-1 1:0.364865 2:0.344538 3:-0.769912 4:-0.39791 5:-0.449899 6:-0.54707 7:-0.554559 \n",
      "7 1:0.378378 2:0.327731 3:-0.734513 4:-0.336993 5:-0.373235 6:-0.466754 7:-0.554559 \n",
      "8 1:0.378378 2:0.277311 3:-0.752212 4:-0.508766 5:-0.586416 6:-0.661619 7:-0.555556 \n",
      "8 0:-1 1:0.391892 2:0.394958 3:-0.734513 4:-0.391535 5:-0.445864 6:-0.569454 7:-0.506726 \n",
      "9 0:-1 1:0.391892 2:0.361345 3:-0.752212 4:-0.290243 5:-0.33423 6:-0.431205 7:-0.484803 \n",
      "8 1:0.391892 2:0.361345 3:-0.716814 4:-0.28493 5:-0.402824 6:-0.312706 7:-0.491779 \n",
      "8 1:0.405405 2:0.378151 3:-0.734513 4:-0.221888 5:-0.273033 6:-0.564187 7:-0.474838 \n",
      "9 0:-1 1:0.405405 2:0.394958 3:-0.707965 4:-0.216575 5:-0.340282 6:-0.389072 7:-0.336323 \n",
      "9 0:-1 1:0.405405 2:0.361345 3:-0.752212 4:-0.39791 5:-0.434432 6:-0.407505 7:-0.550573 \n",
      "10 0:-1 1:0.418919 2:0.462185 3:-0.628319 4:0.40641 5:0.35037 6:0.102041 7:-0.0244145 \n",
      "10 1:0.432432 2:0.428571 3:-0.734513 4:-0.237117 5:-0.395427 6:-0.254773 7:-0.419033 \n",
      "9 1:0.459459 2:0.411765 3:-0.699115 4:-0.254117 5:-0.271015 6:-0.353522 7:-0.535625 \n",
      "7 0:-1 1:0.459459 2:0.327731 3:-0.734513 4:-0.152825 5:-0.0504371 6:-0.449638 7:-0.503737 \n",
      "9 1:0.459459 2:0.394958 3:-0.725664 4:-0.233575 5:-0.209818 6:-0.450955 7:-0.512706 \n",
      "10 0:-1 1:0.459459 2:0.411765 3:-0.690265 4:-0.220117 5:-0.377942 6:-0.186307 7:-0.45989 \n",
      "9 0:-1 1:0.472973 2:0.462185 3:-0.725664 4:-0.222242 5:-0.322125 6:-0.349572 7:-0.385152 \n",
      "10 0:-1 1:0.472973 2:0.428571 3:-0.734513 4:-0.221179 5:-0.333557 6:-0.361422 7:-0.395117 \n",
      "11 0:-1 1:0.486486 2:0.478992 3:-0.672566 4:-0.0214273 5:-0.0457297 6:-0.210007 7:-0.315396 \n",
      "9 1:0.486486 2:0.462185 3:-0.725664 4:-0.211617 5:-0.35037 6:-0.271889 7:-0.386148 \n",
      "10 0:-1 1:0.486486 2:0.428571 3:-0.743363 4:-0.232867 5:-0.376597 6:-0.357472 7:-0.351271 \n",
      "11 0:-1 1:0.5 2:0.512605 3:-0.734513 4:-0.0688861 5:-0.150639 6:-0.352205 7:-0.265571 \n",
      "10 0:-1 1:0.5 2:0.529412 3:-0.690265 4:-0.0511776 5:-0.117014 6:-0.311389 7:-0.255605 \n",
      "8 0:-1 1:0.5 2:0.378151 3:-0.734513 4:-0.273951 5:-0.279085 6:-0.506254 7:-0.652217 \n",
      "9 0:-1 1:0.540541 2:0.546218 3:-0.716814 4:-0.162387 5:-0.32078 6:-0.182357 7:-0.335326 \n",
      "9 0:-1 1:0.540541 2:0.428571 3:-0.734513 4:-0.157075 5:-0.186954 6:-0.317972 7:-0.434978 \n",
      "9 1:0.540541 2:0.563025 3:-0.681416 4:-0.0911989 5:-0.224613 6:-0.074391 7:-0.371201 \n",
      "9 0:-1 1:0.554054 2:0.546218 3:-0.778761 4:-0.16522 5:-0.297243 6:-0.25609 7:-0.350274 \n",
      "9 0:-1 1:0.554054 2:0.563025 3:-0.690265 4:-0.105012 5:-0.174176 6:-0.270573 7:-0.333333 \n",
      "10 1:0.554054 2:0.613445 3:-0.690265 4:-0.0880113 5:-0.181574 6:-0.273206 7:-0.317389 \n",
      "9 0:-1 1:0.554054 2:0.529412 3:-0.725664 4:-0.0047813 5:-0.0309348 6:-0.21659 7:-0.335326 \n",
      "9 1:0.554054 2:0.462185 3:-0.725664 4:-0.206658 5:-0.268325 6:-0.400922 7:-0.394121 \n",
      "10 0:-1 1:0.581081 2:0.546218 3:-0.707965 4:0.0231982 5:-0.067922 6:-0.21659 7:-0.254609 \n",
      "9 1:0.594595 2:0.512605 3:-0.707965 4:-0.0458651 5:-0.196369 6:-0.16524 7:-0.285501 \n",
      "11 0:-1 1:0.608108 2:0.495798 3:-0.646018 4:-0.102532 5:-0.226631 6:-0.215273 7:-0.303438 \n",
      "13 0:-1 1:0.608108 2:0.529412 3:-0.681416 4:0.188596 5:0.244116 6:-0.217907 7:-0.218734 \n",
      "13 1:0.621622 2:0.663866 3:-0.663717 4:0.0972198 5:-0.0457297 6:-0.0309414 7:-0.181863 \n",
      "11 0:-1 1:0.635135 2:0.563025 3:-0.707965 4:0.0451567 5:-0.0275723 6:-0.266623 7:-0.193822 \n",
      "11 0:-1 1:0.635135 2:0.596639 3:-0.681416 4:0.0816363 5:0.0255548 6:-0.18104 7:-0.200797 \n",
      "10 0:-1 1:0.689189 2:0.579832 3:-0.690265 4:0.2442 5:0.174849 6:-0.0493746 7:-0.0662681 \n",
      "13 0:-1 1:0.689189 2:0.663866 3:-0.646018 4:0.0773862 5:-0.069267 6:-0.195523 7:-0.125062 \n",
      "11 1:0.756757 2:0.596639 3:-0.663717 4:0.225075 5:0.114997 6:0.0467413 7:-0.195815 \n",
      "9 0:-1 1:0.756757 2:0.663866 3:-0.646018 4:0.0681778 5:0.173504 6:0.121791 7:0.0104634 \n",
      "11 0:-1 1:0.783784 2:0.731092 3:-0.690265 4:0.330264 5:0.221923 6:0.0177749 7:-0.0303936 \n",
      "12 1:0.797297 2:0.747899 3:-0.610619 4:0.423765 5:0.197714 6:0.385122 7:-0.064275 \n",
      "11 0:-1 1:0.824324 2:0.680672 3:-0.619469 4:0.557641 5:0.426362 6:0.377222 7:0.0503239 \n",
      "3 0:1 1:-0.689189 2:-0.714286 3:-0.946903 4:-0.979104 5:-0.984533 6:-0.98815 7:-0.982063 \n",
      "4 0:1 1:-0.635135 2:-0.680672 3:-0.920354 4:-0.973083 5:-0.983188 6:-0.98025 7:-0.982063 \n",
      "4 0:1 1:-0.527027 2:-0.596639 3:-0.893805 4:-0.956437 5:-0.964358 6:-0.98025 7:-0.963129 \n",
      "5 0:1 1:-0.418919 2:-0.462185 3:-0.884956 4:-0.931645 5:-0.944183 6:-0.946017 7:-0.941206 \n",
      "6 0:1 1:-0.297297 2:-0.344538 3:-0.858407 4:-0.883124 5:-0.910558 6:-0.915734 7:-0.888391 \n",
      "6 0:1 1:-0.283784 2:-0.361345 3:-0.849558 4:-0.858686 5:-0.866174 6:-0.901251 7:-0.897359 \n",
      "6 0:1 1:-0.27027 2:-0.327731 3:-0.831858 4:-0.87179 5:-0.900471 6:-0.899934 7:-0.883408 \n",
      "7 0:1 1:-0.243243 2:-0.327731 3:-0.858407 4:-0.868957 5:-0.896436 6:-0.868334 7:-0.887394 \n",
      "5 0:1 1:-0.22973 2:-0.310924 3:-0.858407 4:-0.87427 5:-0.901816 6:-0.918367 7:-0.883408 \n",
      "7 0:1 1:-0.202703 2:-0.260504 3:-0.840708 4:-0.855144 5:-0.872226 6:-0.897301 7:-0.887394 \n",
      "8 0:1 1:-0.189189 2:-0.210084 3:-0.752212 4:-0.788914 5:-0.813046 6:-0.836735 7:-0.838565 \n",
      "7 0:1 1:-0.189189 2:-0.260504 3:-0.831858 4:-0.838852 5:-0.873571 6:-0.857801 7:-0.87145 \n",
      "7 0:1 1:-0.162162 2:-0.176471 3:-0.778761 4:-0.758456 5:-0.772024 6:-0.807768 7:-0.841555 \n",
      "8 0:1 1:-0.162162 2:-0.226891 3:-0.849558 4:-0.828582 5:-0.837929 6:-0.884134 7:-0.867464 \n",
      "8 0:1 1:-0.135135 2:-0.109244 3:-0.823009 4:-0.783602 5:-0.800941 6:-0.860434 7:-0.823617 \n",
      "7 0:1 1:-0.121622 2:-0.159664 3:-0.823009 4:-0.759518 5:-0.764627 6:-0.836735 7:-0.830593 \n",
      "7 0:1 1:-0.108108 2:-0.159664 3:-0.823009 4:-0.809456 5:-0.871553 6:-0.840685 7:-0.821624 \n",
      "8 0:1 1:-0.108108 2:-0.142857 3:-0.80531 4:-0.356827 5:-0.441829 6:-0.454905 7:-0.804684 \n",
      "7 0:1 1:-0.108108 2:-0.159664 3:-0.823009 4:-0.811581 5:-0.847344 6:-0.861751 7:-0.833582 \n",
      "6 0:1 1:-0.108108 2:-0.176471 3:-0.840708 4:-0.79706 5:-0.815736 6:-0.834101 7:-0.850523 \n",
      "8 0:1 1:-0.0945946 2:-0.12605 3:-0.823009 4:-0.788914 5:-0.834566 6:-0.849901 7:-0.803687 \n",
      "8 0:1 1:-0.0945946 2:-0.092437 3:-0.80531 4:-0.770498 5:-0.823134 6:-0.803818 7:-0.801694 \n",
      "7 0:1 1:-0.0810811 2:-0.0588235 3:-0.823009 4:-0.747831 5:-0.774042 6:-0.824885 7:-0.793722 \n",
      "8 0:1 1:-0.0675676 2:-0.092437 3:-0.79646 4:-0.778998 5:-0.827169 6:-0.834101 7:-0.803687 \n",
      "8 0:1 1:-0.0675676 2:-0.12605 3:-0.80531 4:-0.71631 5:-0.751849 6:-0.782752 7:-0.800698 \n",
      "7 0:1 1:-0.0405405 2:-0.0420168 3:-0.80531 4:-0.743226 5:-0.787492 6:-0.776169 7:-0.793722 \n",
      "9 0:1 2:0.0252101 3:-0.80531 4:-0.701434 5:-0.756557 6:-0.799868 7:-0.723966 \n",
      "8 0:-1 1:0.0135135 2:-0.092437 3:-0.79646 4:-0.696476 5:-0.70074 6:-0.794602 7:-0.772795 \n",
      "7 0:1 1:0.0135135 2:-0.0588235 3:-0.831858 4:-0.753143 5:-0.784129 6:-0.836735 7:-0.766816 \n",
      "8 0:1 1:0.027027 2:-0.0420168 3:-0.79646 4:-0.657163 5:-0.650303 6:-0.828835 7:-0.740907 \n",
      "8 0:1 1:0.0405405 2:-0.00840336 3:-0.823009 4:-0.667788 5:-0.662408 6:-0.798552 7:-0.757848 \n",
      "8 0:1 1:0.0405405 2:-0.0252101 3:-0.814159 4:-0.707455 5:-0.749832 6:-0.772219 7:-0.783757 \n",
      "7 0:1 1:0.0810811 2:0.00840336 3:-0.79646 4:-0.633434 5:-0.626093 6:-0.769585 7:-0.738914 \n",
      "11 0:-1 1:0.0945946 2:0.0756303 3:-0.787611 4:-0.58385 5:-0.66039 6:-0.664253 7:-0.660189 \n",
      "8 0:1 1:0.108108 2:0.092437 3:-0.778761 4:-0.632017 5:-0.70343 6:-0.690586 7:-0.684106 \n",
      "8 0:1 1:0.108108 2:0.0420168 3:-0.752212 4:-0.684434 5:-0.746469 6:-0.757735 7:-0.543597 \n",
      "10 0:1 1:0.121622 2:0.0420168 3:-0.778761 4:-0.605808 5:-0.662408 6:-0.669519 7:-0.681116 \n",
      "9 0:1 1:0.162162 2:0.109244 3:-0.778761 4:-0.579246 5:-0.671822 6:-0.745885 7:-0.584454 \n",
      "9 0:1 1:0.162162 2:0.092437 3:-0.761062 4:-0.619975 5:-0.645595 6:-0.751152 7:-0.674141 \n",
      "8 0:1 1:0.175676 2:0.109244 3:-0.743363 4:-0.458474 5:-0.465367 6:-0.632653 7:-0.643249 \n",
      "9 1:0.189189 2:0.142857 3:-0.761062 4:-0.635913 5:-0.73033 6:-0.65372 7:-0.68012 \n",
      "7 0:-1 1:0.189189 2:0.193277 3:-0.752212 4:-0.480432 5:-0.589106 6:-0.640553 7:-0.604385 \n",
      "8 0:1 1:0.189189 2:0.12605 3:-0.80531 4:-0.625288 5:-0.67653 6:-0.743252 7:-0.681116 \n",
      "8 0:1 1:0.216216 2:0.109244 3:-0.769912 4:-0.571454 5:-0.6846 6:-0.672153 7:-0.61435 \n",
      "10 1:0.216216 2:0.210084 3:-0.734513 4:-0.501682 5:-0.558843 6:-0.61422 7:-0.606378 \n",
      "8 0:1 1:0.216216 2:0.159664 3:-0.769912 4:-0.544891 5:-0.537323 6:-0.662936 7:-0.604385 \n",
      "9 0:1 1:0.216216 2:0.0756303 3:-0.787611 4:-0.5541 5:-0.591796 6:-0.701119 7:-0.624315 \n",
      "10 0:-1 1:0.243243 2:0.260504 3:-0.725664 4:-0.445723 5:-0.559516 6:-0.556287 7:-0.514699 \n",
      "8 1:0.27027 2:0.294118 3:-0.734513 4:-0.330264 5:-0.509079 6:-0.371955 7:-0.454908 \n",
      "10 0:1 1:0.283784 2:0.260504 3:-0.743363 4:-0.497078 5:-0.594486 6:-0.601053 7:-0.554559 \n",
      "10 0:1 1:0.283784 2:0.243697 3:-0.743363 4:-0.370993 5:-0.419637 6:-0.55102 7:-0.532636 \n",
      "9 0:1 1:0.283784 2:0.226891 3:-0.725664 4:-0.35541 5:-0.335575 6:-0.526004 7:-0.59442 \n",
      "10 0:1 1:0.283784 2:0.243697 3:-0.761062 4:-0.536745 5:-0.655683 6:-0.553654 7:-0.598406 \n",
      "9 0:1 1:0.283784 2:0.378151 3:-0.734513 4:-0.33841 5:-0.354405 6:-0.54312 7:-0.517688 \n",
      "11 0:1 1:0.297297 2:0.277311 3:-0.743363 4:-0.507349 5:-0.648958 6:-0.58657 7:-0.524664 \n",
      "8 1:0.297297 2:0.310924 3:-0.690265 4:-0.189304 5:-0.260256 6:-0.358789 7:-0.447932 \n",
      "10 0:1 1:0.310811 2:0.294118 3:-0.752212 4:-0.417036 5:-0.460659 6:-0.635286 7:-0.514699 \n",
      "11 0:1 1:0.310811 2:0.277311 3:-0.761062 4:-0.491411 5:-0.558843 6:-0.730086 7:-0.50274 \n",
      "11 0:1 1:0.324324 2:0.260504 3:-0.734513 4:-0.419515 5:-0.554808 6:-0.557604 7:-0.425012 \n",
      "10 1:0.337838 2:0.310924 3:-0.725664 4:-0.281034 5:-0.293208 6:-0.468071 7:-0.474838 \n",
      "9 1:0.351351 2:0.277311 3:-0.725664 4:-0.365681 5:-0.447882 6:-0.389072 7:-0.544594 \n",
      "10 0:-1 1:0.364865 2:0.294118 3:-0.690265 4:-0.133345 5:-0.274378 6:-0.289006 7:-0.352267 \n",
      "11 1:0.364865 2:0.378151 3:-0.743363 4:-0.302639 5:-0.3692 6:-0.433838 7:-0.504733 \n",
      "9 1:0.364865 2:0.243697 3:-0.734513 4:-0.403577 5:-0.511096 6:-0.514154 7:-0.463876 \n",
      "10 0:1 1:0.378378 2:0.361345 3:-0.743363 4:-0.401806 5:-0.545393 6:-0.561554 7:-0.415047 \n",
      "10 0:-1 1:0.378378 2:0.378151 3:-0.707965 4:-0.374535 5:-0.459987 6:-0.573404 7:-0.456901 \n",
      "9 0:1 1:0.378378 2:0.226891 3:-0.743363 4:-0.524349 5:-0.61197 6:-0.647136 7:-0.564524 \n",
      "8 1:0.378378 2:0.344538 3:-0.769912 4:-0.381264 5:-0.448554 6:-0.457538 7:-0.554559 \n",
      "9 0:-1 1:0.391892 2:0.394958 3:-0.743363 4:-0.347264 5:-0.390047 6:-0.545754 7:-0.496761 \n",
      "10 0:-1 1:0.391892 2:0.411765 3:-0.752212 4:-0.309368 5:-0.379287 6:-0.468071 7:-0.454908 \n",
      "10 0:-1 1:0.405405 2:0.411765 3:-0.752212 4:-0.271472 5:-0.338937 6:-0.429888 7:-0.448929 \n",
      "8 0:-1 1:0.418919 2:0.428571 3:-0.840708 4:-0.257659 5:-0.386685 6:-0.294273 7:-0.444943 \n",
      "11 0:-1 1:0.418919 2:0.478992 3:-0.672566 4:-0.211971 5:-0.321453 6:-0.307439 7:-0.27155 \n",
      "9 0:-1 1:0.418919 2:0.327731 3:-0.743363 4:-0.380202 5:-0.419637 6:-0.593153 7:-0.524664 \n",
      "9 0:-1 1:0.418919 2:0.529412 3:-0.672566 4:-0.0911989 5:-0.181574 6:-0.278473 7:-0.375187 \n",
      "10 0:-1 1:0.445946 2:0.428571 3:-0.672566 4:-0.0759695 5:-0.0739744 6:-0.233706 7:-0.425012 \n",
      "11 1:0.445946 2:0.327731 3:-0.769912 4:-0.383389 5:-0.478144 6:-0.549704 7:-0.460887 \n",
      "10 1:0.459459 2:0.361345 3:-0.734513 4:-0.274305 5:-0.337592 6:-0.472021 7:-0.455904 \n",
      "9 1:0.472973 2:0.378151 3:-0.752212 4:-0.179387 5:-0.193679 6:-0.423305 7:-0.391131 \n",
      "10 1:0.472973 2:0.428571 3:-0.707965 4:-0.284222 5:-0.285138 6:0.148124 7:-0.35725 \n",
      "10 0:-1 1:0.486486 2:0.495798 3:-0.752212 4:-0.225075 5:-0.268998 6:-0.431205 7:-0.415047 \n",
      "10 0:-1 1:0.486486 2:0.462185 3:-0.707965 4:-0.147866 5:-0.305313 6:-0.183673 7:-0.312407 \n",
      "12 0:-1 1:0.5 2:0.512605 3:-0.690265 4:-0.136533 5:-0.254876 6:-0.337722 7:-0.325361 \n",
      "10 1:0.5 2:0.411765 3:-0.725664 4:-0.292722 5:-0.39341 6:-0.337722 7:-0.474838 \n",
      "12 0:-1 1:0.5 2:0.394958 3:-0.734513 4:-0.197096 5:-0.276395 6:-0.389072 7:-0.382162 \n",
      "10 0:-1 1:0.5 2:0.579832 3:-0.654867 4:-0.0710112 5:-0.337592 6:-0.325872 7:-0.0762332 \n",
      "11 0:-1 1:0.527027 2:0.512605 3:-0.725664 4:-0.154595 5:-0.252858 6:-0.445688 7:-0.313403 \n",
      "10 0:-1 1:0.527027 2:0.445378 3:-0.734513 4:-0.223659 5:-0.302623 6:-0.416722 7:-0.370204 \n",
      "10 0:-1 1:0.527027 2:0.478992 3:-0.699115 4:-0.194617 5:-0.275723 6:-0.258723 7:-0.434978 \n",
      "11 1:0.527027 2:0.478992 3:-0.699115 4:-0.132637 5:-0.3423 6:-0.00855826 7:-0.429995 \n",
      "10 0:-1 1:0.527027 2:0.546218 3:-0.858407 4:-0.263326 5:-0.308675 6:-0.539171 7:-0.65421 \n",
      "9 0:-1 1:0.554054 2:0.563025 3:-0.725664 4:-0.0324066 5:-0.169469 6:-0.242923 7:-0.275536 \n",
      "12 0:-1 1:0.554054 2:0.529412 3:-0.690265 4:0.022844 5:-0.129119 6:-0.289006 7:-0.106129 \n",
      "9 1:0.581081 2:0.512605 3:-0.663717 4:-0.00655215 5:-0.160054 6:-0.112574 7:-0.256602 \n",
      "11 1:0.581081 2:0.579832 3:-0.646018 4:0.0348858 5:-0.123739 6:-0.21264 7:-0.161933 \n",
      "9 1:0.621622 2:0.579832 3:-0.699115 4:0.210554 5:0.123739 6:-0.074391 7:-0.0563029 \n",
      "13 0:-1 1:0.689189 2:0.630252 3:-0.637168 4:0.231096 5:0.0591796 6:-0.0190915 7:-0.0333832 \n",
      "9 1:0.702703 2:0.630252 3:-0.637168 4:0.243138 5:0.110289 6:0.0967742 7:-0.0842053 \n",
      "11 0:-1 1:0.716216 2:0.714286 3:-0.646018 4:0.132637 5:-0.0517821 6:-0.156024 7:-0.106129 \n",
      "10 0:-1 1:0.743243 2:0.663866 3:-0.637168 4:0.532141 5:0.485541 6:0.381172 7:-0.197808 \n",
      "9 0:-1 1:0.756757 2:0.731092 3:-0.663717 4:0.649371 5:0.683927 6:0.423305 7:0.0333832 \n",
      "5 0:1 1:-0.554054 2:-0.613445 3:-0.911504 4:-0.962812 5:-0.973773 6:-0.959184 7:-0.972098 \n",
      "4 0:1 1:-0.513514 2:-0.529412 3:-0.902655 4:-0.950062 5:-0.963013 6:-0.95655 7:-0.961136 \n",
      "4 0:1 1:-0.459459 2:-0.512605 3:-0.902655 4:-0.935895 5:-0.950235 6:-0.946017 7:-0.951171 \n",
      "7 0:1 1:-0.337838 2:-0.394958 3:-0.840708 4:-0.87179 5:-0.869536 6:-0.913101 7:-0.919283 \n",
      "6 0:1 1:-0.324324 2:-0.378151 3:-0.867257 4:-0.893395 5:-0.904506 6:-0.831468 7:-0.917289 \n",
      "7 0:1 1:-0.310811 2:-0.428571 3:-0.867257 4:-0.868957 5:-0.874243 6:-0.897301 7:-0.918286 \n",
      "8 0:1 1:-0.22973 2:-0.277311 3:-0.840708 4:-0.837082 5:-0.839946 6:-0.886768 7:-0.89138 \n",
      "5 0:1 1:-0.189189 2:-0.294118 3:-0.831858 4:-0.862582 5:-0.887021 6:-0.890718 7:-0.886398 \n",
      "6 0:1 1:-0.189189 2:-0.226891 3:-0.840708 4:-0.821144 5:-0.841291 6:-0.844635 7:-0.86846 \n",
      "7 0:1 1:-0.148649 2:-0.210084 3:-0.840708 4:-0.815477 5:-0.843981 6:-0.859118 7:-0.849527 \n",
      "7 0:1 1:0.0135135 2:-0.0588235 3:-0.814159 4:-0.744997 5:-0.789509 6:-0.791968 7:-0.784753 \n",
      "7 0:1 1:0.027027 2:-0.00840336 3:-0.814159 4:-0.686559 5:-0.714862 6:-0.719552 7:-0.780767 \n",
      "7 0:1 1:0.0405405 2:0.0420168 3:-0.79646 4:-0.639455 5:-0.683255 6:-0.690586 7:-0.757848 \n",
      "8 0:1 1:0.135135 2:0.0756303 3:-0.787611 4:-0.584204 5:-0.587761 6:-0.681369 7:-0.72297 \n",
      "9 0:-1 1:0.148649 2:0.0420168 3:-0.769912 4:-0.580308 5:-0.585743 6:-0.715602 7:-0.69706 \n",
      "8 0:1 1:0.148649 2:0.0756303 3:-0.787611 4:-0.626704 5:-0.70074 6:-0.677419 7:-0.684106 \n",
      "9 0:-1 1:0.202703 2:0.159664 3:-0.814159 4:-0.383744 5:-0.394082 6:-0.576037 7:-0.607374 \n",
      "7 0:1 1:0.202703 2:0.142857 3:-0.743363 4:-0.455994 5:-0.431069 6:-0.627386 7:-0.625311 \n",
      "9 1:0.216216 2:0.260504 3:-0.761062 4:-0.403931 5:-0.419637 6:-0.527321 7:-0.641256 \n",
      "7 0:-1 1:0.243243 2:0.176471 3:-0.752212 4:-0.421994 5:-0.460659 6:-0.549704 7:-0.626308 \n",
      "9 1:0.256757 2:0.226891 3:-0.752212 4:-0.432265 5:-0.490249 6:-0.526004 7:-0.584454 \n",
      "9 1:0.256757 2:0.210084 3:-0.734513 4:-0.426598 5:-0.480161 6:-0.507571 7:-0.597409 \n",
      "8 1:0.337838 2:0.243697 3:-0.769912 4:-0.447494 5:-0.504371 6:-0.541804 7:-0.61136 \n",
      "8 0:-1 1:0.337838 2:0.226891 3:-0.752212 4:-0.381973 5:-0.441829 6:-0.56682 7:-0.504733 \n",
      "9 0:-1 1:0.364865 2:0.310924 3:-0.716814 4:-0.304409 5:-0.3423 6:-0.472021 7:-0.464873 \n",
      "8 1:0.364865 2:0.310924 3:-0.761062 4:-0.328493 5:-0.35037 6:-0.522054 7:-0.539611 \n",
      "9 0:-1 1:0.391892 2:0.394958 3:-0.725664 4:-0.170179 5:-0.161399 6:-0.387755 7:-0.4858 \n",
      "10 1:0.391892 2:0.344538 3:-0.734513 4:-0.310076 5:-0.375925 6:-0.460171 7:-0.451918 \n",
      "9 0:-1 1:0.391892 2:0.445378 3:-0.725664 4:-0.237471 5:-0.391392 6:-0.360105 7:-0.385152 \n",
      "9 0:-1 1:0.405405 2:0.277311 3:-0.716814 4:-0.252701 5:-0.429052 6:-0.411455 7:-0.385152 \n",
      "9 0:-1 1:0.418919 2:0.411765 3:-0.690265 4:-0.215158 5:-0.314728 6:-0.327189 7:-0.434978 \n",
      "10 0:-1 1:0.418919 2:0.327731 3:-0.716814 4:-0.192492 5:-0.276395 6:-0.408822 7:-0.391131 \n",
      "10 0:-1 1:0.432432 2:0.411765 3:-0.663717 4:-0.204179 5:-0.207801 6:-0.350889 7:-0.484803 \n",
      "10 1:0.472973 2:0.428571 3:-0.699115 4:-0.219054 5:-0.281775 6:-0.342989 7:-0.430992 \n",
      "10 0:-1 1:0.486486 2:0.411765 3:-0.690265 4:-0.051886 5:-0.119032 6:-0.25609 7:-0.33134 \n",
      "8 0:-1 1:0.486486 2:0.495798 3:-0.769912 4:-0.234992 5:-0.223268 6:-0.462804 7:-0.504733 \n",
      "9 1:0.486486 2:0.445378 3:-0.716814 4:-0.113157 5:-0.206456 6:-0.319289 7:-0.308421 \n",
      "10 0:-1 1:0.5 2:0.462185 3:-0.707965 4:-0.151054 5:-0.227976 6:-0.282423 7:-0.41704 \n",
      "10 0:-1 1:0.5 2:0.445378 3:-0.716814 4:-0.120949 5:-0.163416 6:-0.277156 7:-0.405082 \n",
      "12 1:0.513514 2:0.529412 3:-0.672566 4:-0.0904905 5:-0.29388 6:-0.22449 7:-0.184853 \n",
      "9 1:0.540541 2:0.462185 3:-0.716814 4:-0.175137 5:-0.337592 6:-0.170507 7:-0.407075 \n",
      "8 1:0.540541 2:0.462185 3:-0.716814 4:-0.191075 5:-0.326833 6:-0.24029 7:-0.367215 \n",
      "11 1:0.554054 2:0.579832 3:-0.663717 4:-0.0203648 5:0.192334 6:-0.186307 7:-0.195815 \n",
      "11 1:0.567568 2:0.546218 3:-0.725664 4:-0.0741987 5:-0.29657 6:-0.0901909 7:-0.235675 \n",
      "11 1:0.567568 2:0.546218 3:-0.699115 4:0.0802196 5:0.139879 6:-0.307439 7:-0.343298 \n",
      "11 0:-1 1:0.594595 2:0.546218 3:-0.663717 4:0.1592 5:0.116342 6:-0.0599078 7:-0.263577 \n",
      "11 0:-1 1:0.675676 2:0.630252 3:-0.654867 4:0.196387 5:0.0316073 6:-0.0454246 7:-0.0553064 \n",
      "11 1:0.743243 2:0.714286 3:-0.681416 4:0.216221 5:0.137189 6:0.0704411 7:-0.230693 \n",
      "10 1:0.743243 2:0.663866 3:-0.681416 4:0.0752612 5:-0.144586 6:-0.14549 7:-0.136024 \n",
      "12 1:0.743243 2:0.714286 3:-0.699115 4:0.141137 5:-0.0289173 6:-0.144174 7:-0.017439 \n",
      "10 0:-1 1:0.783784 2:0.731092 3:-0.628319 4:0.582079 5:0.572966 6:0.217907 7:0.0563029 \n",
      "11 0:-1 1:0.797297 2:0.815126 3:-0.663717 4:0.644413 5:0.544721 6:0.345622 7:0.00348779 \n",
      "6 0:1 1:-0.364865 2:-0.411765 3:-0.876106 4:-0.913228 5:-0.933423 6:-0.931534 7:-0.927255 \n",
      "4 0:1 1:-0.351351 2:-0.394958 3:-0.867257 4:-0.910395 5:-0.932751 6:-0.927584 7:-0.922272 \n",
      "5 0:1 1:-0.337838 2:-0.495798 3:-0.858407 4:-0.873207 5:-0.882986 6:-0.911784 7:-0.904335 \n",
      "6 0:1 1:-0.324324 2:-0.344538 3:-0.867257 4:-0.889145 5:-0.900471 6:-0.921001 7:-0.912307 \n",
      "7 0:1 1:-0.297297 2:-0.310924 3:-0.840708 4:-0.862228 5:-0.883658 6:-0.893351 7:-0.89138 \n",
      "5 0:1 1:-0.202703 2:-0.243697 3:-0.849558 4:-0.861165 5:-0.893073 6:-0.881501 7:-0.887394 \n",
      "7 0:1 1:-0.202703 2:-0.277311 3:-0.840708 4:-0.870019 5:-0.907196 6:-0.889401 7:-0.873443 \n",
      "7 0:1 1:-0.189189 2:-0.243697 3:-0.849558 4:-0.849478 5:-0.886348 6:-0.873601 7:-0.859492 \n",
      "8 0:1 1:-0.121622 2:-0.12605 3:-0.840708 4:-0.77156 5:-0.798252 6:-0.809085 7:-0.827603 \n",
      "6 0:1 1:-0.0945946 2:-0.159664 3:-0.831858 4:-0.815477 5:-0.866846 6:-0.865701 7:-0.823617 \n",
      "8 0:1 1:-0.0540541 2:-0.0420168 3:-0.823009 4:-0.738622 5:-0.799597 6:-0.773535 7:-0.773792 \n",
      "7 0:1 1:-0.027027 2:-0.0588235 3:-0.831858 4:-0.790331 5:-0.854741 6:-0.848585 7:-0.773792 \n",
      "7 0:1 2:-0.142857 3:-0.840708 4:-0.763414 5:-0.792199 6:-0.764319 7:-0.832586 \n",
      "9 0:1 1:0.0405405 2:0.0252101 3:-0.752212 4:-0.684788 5:-0.784802 6:-0.772219 7:-0.684106 \n",
      "8 1:0.0540541 2:-0.00840336 3:-0.80531 4:-0.71206 5:-0.779422 6:-0.732719 7:-0.741903 \n",
      "8 0:1 1:0.0675676 2:0.109244 3:-0.769912 4:-0.585621 5:-0.646268 6:-0.693219 7:-0.656203 \n",
      "8 0:1 1:0.0810811 2:0.0756303 3:-0.80531 4:-0.651496 5:-0.717552 6:-0.714286 7:-0.695067 \n",
      "8 0:1 1:0.135135 2:0.0756303 3:-0.787611 4:-0.566495 5:-0.61735 6:-0.64187 7:-0.682113 \n",
      "9 0:1 1:0.148649 2:0.12605 3:-0.769912 4:-0.641934 5:-0.71688 6:-0.727452 7:-0.653214 \n",
      "8 0:1 1:0.148649 2:0.0588235 3:-0.787611 4:-0.615725 5:-0.666443 6:-0.720869 7:-0.70005 \n",
      "9 0:1 1:0.162162 2:0.243697 3:-0.778761 4:-0.568266 5:-0.671822 6:-0.639236 7:-0.604385 \n",
      "8 0:1 1:0.162162 2:0.159664 3:-0.778761 4:-0.604392 5:-0.69805 6:-0.623436 7:-0.664175 \n",
      "8 0:-1 1:0.162162 2:0.0420168 3:-0.79646 4:-0.632371 5:-0.665098 6:-0.748519 7:-0.704036 \n",
      "9 0:1 1:0.175676 2:0.159664 3:-0.743363 4:-0.59235 5:-0.690652 6:-0.624753 7:-0.650224 \n",
      "8 0:1 1:0.175676 2:0.159664 3:-0.778761 4:-0.581016 5:-0.679892 6:-0.658986 7:-0.596413 \n",
      "8 0:1 1:0.202703 2:0.159664 3:-0.80531 4:-0.578537 5:-0.60659 6:-0.697169 7:-0.684106 \n",
      "8 0:-1 1:0.202703 2:0.378151 3:-0.734513 4:-0.328139 5:-0.38803 6:-0.477288 7:-0.494768 \n",
      "8 0:1 1:0.22973 2:0.092437 3:-0.778761 4:-0.565079 5:-0.608608 6:-0.703752 7:-0.63428 \n",
      "10 0:-1 1:0.22973 2:0.176471 3:-0.734513 4:-0.412431 5:-0.527909 6:-0.508887 7:-0.499751 \n",
      "8 1:0.243243 2:0.310924 3:-0.778761 4:-0.383389 5:-0.440484 6:-0.477288 7:-0.524664 \n",
      "11 0:1 1:0.256757 2:0.243697 3:-0.769912 4:-0.423765 5:-0.507061 6:-0.64187 7:-0.512706 \n",
      "10 0:1 1:0.256757 2:0.210084 3:-0.80531 4:-0.562954 5:-0.6308 6:-0.60632 7:-0.651221 \n",
      "8 0:1 1:0.27027 2:0.260504 3:-0.769912 4:-0.463432 5:-0.519839 6:-0.59842 7:-0.593423 \n",
      "10 0:1 1:0.27027 2:0.260504 3:-0.734513 4:-0.475828 5:-0.527236 6:-0.585253 7:-0.58844 \n",
      "8 0:1 1:0.283784 2:0.277311 3:-0.707965 4:-0.431911 5:-0.544048 6:-0.490454 7:-0.516692 \n",
      "8 0:1 1:0.283784 2:0.243697 3:-0.769912 4:-0.531078 5:-0.63887 6:-0.572087 7:-0.584454 \n",
      "9 1:0.283784 2:0.277311 3:-0.752212 4:-0.473703 5:-0.534633 6:-0.54312 7:-0.55157 \n",
      "8 0:1 1:0.310811 2:0.260504 3:-0.769912 4:-0.485745 5:-0.550101 6:-0.623436 7:-0.569507 \n",
      "9 0:1 1:0.310811 2:0.277311 3:-0.769912 4:-0.451036 5:-0.525219 6:-0.545754 7:-0.560538 \n",
      "8 1:0.351351 2:0.243697 3:-0.734513 4:-0.380556 5:-0.389375 6:-0.527321 7:-0.54858 \n",
      "9 0:1 1:0.351351 2:0.344538 3:-0.716814 4:-0.300514 5:-0.335575 6:-0.487821 7:-0.512706 \n",
      "8 0:-1 1:0.351351 2:0.327731 3:-0.707965 4:-0.317514 5:-0.33154 6:-0.500987 7:-0.544594 \n",
      "8 0:-1 1:0.364865 2:0.378151 3:-0.734513 4:-0.359306 5:-0.502354 6:-0.483871 7:-0.425012 \n",
      "10 0:-1 1:0.364865 2:0.361345 3:-0.734513 4:-0.258367 5:-0.301278 6:-0.491771 7:-0.395117 \n",
      "9 1:0.364865 2:0.327731 3:-0.699115 4:-0.313972 5:-0.380632 6:-0.390388 7:-0.508719 \n",
      "9 1:0.364865 2:0.327731 3:-0.734513 4:-0.349743 5:-0.472764 6:-0.443055 7:-0.426009 \n",
      "11 0:-1 1:0.364865 2:0.310924 3:-0.734513 4:-0.326722 5:-0.420982 6:-0.489138 7:-0.430992 \n",
      "11 1:0.364865 2:0.294118 3:-0.778761 4:-0.445015 5:-0.513114 6:-0.486504 7:-0.61435 \n",
      "9 0:1 1:0.378378 2:0.327731 3:-0.761062 4:-0.395785 5:-0.490921 6:-0.508887 7:-0.484803 \n",
      "9 0:-1 1:0.391892 2:0.495798 3:-0.734513 4:-0.192492 5:-0.349025 6:-0.303489 7:-0.315396 \n",
      "9 0:1 1:0.391892 2:0.361345 3:-0.778761 4:-0.466619 5:-0.552118 6:-0.60632 7:-0.52865 \n",
      "10 0:1 1:0.391892 2:0.411765 3:-0.743363 4:-0.311139 5:-0.372562 6:-0.456221 7:-0.486796 \n",
      "10 0:-1 1:0.405405 2:0.394958 3:-0.725664 4:-0.150345 5:-0.33961 6:0.0164582 7:-0.474838 \n",
      "10 0:-1 1:0.405405 2:0.361345 3:-0.699115 4:-0.201346 5:-0.234701 6:-0.328506 7:-0.474838 \n",
      "9 0:1 1:0.418919 2:0.310924 3:-0.761062 4:-0.349389 5:-0.402824 6:-0.465438 7:-0.498754 \n",
      "10 1:0.418919 2:0.428571 3:-0.699115 4:-0.253409 5:-0.386012 6:-0.360105 7:-0.378176 \n",
      "10 0:-1 1:0.418919 2:0.327731 3:-0.654867 4:-0.0522401 5:-0.171486 6:-0.144174 7:-0.284504 \n",
      "9 1:0.418919 2:0.327731 3:-0.734513 4:-0.319639 5:-0.412912 6:-0.415405 7:-0.449925 \n",
      "11 0:-1 1:0.418919 2:0.378151 3:-0.707965 4:-0.25943 5:-0.375925 6:-0.383805 7:-0.375187 \n",
      "13 1:0.432432 2:0.478992 3:-0.699115 4:-0.228263 5:-0.414257 6:-0.286373 7:-0.335326 \n",
      "9 0:-1 1:0.432432 2:0.462185 3:-0.681416 4:-0.174783 5:-0.386685 6:-0.237656 7:-0.256602 \n",
      "10 0:1 1:0.432432 2:0.428571 3:-0.725664 4:-0.29343 5:-0.429724 6:-0.478604 7:-0.405082 \n",
      "11 0:1 1:0.445946 2:0.243697 3:-0.725664 4:-0.258721 5:-0.319435 6:-0.486504 7:-0.456901 \n",
      "12 1:0.445946 2:0.394958 3:-0.654867 4:-0.0993448 5:-0.370545 6:-0.128374 7:-0.209766 \n",
      "11 0:-1 1:0.445946 2:0.428571 3:-0.752212 4:-0.248805 5:-0.30733 6:-0.408822 7:-0.422023 \n",
      "11 0:1 1:0.445946 2:0.462185 3:-0.716814 4:-0.183637 5:-0.212508 6:-0.373272 7:-0.422023 \n",
      "11 1:0.459459 2:0.411765 3:-0.690265 4:-0.155658 5:-0.249496 6:-0.319289 7:-0.372197 \n",
      "9 1:0.459459 2:0.546218 3:-0.761062 4:-0.207013 5:-0.268325 6:-0.394338 7:-0.425012 \n",
      "10 0:-1 1:0.459459 2:0.344538 3:-0.734513 4:-0.340181 5:-0.487559 6:-0.350889 7:-0.481814 \n",
      "10 1:0.459459 2:0.478992 3:-0.707965 4:-0.152825 5:-0.273033 6:-0.246873 7:-0.368211 \n",
      "9 1:0.472973 2:0.411765 3:-0.734513 4:-0.325305 5:-0.389375 6:-0.510204 7:-0.450922 \n",
      "10 0:-1 1:0.472973 2:0.411765 3:-0.654867 4:-0.0391358 5:-0.203093 6:-0.115207 7:-0.256602 \n",
      "11 0:-1 1:0.486486 2:0.478992 3:-0.690265 4:-0.146095 5:-0.287155 6:-0.261356 7:-0.300448 \n",
      "10 0:-1 1:0.486486 2:0.546218 3:-0.707965 4:-0.139366 5:-0.104237 6:-0.457538 7:-0.382162 \n",
      "11 1:0.486486 2:0.495798 3:-0.716814 4:-0.139366 5:-0.231338 6:-0.456221 7:-0.295466 \n",
      "10 1:0.486486 2:0.462185 3:-0.743363 4:-0.349743 5:-0.413584 6:-0.544437 7:-0.444943 \n",
      "10 0:-1 1:0.486486 2:0.462185 3:-0.787611 4:-0.380556 5:-0.38803 6:-0.527321 7:-0.538615 \n",
      "10 1:0.5 2:0.428571 3:-0.707965 4:-0.107845 5:-0.260928 6:-0.271889 7:-0.2287 \n",
      "9 0:-1 1:0.5 2:0.596639 3:-0.681416 4:-0.0950947 5:-0.170141 6:-0.327189 7:-0.375187 \n",
      "10 1:0.5 2:0.445378 3:-0.672566 4:-0.174783 5:-0.264291 6:-0.346939 7:-0.325361 \n",
      "7 0:-1 1:0.5 2:0.529412 3:-0.699115 4:-0.159554 5:-0.340282 6:-0.194207 7:-0.309417 \n",
      "11 1:0.513514 2:0.445378 3:-0.663717 4:-0.0263857 5:-0.148621 6:-0.241606 7:-0.193822 \n",
      "11 0:-1 1:0.513514 2:0.563025 3:-0.690265 4:-0.0862405 5:-0.194351 6:-0.292956 7:-0.27155 \n",
      "10 0:-1 1:0.513514 2:0.445378 3:-0.681416 4:-0.165929 5:-0.357767 6:-0.270573 7:-0.295466 \n",
      "8 1:0.513514 2:0.495798 3:-0.663717 4:-0.0876572 5:-0.203766 6:-0.199473 7:-0.301445 \n",
      "11 0:-1 1:0.513514 2:0.546218 3:-0.716814 4:-0.146095 5:-0.277068 6:-0.258723 7:-0.315396 \n",
      "11 0:-1 1:0.527027 2:0.512605 3:-0.681416 4:-0.0826988 5:-0.207801 6:-0.178407 7:-0.279522 \n",
      "11 0:-1 1:0.527027 2:0.747899 3:-0.690265 4:0.0316982 5:-0.160726 6:-0.300856 7:-0.127055 \n",
      "12 1:0.540541 2:0.445378 3:-0.734513 4:-0.186117 5:-0.203093 6:-0.391705 7:-0.419033 \n",
      "11 1:0.540541 2:0.563025 3:-0.699115 4:-0.153533 5:-0.29388 6:-0.231073 7:-0.371201 \n",
      "12 0:-1 1:0.540541 2:0.478992 3:-0.663717 4:0.0887197 5:-0.178884 6:0.0730744 7:-0.116094 \n",
      "15 0:-1 1:0.554054 2:0.563025 3:-0.654867 4:0.185762 5:-0.069267 6:0.15734 7:-0.0662681 \n",
      "12 1:0.554054 2:0.714286 3:-0.646018 4:0.177616 5:0.0114324 6:-0.0348914 7:-0.145989 \n",
      "11 1:0.567568 2:0.495798 3:-0.637168 4:0.0809279 5:-0.165434 6:-0.0204082 7:-0.099153 \n",
      "12 1:0.567568 2:0.546218 3:-0.646018 4:0.0568443 5:-0.0255548 6:-0.187623 7:-0.195815 \n",
      "13 1:0.581081 2:0.579832 3:-0.716814 4:-0.0968656 5:-0.332213 6:-0.16129 7:-0.217738 \n",
      "10 1:0.581081 2:0.579832 3:-0.681416 4:0.129449 5:0.0430397 6:0.044108 7:-0.284504 \n",
      "10 1:0.594595 2:0.529412 3:-0.690265 4:-0.0235523 5:-0.0934768 6:-0.215273 7:-0.355257 \n",
      "9 0:1 1:0.608108 2:0.445378 3:-0.690265 4:-0.111387 5:-0.281103 6:-0.15339 7:-0.233682 \n",
      "12 1:0.608108 2:0.579832 3:-0.663717 4:0.0802196 5:-0.227303 6:-0.0717577 7:-0.126059 \n",
      "9 0:-1 1:0.608108 2:0.579832 3:-0.699115 4:-0.00938551 5:-0.0396772 6:-0.20474 7:-0.231689 \n",
      "9 0:-1 1:0.608108 2:0.579832 3:-0.654867 4:0.0189481 5:-0.114324 6:-0.296906 7:-0.155954 \n",
      "10 0:-1 1:0.608108 2:0.630252 3:-0.690265 4:0.0483443 5:-0.00739744 6:-0.231073 7:-0.275536 \n",
      "11 0:-1 1:0.635135 2:0.546218 3:-0.716814 4:-0.12697 5:-0.170141 6:-0.310072 7:-0.355257 \n",
      "12 1:0.635135 2:0.512605 3:-0.699115 4:-0.0497609 5:-0.117687 6:-0.219223 7:-0.295466 \n",
      "10 0:-1 1:0.648649 2:0.512605 3:-0.663717 4:0.0844696 5:-0.104237 6:0.0664911 7:-0.18585 \n",
      "13 0:-1 1:0.662162 2:0.546218 3:-0.681416 4:0.305118 5:0.318763 6:0.22449 7:-0.323368 \n",
      "11 0:-1 1:0.72973 2:0.663866 3:-0.690265 4:0.291305 5:0.260256 6:0.000658328 7:-0.126059 \n",
      "13 0:-1 1:0.743243 2:0.764706 3:-0.663717 4:0.477953 5:0.337592 6:0.257406 7:0.0543099 \n",
      "12 0:-1 1:0.783784 2:0.798319 3:-0.637168 4:0.47689 5:0.221251 6:0.246873 7:0.242651 \n",
      "12 0:-1 1:0.810811 2:0.747899 3:-0.646018 4:0.333097 5:0.281775 6:-0.116524 7:-0.0164425 \n",
      "6 0:1 1:-0.337838 2:-0.462185 3:-0.831858 4:-0.785373 5:-0.813046 6:-0.824885 7:-0.826607 \n",
      "8 0:1 1:-0.0405405 2:-0.0252101 3:-0.79646 4:-0.697184 5:-0.716207 6:-0.716919 7:-0.78575 \n",
      "6 0:1 1:-0.0405405 2:-0.0756303 3:-0.823009 4:-0.683372 5:-0.659718 6:-0.784068 7:-0.809666 \n",
      "8 0:-1 1:0.108108 2:0.0420168 3:-0.725664 4:-0.272534 5:-0.431742 6:-0.399605 7:-0.379173 \n",
      "8 0:-1 1:0.121622 2:0.00840336 3:-0.725664 4:-0.306534 5:-0.375925 6:-0.470704 7:-0.503737 \n",
      "7 0:1 1:0.148649 2:0.0588235 3:-0.79646 4:-0.594475 5:-0.589778 6:-0.706386 7:-0.721973 \n",
      "8 1:0.162162 2:0.092437 3:-0.769912 4:-0.510537 5:-0.475454 6:-0.686636 7:-0.701046 \n",
      "9 1:0.175676 2:0.0588235 3:-0.628319 4:-0.16345 5:-0.31809 6:-0.23239 7:-0.319382 \n",
      "8 1:0.216216 2:0.193277 3:-0.761062 4:-0.441473 5:-0.454607 6:-0.479921 7:-0.650224 \n",
      "10 1:0.243243 2:0.159664 3:-0.734513 4:-0.134408 5:-0.170141 6:-0.277156 7:-0.429995 \n",
      "8 0:1 1:0.243243 2:0.159664 3:-0.761062 4:-0.452453 5:-0.506389 6:-0.453588 7:-0.593423 \n",
      "13 0:-1 1:0.243243 2:0.176471 3:-0.690265 4:-0.10147 5:-0.264291 6:-0.14154 7:-0.33134 \n",
      "10 0:-1 1:0.297297 2:0.176471 3:-0.663717 4:-0.00548964 5:-0.178884 6:-0.100724 7:-0.227703 \n",
      "9 0:-1 1:0.297297 2:0.243697 3:-0.734513 4:-0.383035 5:-0.379287 6:-0.515471 7:-0.61136 \n",
      "9 0:-1 1:0.310811 2:0.243697 3:-0.761062 4:-0.334514 5:-0.316745 6:-0.470704 7:-0.609367 \n",
      "9 1:0.391892 2:0.294118 3:-0.752212 4:-0.288118 5:-0.359112 6:-0.447005 7:-0.42003 \n",
      "8 0:-1 1:0.405405 2:0.445378 3:-0.734513 4:-0.23393 5:-0.287828 6:-0.393022 7:-0.452915 \n",
      "8 0:1 1:0.405405 2:0.260504 3:-0.707965 4:-0.304055 5:-0.392737 6:-0.456221 7:-0.45989 \n",
      "11 1:0.405405 2:0.260504 3:-0.628319 4:0.0784487 5:-0.123067 6:0.0414747 7:-0.18585 \n",
      "9 0:-1 1:0.445946 2:0.411765 3:-0.690265 4:-0.276076 5:-0.451244 6:-0.314022 7:-0.361236 \n",
      "9 0:-1 1:0.445946 2:0.445378 3:-0.699115 4:-0.0940322 5:-0.198386 6:-0.202107 7:-0.345291 \n",
      "10 1:0.472973 2:0.495798 3:-0.699115 4:-0.188242 5:-0.264963 6:-0.421988 7:-0.342302 \n",
      "9 1:0.486486 2:0.462185 3:-0.80531 4:-0.196742 5:-0.293208 6:-0.497038 7:-0.42003 \n",
      "9 1:0.513514 2:0.529412 3:-0.699115 4:-0.134762 5:-0.28581 6:-0.287689 7:-0.297459 \n",
      "11 1:0.513514 2:0.579832 3:-0.681416 4:-0.0313441 5:-0.148621 6:-0.163924 7:-0.279522 \n",
      "9 0:-1 1:0.527027 2:0.445378 3:-0.716814 4:-0.288826 5:-0.38803 6:-0.410138 7:-0.437967 \n",
      "11 0:-1 1:0.527027 2:0.478992 3:-0.707965 4:-0.0756154 5:-0.0894418 6:-0.23239 7:-0.472845 \n",
      "10 0:-1 1:0.540541 2:0.512605 3:-0.672566 4:0.0348858 5:-0.205111 6:0.0269914 7:-0.173891 \n",
      "10 1:0.567568 2:0.512605 3:-0.690265 4:-0.087303 5:-0.166779 6:-0.22054 7:-0.353264 \n",
      "11 1:0.608108 2:0.546218 3:-0.699115 4:0.00903134 5:-0.112979 6:-0.107307 7:-0.265571 \n",
      "13 0:-1 1:0.635135 2:0.630252 3:-0.628319 4:0.265451 5:0.121049 6:0.0730744 7:-0.132038 \n",
      "11 0:-1 1:0.689189 2:0.647059 3:-0.672566 4:0.141491 5:0.00739744 6:0.0612245 7:-0.26856 \n",
      "14 0:-1 1:0.77027 2:0.781513 3:-0.60177 4:0.578537 5:0.66577 6:0.109941 7:0.119083 \n",
      "12 1:0.824324 2:0.882353 3:-0.637168 4:0.601913 5:0.102892 6:0.112574 7:0.443946 \n",
      "4 0:1 1:-0.513514 2:-0.563025 3:-0.884956 4:-0.948999 5:-0.960323 6:-0.957867 7:-0.963129 \n",
      "6 0:1 1:-0.189189 2:-0.310924 3:-0.858407 4:-0.854436 5:-0.880296 6:-0.892034 7:-0.863478 \n",
      "8 0:1 1:-0.189189 2:-0.226891 3:-0.840708 4:-0.83354 5:-0.858776 6:-0.897301 7:-0.843548 \n",
      "7 0:1 1:-0.148649 2:-0.176471 3:-0.823009 4:-0.812644 5:-0.852724 6:-0.845951 7:-0.835575 \n",
      "8 0:1 1:-0.148649 2:-0.243697 3:-0.840708 4:-0.849123 5:-0.887693 6:-0.911784 7:-0.845541 \n",
      "6 0:1 1:-0.135135 2:-0.176471 3:-0.840708 4:-0.822206 5:-0.845999 6:-0.869651 7:-0.853513 \n",
      "6 0:1 1:-0.0675676 2:-0.109244 3:-0.80531 4:-0.782539 5:-0.846671 6:-0.831468 7:-0.814649 \n",
      "7 0:1 1:-0.027027 2:-0.0588235 3:-0.814159 4:-0.751018 5:-0.791527 6:-0.869651 7:-0.776781 \n",
      "8 0:1 1:-0.027027 2:-0.092437 3:-0.814159 4:-0.764123 5:-0.818426 6:-0.830151 7:-0.773792 \n",
      "9 0:1 1:-0.0135135 2:-0.109244 3:-0.814159 4:-0.726935 5:-0.765299 6:-0.806452 7:-0.763827 \n",
      "8 0:1 1:0.0135135 2:-0.0756303 3:-0.79646 4:-0.742872 5:-0.813046 6:-0.784068 7:-0.754858 \n",
      "8 0:1 1:0.0135135 2:-0.0420168 3:-0.778761 4:-0.714893 5:-0.771352 6:-0.816985 7:-0.734928 \n",
      "9 0:1 1:0.027027 2:0.00840336 3:-0.814159 4:-0.737914 5:-0.815736 6:-0.799868 7:-0.733931 \n",
      "8 0:1 1:0.0405405 2:0.0588235 3:-0.80531 4:-0.72056 5:-0.801614 6:-0.776169 7:-0.713004 \n",
      "8 0:1 1:0.0675676 2:0.0756303 3:-0.778761 4:-0.631309 5:-0.696705 6:-0.727452 7:-0.68012 \n",
      "9 0:1 1:0.0810811 2:0.0756303 3:-0.80531 4:-0.678413 5:-0.756557 6:-0.740619 7:-0.684106 \n",
      "7 0:1 1:0.135135 2:-0.0756303 3:-0.823009 4:-0.689747 5:-0.763282 6:-0.751152 7:-0.704036 \n",
      "8 0:1 1:0.135135 2:0.0756303 3:-0.79646 4:-0.642288 5:-0.677202 6:-0.730086 7:-0.704036 \n",
      "8 0:1 1:0.148649 2:0.092437 3:-0.761062 4:-0.627059 5:-0.697377 6:-0.677419 7:-0.586447 \n",
      "7 0:1 1:0.189189 2:0.109244 3:-0.778761 4:-0.596246 5:-0.682582 6:-0.623436 7:-0.674141 \n",
      "11 0:1 1:0.202703 2:0.193277 3:-0.752212 4:-0.532141 5:-0.62811 6:-0.591837 7:-0.612357 \n",
      "11 0:1 1:0.202703 2:0.142857 3:-0.79646 4:-0.544891 5:-0.577001 6:-0.673469 7:-0.632287 \n",
      "8 0:1 1:0.216216 2:0.159664 3:-0.80531 4:-0.556933 5:-0.595831 6:-0.669519 7:-0.644245 \n",
      "9 0:1 1:0.243243 2:0.226891 3:-0.743363 4:-0.513724 5:-0.634163 6:-0.60237 7:-0.530643 \n",
      "8 0:-1 1:0.243243 2:0.193277 3:-0.787611 4:-0.517266 5:-0.581036 6:-0.565504 7:-0.686099 \n",
      "10 0:-1 1:0.256757 2:0.226891 3:-0.663717 4:-0.515849 5:-0.607263 6:-0.572087 7:-0.24564 \n",
      "10 0:1 1:0.283784 2:0.176471 3:-0.734513 4:-0.522933 5:-0.595831 6:-0.615537 7:-0.584454 \n",
      "11 0:1 1:0.283784 2:0.310924 3:-0.743363 4:-0.446786 5:-0.591796 6:-0.587887 7:-0.474838 \n",
      "9 0:-1 1:0.310811 2:0.327731 3:-0.743363 4:-0.36816 5:-0.478816 6:-0.449638 7:-0.476831 \n",
      "9 0:1 1:0.324324 2:0.294118 3:-0.761062 4:-0.457411 5:-0.556826 6:-0.636603 7:-0.509716 \n",
      "10 0:-1 1:0.337838 2:0.327731 3:-0.743363 4:-0.328493 5:-0.462677 6:-0.412772 7:-0.435974 \n",
      "10 1:0.337838 2:0.394958 3:-0.752212 4:-0.384452 5:-0.483524 6:-0.445688 7:-0.541604 \n",
      "9 0:-1 1:0.351351 2:0.394958 3:-0.734513 4:-0.308305 5:-0.395427 6:-0.485188 7:-0.452915 \n",
      "8 0:1 1:0.351351 2:0.260504 3:-0.769912 4:-0.475474 5:-0.61197 6:-0.473338 7:-0.564524 \n",
      "10 0:-1 1:0.351351 2:0.310924 3:-0.752212 4:-0.47937 5:-0.564223 6:-0.631336 7:-0.530643 \n",
      "9 0:1 1:0.351351 2:0.310924 3:-0.716814 4:-0.351514 5:-0.3961 6:-0.491771 7:-0.524664 \n",
      "10 1:0.364865 2:0.277311 3:-0.725664 4:-0.379139 5:-0.429724 6:-0.557604 7:-0.519681 \n",
      "11 0:-1 1:0.378378 2:0.327731 3:-0.690265 4:-0.202763 5:-0.338937 6:-0.311389 7:-0.335326 \n",
      "9 0:-1 1:0.391892 2:0.277311 3:-0.707965 4:-0.309722 5:-0.392737 6:-0.370639 7:-0.534629 \n",
      "8 0:1 1:0.391892 2:0.394958 3:-0.743363 4:-0.311493 5:-0.392065 6:-0.379855 7:-0.427005 \n",
      "9 0:-1 1:0.391892 2:0.176471 3:-0.734513 4:-0.397202 5:-0.562878 6:-0.312706 7:-0.514699 \n",
      "12 0:-1 1:0.405405 2:0.394958 3:-0.690265 4:-0.299451 5:-0.487559 6:-0.370639 7:-0.00647733 \n",
      "9 0:-1 1:0.405405 2:0.428571 3:-0.752212 4:-0.355056 5:-0.450572 6:-0.520737 7:-0.427005 \n",
      "9 1:0.405405 2:0.361345 3:-0.716814 4:-0.349035 5:-0.462677 6:-0.468071 7:-0.429995 \n",
      "11 1:0.418919 2:0.327731 3:-0.752212 4:-0.385869 5:-0.540686 6:-0.487821 7:-0.423019 \n",
      "12 0:-1 1:0.418919 2:0.327731 3:-0.734513 4:-0.387639 5:-0.504371 6:-0.486504 7:-0.494768 \n",
      "11 1:0.445946 2:0.478992 3:-0.716814 4:-0.230034 5:-0.370545 6:-0.479921 7:-0.237668 \n",
      "11 0:-1 1:0.459459 2:0.445378 3:-0.619469 4:-0.320347 5:-0.433759 6:-0.537854 7:-0.425012 \n",
      "13 0:-1 1:0.459459 2:0.462185 3:-0.699115 4:-0.190367 5:-0.340282 6:-0.453588 7:-0.319382 \n",
      "11 0:1 1:0.472973 2:0.411765 3:-0.716814 4:-0.358952 5:-0.502354 6:-0.561554 7:-0.390135 \n",
      "11 1:0.486486 2:0.546218 3:-0.725664 4:-0.177262 5:-0.345662 6:-0.319289 7:-0.295466 \n",
      "11 0:-1 1:0.5 2:0.546218 3:-0.690265 4:-0.154595 5:-0.33961 6:-0.350889 7:-0.265571 \n",
      "10 0:-1 1:0.5 2:0.478992 3:-0.681416 4:-0.0734903 5:-0.335575 6:-0.22449 7:-0.0672646 \n",
      "11 1:0.513514 2:0.512605 3:-0.707965 4:-0.115282 5:-0.225286 6:-0.403555 7:-0.240658 \n",
      "9 1:0.513514 2:0.462185 3:-0.725664 4:-0.190367 5:-0.359112 6:-0.202107 7:-0.37419 \n",
      "9 0:-1 1:0.513514 2:0.495798 3:-0.681416 4:-0.183991 5:-0.408877 6:-0.391705 7:-0.231689 \n",
      "11 1:0.527027 2:0.445378 3:-0.743363 4:-0.198512 5:-0.258238 6:-0.341672 7:-0.402093 \n",
      "9 1:0.527027 2:0.495798 3:-0.734513 4:-0.150345 5:-0.249496 6:-0.393022 7:-0.33433 \n",
      "11 0:-1 1:0.554054 2:0.512605 3:-0.699115 4:0.103241 5:-0.066577 6:-0.0757077 7:-0.215745 \n",
      "12 0:-1 1:0.554054 2:0.529412 3:-0.690265 4:-0.0688861 5:-0.147949 6:-0.315339 7:-0.277529 \n",
      "11 0:-1 1:0.567568 2:0.630252 3:-0.707965 4:-0.00761466 5:-0.0659045 6:-0.373272 7:-0.165919 \n",
      "9 1:0.567568 2:0.462185 3:-0.716814 4:-0.148574 5:-0.267653 6:-0.312706 7:-0.360239 \n",
      "11 1:0.567568 2:0.344538 3:-0.699115 4:-0.0880113 5:-0.211836 6:-0.167874 7:-0.322372 \n",
      "10 1:0.581081 2:0.596639 3:-0.681416 4:0.0734903 5:0.0430397 6:-0.206057 7:-0.203787 \n",
      "10 0:-1 1:0.594595 2:0.579832 3:-0.725664 4:-0.0398442 5:-0.285138 6:-0.199473 7:-0.109118 \n",
      "11 0:-1 1:0.621622 2:0.563025 3:-0.743363 4:-0.0348858 5:-0.252186 6:-0.104674 7:-0.235675 \n",
      "11 1:0.635135 2:0.563025 3:-0.672566 4:0.0568443 5:-0.174176 6:0.0348914 7:-0.193822 \n",
      "11 1:0.635135 2:0.697479 3:-0.654867 4:0.177262 5:-0.221251 6:0.0138249 7:0.0832088 \n",
      "10 0:-1 1:0.648649 2:0.529412 3:-0.707965 4:0.0929697 5:-0.0786819 6:-0.00724161 7:-0.194818 \n",
      "11 1:0.675676 2:0.613445 3:-0.646018 4:0.121658 5:-0.104237 6:-0.121791 7:-0.064275 \n",
      "11 1:0.689189 2:0.680672 3:-0.610619 4:0.178679 5:-0.131137 6:0.127057 7:-0.0961634 \n",
      "13 0:-1 1:0.716216 2:0.697479 3:-0.690265 4:0.219763 5:-0.240081 6:0.203423 7:-0.0812157 \n",
      "14 1:0.77027 2:0.663866 3:-0.637168 4:0.350097 5:-0.273033 6:-0.0625411 7:0.18585 \n",
      "11 1:0.837838 2:0.747899 3:-0.646018 4:0.466974 5:0.361802 6:0.22449 7:-0.0463378 \n",
      "4 0:1 1:-0.594595 2:-0.613445 3:-0.911504 4:-0.964937 5:-0.97579 6:-0.969717 7:-0.969108 \n",
      "5 0:1 1:-0.581081 2:-0.613445 3:-0.911504 4:-0.961041 5:-0.966375 6:-0.967084 7:-0.971101 \n",
      "6 0:1 1:-0.513514 2:-0.563025 3:-0.893805 4:-0.935895 5:-0.94889 6:-0.946017 7:-0.953164 \n",
      "7 0:1 1:-0.243243 2:-0.277311 3:-0.867257 4:-0.856915 5:-0.591796 6:-0.880184 7:-0.884405 \n",
      "7 0:1 1:-0.0540541 2:-0.142857 3:-0.831858 4:-0.783602 5:-0.814391 6:-0.805135 7:-0.817638 \n",
      "8 0:1 1:-0.0540541 2:-0.109244 3:-0.849558 4:-0.815831 5:-0.835239 6:-0.824885 7:-0.858495 \n",
      "8 0:-1 1:0.027027 2:-0.00840336 3:-0.80531 4:-0.676997 5:-0.732347 6:-0.709019 7:-0.7429 \n",
      "10 0:-1 1:0.0405405 2:0.00840336 3:-0.752212 4:-0.653621 5:-0.722932 6:-0.698486 7:-0.656203 \n",
      "10 0:-1 1:0.135135 2:0.092437 3:-0.787611 4:-0.665663 5:-0.736382 6:-0.720869 7:-0.695067 \n",
      "11 0:-1 1:0.175676 2:0.142857 3:-0.778761 4:-0.590225 5:-0.673167 6:-0.64977 7:-0.628301 \n",
      "11 1:0.202703 2:0.260504 3:-0.734513 4:-0.485745 5:-0.595158 6:-0.58657 7:-0.534629 \n",
      "9 0:-1 1:0.216216 2:0.159664 3:-0.769912 4:-0.560829 5:-0.64425 6:-0.61422 7:-0.636273 \n",
      "9 0:-1 1:0.22973 2:0.210084 3:-0.787611 4:-0.501328 5:-0.550101 6:-0.57077 7:-0.734928 \n",
      "9 1:0.22973 2:0.142857 3:-0.79646 4:-0.598725 5:-0.666443 6:-0.639236 7:-0.682113 \n",
      "11 0:-1 1:0.27027 2:0.277311 3:-0.743363 4:-0.336639 5:-0.505716 6:-0.673469 7:-0.315396 \n",
      "8 1:0.283784 2:0.260504 3:-0.734513 4:-0.537454 5:-0.64694 6:-0.680053 7:-0.562531 \n",
      "9 0:-1 1:0.351351 2:0.428571 3:-0.734513 4:-0.330972 5:-0.415602 6:-0.315339 7:-0.503737 \n",
      "10 0:-1 1:0.364865 2:0.260504 3:-0.778761 4:-0.355764 5:-0.401479 6:-0.454905 7:-0.761834 \n",
      "11 0:-1 1:0.405405 2:0.344538 3:-0.743363 4:-0.33416 5:-0.422999 6:-0.522054 7:-0.450922 \n",
      "8 0:-1 1:0.418919 2:0.378151 3:-0.681416 4:-0.156366 5:-0.309348 6:-0.171824 7:-0.394121 \n",
      "11 0:-1 1:0.540541 2:0.495798 3:-0.681416 4:0.0334691 5:-0.196369 6:-0.362739 7:-0.128052 \n",
      "10 0:-1 1:0.581081 2:0.579832 3:-0.646018 4:0.0533026 5:-0.185609 6:-0.00197498 7:-0.163926 \n",
      "5 0:1 1:-0.418919 2:-0.462185 3:-0.893805 4:-0.922437 5:-0.930061 6:-0.9526 7:-0.939213 \n",
      "5 0:1 1:-0.391892 2:-0.445378 3:-0.884956 4:-0.913937 5:-0.921991 6:-0.932851 7:-0.940209 \n",
      "5 0:1 1:-0.202703 2:-0.260504 3:-0.823009 4:-0.802019 5:-0.798924 6:-0.868334 7:-0.867464 \n",
      "7 0:1 1:-0.189189 2:-0.226891 3:-0.858407 4:-0.841332 5:-0.870208 6:-0.895984 7:-0.858495 \n",
      "7 0:1 1:-0.175676 2:-0.210084 3:-0.849558 4:-0.839561 5:-0.882986 6:-0.878868 7:-0.853513 \n",
      "8 0:1 1:-0.135135 2:-0.176471 3:-0.787611 4:-0.789269 5:-0.831204 6:-0.822251 7:-0.824614 \n",
      "8 0:1 1:-0.0945946 2:-0.092437 3:-0.814159 4:-0.745706 5:-0.785474 6:-0.826201 7:-0.797708 \n",
      "7 0:1 1:-0.0810811 2:-0.109244 3:-0.79646 4:-0.785727 5:-0.837929 6:-0.807768 7:-0.815645 \n",
      "7 0:1 1:-0.0540541 2:-0.092437 3:-0.814159 4:-0.719851 5:-0.757229 6:-0.788018 7:-0.768809 \n",
      "7 0:1 1:-0.0135135 2:-0.0420168 3:-0.823009 4:-0.732956 5:-0.769334 6:-0.786702 7:-0.801694 \n",
      "6 0:1 1:-0.0135135 2:-0.0420168 3:-0.787611 4:-0.6476 5:-0.602555 6:-0.752469 7:-0.766816 \n",
      "10 0:-1 1:0.0540541 2:0.176471 3:-0.761062 4:-0.450682 5:-0.414929 6:-0.549704 7:-0.713004 \n",
      "8 1:0.0675676 2:0.0252101 3:-0.823009 4:-0.668142 5:-0.781439 6:-0.766952 7:-0.726956 \n",
      "8 0:-1 1:0.175676 2:0.210084 3:-0.743363 4:-0.469453 5:-0.558171 6:-0.518104 7:-0.598406 \n",
      "10 1:0.216216 2:0.159664 3:-0.761062 4:-0.495661 5:-0.573638 6:-0.63792 7:-0.58844 \n",
      "9 1:0.216216 2:0.159664 3:-0.769912 4:-0.505932 5:-0.582381 6:-0.656353 7:-0.558545 \n",
      "7 1:0.283784 2:0.243697 3:-0.752212 4:-0.327076 5:-0.342972 6:-0.489138 7:-0.567514 \n",
      "8 0:-1 1:0.310811 2:0.226891 3:-0.734513 4:-0.381264 5:-0.409549 6:-0.483871 7:-0.541604 \n",
      "9 0:-1 1:0.351351 2:0.327731 3:-0.761062 4:-0.348681 5:-0.525219 6:-0.450955 7:-0.531639 \n",
      "10 1:0.351351 2:0.327731 3:-0.761062 4:-0.414556 5:-0.514459 6:-0.565504 7:-0.532636 \n",
      "10 0:-1 1:0.378378 2:0.361345 3:-0.734513 4:-0.147158 5:-0.219906 6:-0.432521 7:-0.359243 \n",
      "11 0:-1 1:0.459459 2:0.478992 3:-0.725664 4:-0.0901364 5:-0.416274 6:-0.229756 7:-0.356253 \n",
      "9 1:0.472973 2:0.445378 3:-0.725664 4:-0.155304 5:-0.314728 6:-0.287689 7:-0.301445 \n",
      "10 1:0.5 2:0.478992 3:-0.663717 4:-0.175846 5:-0.28043 6:-0.444371 7:-0.679123 \n",
      "10 1:0.5 2:0.462185 3:-0.699115 4:-0.140429 5:-0.379287 6:-0.462804 7:-0.384155 \n",
      "11 0:-1 1:0.608108 2:0.546218 3:-0.707965 4:-0.170179 5:-0.29388 6:-0.250823 7:-0.373194 \n",
      "11 0:-1 1:0.621622 2:0.512605 3:-0.716814 4:0.0837613 5:-0.00605245 6:-0.0612245 7:-0.242651 \n",
      "10 1:0.648649 2:0.596639 3:-0.699115 4:0.068532 5:-0.00806994 6:-0.0730744 7:-0.260588 \n"
     ]
    }
   ],
   "source": [
    "!svm-scale -s ./cached_datasets/train_range ./cached_datasets/abalone_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "812407e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/zsh: /home/xiangpan/.conda/envs/39/lib/libncursesw.so.6: no version information available (required by /bin/zsh)\n",
      "WARNING: minimal feature index is 0, but indices should start from 1\n"
     ]
    }
   ],
   "source": [
    "!svm-scale -r ./cached_datasets/train_range  ./cached_datasets/abalone_train >  ./cached_datasets/abalone_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "190777d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/zsh: /home/xiangpan/.conda/envs/39/lib/libncursesw.so.6: no version information available (required by /bin/zsh)\n",
      "WARNING: minimal feature index is 0, but indices should start from 1\n"
     ]
    }
   ],
   "source": [
    "!svm-scale -r ./cached_datasets/train_range  ./cached_datasets/abalone_test >  ./cached_datasets/abalone_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a7a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_scaled, train_x_scaled = svm_read_problem('./cached_datasets/abalone_train_scaled', return_scipy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11a46ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_scaled, test_x_scaled = svm_read_problem('./cached_datasets/abalone_test_scaled', return_scipy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a32ecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinguishing classes 1 through 9 from the rest\n",
    "train_y_scaled = (train_y_scaled<=9).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2075a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_scaled = (test_y_scaled<=9).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c46af53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_svmlight_file(X=train_x_scaled, y=train_y_scaled, f='./cached_datasets/abalone_train_scaled', zero_based=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc09e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_svmlight_file(X=test_x_scaled, y=test_y_scaled, f='./cached_datasets/abalone_test_scaled', zero_based=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7596801",
   "metadata": {},
   "source": [
    "# C3\n",
    "Consider the binary classification that consists of distinguishing classes 1 through 9 from the rest. Use SVMs combined with polynomial kernels to tackle this binary classification problem.\n",
    "\n",
    "To do that, randomly split the training data into five equal-sized disjoint sets. For each value of the polynomial degree, d = 1,2,3,4,5, plot the average cross-validation error plus or minus one standard deviation as a function of C (let other parameters of polynomial kernels in libsvm be equal to their default values), varying C in powers of 3, starting from a small value C = 3 k to C = 3 k , for some value of k. k should be chosen so that you see a significant variation in training error, starting from a very high training error to a low training error. Expect longer training times with libsvm as the value of C increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7992235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_scaled, train_x_scaled = svm_read_problem('./cached_datasets/abalone_train_scaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2b71916",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "1.6935087808430286e-05\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "Accuracy = 100% (627/627) (classification)WARNING: training data in only one class. See README for details.\n",
      "\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "5.080526342529086e-05\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "0.00015241579027587258\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Accuracy = 100% (627/627) (classification)WARNING: training data in only one class. See README for details.\n",
      "\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)WARNING: training data in only one class. See README for details.\n",
      "\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "0.0004572473708276177\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "0.0013717421124828531\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)WARNING: training data in only one class. See README for details.\n",
      "\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "0.00411522633744856\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "0.012345679012345678\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "0.037037037037037035\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "0.1111111111111111Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "0.3333333333333333\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "1.0\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Accuracy = 100% (627/627) (classification)WARNING: training data in only one class. See README for details.\n",
      "\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Accuracy = 100% (627/627) (classification)WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "3.0\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "9.0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "27.0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "81.0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "243.0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "Accuracy = 100% (627/627) (classification)WARNING: training data in only one class. See README for details.\n",
      "\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "729.0\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "Accuracy = 100% (627/627) (classification)WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "2187.0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Accuracy = 100% (627/627) (classification)WARNING: training data in only one class. See README for details.\n",
      "\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "6561.0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Accuracy = 100% (627/627) (classification)WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Total nSV = 0\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)\n",
      "Total nSV = 0\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "WARNING: training data in only one class. See README for details.\n",
      "Accuracy = 100% (627/627) (classification)Total nSV = 0\n",
      "\n",
      "19683.0\n"
     ]
    }
   ],
   "source": [
    "# train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "C = [math.pow(3,i) for i in range(-10, 10)]\n",
    "acc_res = []\n",
    "var_res = []\n",
    "\n",
    "for c in C:\n",
    "    # polynomial kernel\n",
    "    p_acc_list = []\n",
    "    p_acc_var_list = []\n",
    "    for d in range(1,6):\n",
    "        t_acc = []\n",
    "        for i in range(5):\n",
    "            # split train val\n",
    "            train_x_scaled_, val_x_scaled, train_y_scaled_, val_y_scaled = train_test_split(train_x_scaled, train_y_scaled, test_size=0.2)\n",
    "\n",
    "            m = svm_train(train_y_scaled_, train_x_scaled_, f'-t 1 -c {c} -d {d}')\n",
    "            p_label, p_acc, p_val = svm_predict(val_y_scaled, val_x_scaled, m)\n",
    "            t_acc += [p_acc[0]]\n",
    "        p_acc_list += [sum(t_acc)/5]\n",
    "        p_acc_var_list += [np.var(t_acc)]\n",
    "    acc_res = acc_res + [p_acc_list]\n",
    "    var_res = var_res + [p_acc_var_list]\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d121614",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_res = np.array(acc_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9fee8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_res = acc_res.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "354a664c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 20)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc6f7ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_res.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "c8dce6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 18)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import unravel_index\n",
    "unravel_index(res.argmax(), res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "63881a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_star_index = unravel_index(res.argmax(), res.shape)[1]\n",
    "c_star = C[unravel_index(res.argmax(), res.shape)[1]]\n",
    "\n",
    "d_star = unravel_index(res.argmax(), res.shape)[0] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "2a2fcaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6561.0 4\n"
     ]
    }
   ],
   "source": [
    "print(c_star, d_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "6a3e1e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best pair is d = 4, and C = 6561.0. (C*, d*) = (6561.0, 4)\n"
     ]
    }
   ],
   "source": [
    "print(f\"The best pair is d = {d_star}, and C = {c_star}. (C*, d*) = ({c_star}, {d_star})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "c4fbda23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(18.5, 81.77591706539074, '6561.0')"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot p_acc_list vs C for each d with subplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(len(res)):\n",
    "    # plt.figure(figsize=(20,20))\n",
    "    # plt.subplot(5,10,i+1)\n",
    "    plt.plot(res[i])\n",
    "plt.legend(['d=1', 'd=2', 'd=3', 'd=4', 'd=5'])\n",
    "\n",
    "# plt.plot(p_acc_list)\n",
    "plt.title('Cross Val Accuracy vs C')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('val_acc')\n",
    "plt.xticks([i for i in range(20)],[f\"3e{i}\" for i in range(-10, 10)], rotation=40)\n",
    "# mark max\n",
    "plt.plot(c_star_index, res.max(), 'ro')\n",
    "# max lengend\n",
    "plt.annotate(f'{c_star}', xy=(c_star_index, res.max()), xytext=(c_star_index+0.5, res.max()+0.5), arrowprops=dict(facecolor='black', shrink=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d985b11",
   "metadata": {},
   "source": [
    "# C4\n",
    "Let (C ,d ) be the best pair found previously. Fix C to be C . Plot the five-fold cross-validation error and the test errors for the hypotheses obtained as a function of d. Plot the average number of support vectors obtained as a function of d. How many of the support vectors lie on the margin hyperplanes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "517597a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_star = C[unravel_index(res.argmax(), res.shape)[1]]\n",
    "d_star = unravel_index(res.argmax(), res.shape)[0] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "2bcd1653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6561.0"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "95316c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "ec76cf3d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................................*...............*.............................................................................................................................................................................................................................................................................................................................*.......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*..............................................................................................................................................................................................................................................*....................................................................................................................................................................................................................................................................*...............................................................................................................................................................................................................................*..........................................................................................................................................................................................*.................*\n",
      "optimization finished, #iter = 2163185\n",
      "nu = 0.484128\n",
      "obj = -7958794.092537, rho = 2.322541\n",
      "nSV = 1218, nBSV = 1209\n",
      "Total nSV = 1218\n",
      "Accuracy = 77.9904% (489/627) (classification)\n",
      "Accuracy = 47.0307% (491/1044) (classification)\n",
      ".....................................................................................................*......................................*....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*...............................................................................................................................................................................*.............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*..............................................................................................................................................................................*.....................................................................................................................................................................................................................................................................................................................*...............................................................................................................................................................................................................................................................*\n",
      "optimization finished, #iter = 2170475\n",
      "nu = 0.468519\n",
      "obj = -7701834.807021, rho = -2.202916\n",
      "nSV = 1180, nBSV = 1171\n",
      "Total nSV = 1180\n",
      "Accuracy = 77.3525% (485/627) (classification)\n",
      "Accuracy = 46.7433% (488/1044) (classification)\n",
      ".................................................................................................................*..................................................................................................................*...........................................................................................................................................................................................................................................................*.......................................*...................................................................................................................................................................................................................................................................................................................................................*.............................................................................................................................................................................................................................................*..........................................................................................................................*...................................................................................................................................................................................................................................................................................................................................................................................................*............................................*.................................*\n",
      "optimization finished, #iter = 1676014\n",
      "nu = 0.488480\n",
      "obj = -8029973.076594, rho = -2.691439\n",
      "nSV = 1230, nBSV = 1220\n",
      "Total nSV = 1230\n",
      "Accuracy = 79.9043% (501/627) (classification)\n",
      "Accuracy = 47.8927% (500/1044) (classification)\n",
      "........................................................................................................................*........................*...............................................................................................................................................................*...........................................................................................................................................................................................................................*..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*..................................................................................................................................................*................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*........................................................................................................................*.............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*.................................................................................................................................................................*..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*\n",
      "optimization finished, #iter = 4138231\n",
      "nu = 0.485065\n",
      "obj = -7973995.735803, rho = -2.823633\n",
      "nSV = 1221, nBSV = 1212\n",
      "Total nSV = 1221\n",
      "Accuracy = 79.9043% (501/627) (classification)\n",
      "Accuracy = 47.4138% (495/1044) (classification)\n",
      ".............................................................................................................................*.............*..............................................................................................................................................*...............................................................................................................................................................................................................................................................................................................................................................................................................................................*..................................................................................................................................................................................................................................................................................................................................................................*..................................................................................................................................................................................................................................................................................................*...................................................................................................................................................................................*.........................................................................................................................................................................................................................................................................*.*\n",
      "optimization finished, #iter = 1797818\n",
      "nu = 0.477359\n",
      "obj = -7847326.867559, rho = -2.031478\n",
      "nSV = 1205, nBSV = 1193\n",
      "Total nSV = 1205\n",
      "Accuracy = 78.7879% (494/627) (classification)\n",
      "Accuracy = 46.4559% (485/1044) (classification)\n",
      "..............................................................................................................................................................................................................*............................................................................................*..........................................................................................................................................................................................................................................................................................................*.....................................................................................................................................................*\n",
      "optimization finished, #iter = 745627\n",
      "nu = 0.455088\n",
      "obj = -7460744.339813, rho = -2.152577\n",
      "nSV = 1154, nBSV = 1124\n",
      "Total nSV = 1154\n",
      "Accuracy = 79.9043% (501/627) (classification)\n",
      "Accuracy = 52.682% (550/1044) (classification)\n",
      "....................................................................................................................*............................................................................................................................*........................................................................................................................................................................................................................................................*...................................................................................................................................................................*...........................................................................................................................................................................................................................*\n",
      "optimization finished, #iter = 868499\n",
      "nu = 0.455436\n",
      "obj = -7472824.393846, rho = -2.007082\n",
      "nSV = 1158, nBSV = 1125\n",
      "Total nSV = 1158\n",
      "Accuracy = 79.4258% (498/627) (classification)\n",
      "Accuracy = 55.1724% (576/1044) (classification)\n",
      "...............................................................................................................................................................................*...........................................................................................*...............................................................................................................................................................................................................................*............................................................................................................................................................................*.....................................................................................*...............*\n",
      "optimization finished, #iter = 759235\n",
      "nu = 0.455339\n",
      "obj = -7469004.280747, rho = -1.664525\n",
      "nSV = 1155, nBSV = 1125\n",
      "Total nSV = 1155\n",
      "Accuracy = 79.5853% (499/627) (classification)\n",
      "Accuracy = 47.2222% (493/1044) (classification)\n",
      "......................................................................................................................................*............................................................................................................................................*.............................................................................................................................................................*......................................................................................................................................*.........................................................................................................................................................................................................................*......*\n",
      "optimization finished, #iter = 785614\n",
      "nu = 0.456734\n",
      "obj = -7498262.019319, rho = 1.674396\n",
      "nSV = 1163, nBSV = 1130\n",
      "Total nSV = 1163\n",
      "Accuracy = 79.9043% (501/627) (classification)\n",
      "Accuracy = 47.4138% (495/1044) (classification)\n",
      "................................................................................................................................*...........................................................................................*............................................................................................................................................................................................................................................................................*.....................................................................................................................................................................................*..*\n",
      "optimization finished, #iter = 668547\n",
      "nu = 0.450260\n",
      "obj = -7391429.811747, rho = 2.464762\n",
      "nSV = 1144, nBSV = 1115\n",
      "Total nSV = 1144\n",
      "Accuracy = 80.0638% (502/627) (classification)\n",
      "Accuracy = 47.1264% (492/1044) (classification)\n",
      ".........................................................................................*................................................................*.....................................................................................................................................................*.....................................*\n",
      "optimization finished, #iter = 339381\n",
      "nu = 0.448065\n",
      "obj = -7308630.352781, rho = 1.245178\n",
      "nSV = 1160, nBSV = 1099\n",
      "Total nSV = 1160\n",
      "Accuracy = 83.2536% (522/627) (classification)\n",
      "Accuracy = 53.5441% (559/1044) (classification)\n",
      "........................................................................*....................................................................................*.................................................................................*\n",
      "optimization finished, #iter = 237832\n",
      "nu = 0.449681\n",
      "obj = -7334665.343458, rho = -1.452144\n",
      "nSV = 1158, nBSV = 1100\n",
      "Total nSV = 1158\n",
      "Accuracy = 81.3397% (510/627) (classification)\n",
      "Accuracy = 54.023% (564/1044) (classification)\n",
      ".......................................................................................*..............................................................*....................................................................................*................................................................................................................................................................*\n",
      "optimization finished, #iter = 391781\n",
      "nu = 0.447430\n",
      "obj = -7299226.182262, rho = -1.387576\n",
      "nSV = 1152, nBSV = 1097\n",
      "Total nSV = 1152\n",
      "Accuracy = 81.6587% (512/627) (classification)\n",
      "Accuracy = 55.8429% (583/1044) (classification)\n",
      "............................................................................*.........................................................*......................................................................*.................*\n",
      "optimization finished, #iter = 220083\n",
      "nu = 0.449632\n",
      "obj = -7330316.103636, rho = -1.510979\n",
      "nSV = 1156, nBSV = 1100\n",
      "Total nSV = 1156\n",
      "Accuracy = 82.2967% (516/627) (classification)\n",
      "Accuracy = 57.1839% (597/1044) (classification)\n",
      "..................................................................................*.................................................*..............................................................................................................................................................................................*.................*\n",
      "optimization finished, #iter = 338079\n",
      "nu = 0.451302\n",
      "obj = -7357661.883221, rho = 1.219289\n",
      "nSV = 1165, nBSV = 1107\n",
      "Total nSV = 1165\n",
      "Accuracy = 81.8182% (513/627) (classification)\n",
      "Accuracy = 53.8314% (562/1044) (classification)\n",
      ".............................................*.......................*.........*\n",
      "optimization finished, #iter = 76705\n",
      "nu = 0.435963\n",
      "obj = -7081757.190962, rho = 1.081632\n",
      "nSV = 1130, nBSV = 1063\n",
      "Total nSV = 1130\n",
      "Accuracy = 78.9474% (495/627) (classification)\n",
      "Accuracy = 68.1034% (711/1044) (classification)\n",
      ".................................................................................*.................*\n",
      "optimization finished, #iter = 98316\n",
      "nu = 0.448292\n",
      "obj = -7268496.708627, rho = -1.181806\n",
      "nSV = 1163, nBSV = 1093\n",
      "Total nSV = 1163\n",
      "Accuracy = 80.0638% (502/627) (classification)\n",
      "Accuracy = 65.0383% (679/1044) (classification)\n",
      "...................................................*.......................*............*\n",
      "optimization finished, #iter = 86331\n",
      "nu = 0.443154\n",
      "obj = -7196415.125945, rho = -1.432296\n",
      "nSV = 1144, nBSV = 1080\n",
      "Total nSV = 1144\n",
      "Accuracy = 81.1802% (509/627) (classification)\n",
      "Accuracy = 67.0498% (700/1044) (classification)\n",
      "...................................*.................................*..................................*.......*\n",
      "optimization finished, #iter = 108384\n",
      "nu = 0.461955\n",
      "obj = -7493977.717432, rho = 0.910608\n",
      "nSV = 1191, nBSV = 1123\n",
      "Total nSV = 1191\n",
      "Accuracy = 83.5726% (524/627) (classification)\n",
      "Accuracy = 66.954% (699/1044) (classification)\n",
      ".........................................*..........................................................................*........................................................................*............*\n",
      "optimization finished, #iter = 198877\n",
      "nu = 0.436011\n",
      "obj = -7072046.029330, rho = 1.415734\n",
      "nSV = 1130, nBSV = 1059\n",
      "Total nSV = 1130\n",
      "Accuracy = 78.9474% (495/627) (classification)\n",
      "Accuracy = 63.2184% (660/1044) (classification)\n",
      "......................*..................*...*\n",
      "optimization finished, #iter = 43269\n",
      "nu = 0.445773\n",
      "obj = -7196248.984121, rho = 1.107619\n",
      "nSV = 1156, nBSV = 1085\n",
      "Total nSV = 1156\n",
      "Accuracy = 80.3828% (504/627) (classification)\n",
      "Accuracy = 70.8812% (740/1044) (classification)\n",
      ".............................*.................*...............*\n",
      "optimization finished, #iter = 60609\n",
      "nu = 0.443112\n",
      "obj = -7160889.799821, rho = 1.137402\n",
      "nSV = 1157, nBSV = 1085\n",
      "Total nSV = 1157\n",
      "Accuracy = 80.0638% (502/627) (classification)\n",
      "Accuracy = 72.7011% (759/1044) (classification)\n",
      "...............................*..............................*..*\n",
      "optimization finished, #iter = 63595\n",
      "nu = 0.442180\n",
      "obj = -7138262.836732, rho = 1.067685\n",
      "nSV = 1146, nBSV = 1078\n",
      "Total nSV = 1146\n",
      "Accuracy = 81.0207% (508/627) (classification)\n",
      "Accuracy = 71.8391% (750/1044) (classification)\n",
      ".........................................*.....................*..................................*\n",
      "optimization finished, #iter = 96230\n",
      "nu = 0.443952\n",
      "obj = -7161690.303867, rho = 1.070800\n",
      "nSV = 1152, nBSV = 1079\n",
      "Total nSV = 1152\n",
      "Accuracy = 80.7018% (506/627) (classification)\n",
      "Accuracy = 70.7854% (739/1044) (classification)\n",
      "..............................*.......................*\n",
      "optimization finished, #iter = 53245\n",
      "nu = 0.445754\n",
      "obj = -7210014.581831, rho = 1.183098\n",
      "nSV = 1164, nBSV = 1087\n",
      "Total nSV = 1164\n",
      "Accuracy = 80.3828% (504/627) (classification)\n",
      "Accuracy = 72.0307% (752/1044) (classification)\n"
     ]
    }
   ],
   "source": [
    "# list save result\n",
    "\n",
    "val_acc_res = []\n",
    "test_acc_res = []\n",
    "sv_counts_res = []\n",
    "for d in range(1,6):\n",
    "    val_acc_list = []\n",
    "    test_acc_list = []\n",
    "    sv_counts_list = []\n",
    "    m_list = []\n",
    "    for fold in range(5):\n",
    "        # split train val\n",
    "        train_x_scaled_, val_x_scaled, train_y_scaled_, val_y_scaled = train_test_split(train_x_scaled, train_y_scaled, test_size=0.2)\n",
    "\n",
    "        m = svm_train(train_y_scaled_, train_x_scaled_, f'-t 1 -c {c_star} -d {d}')\n",
    "        sv_len = m.get_sv_indices()\n",
    "        sv_counts_list += [len(sv_len)]\n",
    "        p_label, p_acc, p_val = svm_predict(val_y_scaled, val_x_scaled, m)\n",
    "        val_acc_list += [p_acc[0]]\n",
    "        p_label, p_acc, p_val = svm_predict(test_y_scaled, test_x_scaled, m)\n",
    "        test_acc_list += [p_acc[0]]\n",
    "        # get sup vec counts\n",
    "        \n",
    "    val_acc_res += [sum(val_acc_list)/5]\n",
    "    test_acc_res += [sum(test_acc_list)/5]\n",
    "    sv_counts_res += [sum(sv_counts_list)/5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "7d96eea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[78.78787878787878,\n",
       " 79.77671451355661,\n",
       " 82.07336523125997,\n",
       " 80.54226475279106,\n",
       " 80.5103668261563]"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "fb475d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x7f1809c3c5e0>,\n",
       "  <matplotlib.axis.XTick at 0x7f1809c3c5b0>,\n",
       "  <matplotlib.axis.XTick at 0x7f1809c3e1f0>,\n",
       "  <matplotlib.axis.XTick at 0x7f1809b93a60>,\n",
       "  <matplotlib.axis.XTick at 0x7f1809b981f0>],\n",
       " [Text(0, 0, '1'),\n",
       "  Text(1, 0, '2'),\n",
       "  Text(2, 0, '3'),\n",
       "  Text(3, 0, '4'),\n",
       "  Text(4, 0, '5')])"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot val_acc as function of d\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(val_acc_res)\n",
    "plt.legend(['d=1', 'd=2', 'd=3', 'd=4', 'd=5'])\n",
    "plt.title('Cross Val Accuracy vs d')\n",
    "plt.xlabel('d')\n",
    "plt.ylabel('val_acc')\n",
    "plt.xticks([i for i in range(5)],[f\"{i}\" for i in range(1,6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "49ad5aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x7f1809bad640>,\n",
       "  <matplotlib.axis.XTick at 0x7f1809bad760>,\n",
       "  <matplotlib.axis.XTick at 0x7f1809babf70>,\n",
       "  <matplotlib.axis.XTick at 0x7f1809b3fdc0>,\n",
       "  <matplotlib.axis.XTick at 0x7f1809b42550>],\n",
       " [Text(0, 0, '1'),\n",
       "  Text(1, 0, '2'),\n",
       "  Text(2, 0, '3'),\n",
       "  Text(3, 0, '4'),\n",
       "  Text(4, 0, '5')])"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot val error as function of d\n",
    "import matplotlib.pyplot as plt\n",
    "val_acc_res = np.array(val_acc_res)\n",
    "plt.plot(100-val_acc_res)\n",
    "plt.legend(['d=1', 'd=2', 'd=3', 'd=4', 'd=5'])\n",
    "plt.title('Cross Val Error vs d')\n",
    "plt.xlabel('d')\n",
    "plt.ylabel('val_error')\n",
    "plt.xticks([i for i in range(5)],[f\"{i}\" for i in range(1,6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "ae172434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x7f1809b59a00>,\n",
       "  <matplotlib.axis.XTick at 0x7f1809b598e0>,\n",
       "  <matplotlib.axis.XTick at 0x7f1809ba1940>,\n",
       "  <matplotlib.axis.XTick at 0x7f1809afb070>,\n",
       "  <matplotlib.axis.XTick at 0x7f1809afb6a0>],\n",
       " [Text(0, 0, '1'),\n",
       "  Text(1, 0, '2'),\n",
       "  Text(2, 0, '3'),\n",
       "  Text(3, 0, '4'),\n",
       "  Text(4, 0, '5')])"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot val_acc as function of d\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(test_acc_res)\n",
    "plt.legend(['d=1', 'd=2', 'd=3', 'd=4', 'd=5'])\n",
    "plt.title('Test Accuracy vs d')\n",
    "plt.xlabel('d')\n",
    "plt.ylabel('test_acc')\n",
    "plt.xticks([i for i in range(5)],[f\"{i}\" for i in range(1,6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "33bcd184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x7f1809b14730>,\n",
       "  <matplotlib.axis.XTick at 0x7f1809b14a30>,\n",
       "  <matplotlib.axis.XTick at 0x7f1809cad9d0>,\n",
       "  <matplotlib.axis.XTick at 0x7f1809ac75b0>,\n",
       "  <matplotlib.axis.XTick at 0x7f1809ac7d00>],\n",
       " [Text(0, 0, '1'),\n",
       "  Text(1, 0, '2'),\n",
       "  Text(2, 0, '3'),\n",
       "  Text(3, 0, '4'),\n",
       "  Text(4, 0, '5')])"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot test error as function of d\n",
    "import matplotlib.pyplot as plt\n",
    "test_acc_res = np.array(test_acc_res)\n",
    "plt.plot(100-test_acc_res)\n",
    "plt.legend(['d=1', 'd=2', 'd=3', 'd=4', 'd=5'])\n",
    "plt.title('Test Error vs d')\n",
    "plt.xlabel('d')\n",
    "plt.ylabel('test_error')\n",
    "plt.xticks([i for i in range(5)],[f\"{i}\" for i in range(1,6)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e00682c",
   "metadata": {},
   "source": [
    "## Support Vector Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "491d3e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x7f1809aec610>,\n",
       "  <matplotlib.axis.XTick at 0x7f1809aec640>,\n",
       "  <matplotlib.axis.XTick at 0x7f1809bb2c10>,\n",
       "  <matplotlib.axis.XTick at 0x7f1809a99b20>,\n",
       "  <matplotlib.axis.XTick at 0x7f1809aa12b0>],\n",
       " [Text(0, 0, '1'),\n",
       "  Text(1, 0, '2'),\n",
       "  Text(2, 0, '3'),\n",
       "  Text(3, 0, '4'),\n",
       "  Text(4, 0, '5')])"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAtLElEQVR4nO3deXxc9Xnv8c+j3ZIsb5K8yZJtsGV2Y8uWCWAwkIQ0QICUABahabh1w21vm5uU5uambZK2uU1v0r5abtq0lBIgGJMAIQQIAcoSQ+IF2RhjsA3eLdugxZssWdb23D/OkT2WR/bI1uiMpO/79TovzfzOmTPPHMM881vO72fujoiIyKmkRR2AiIgMDEoYIiKSECUMERFJiBKGiIgkRAlDREQSooQhIiIJUcIQkZRiZm5mZ0cdh5xICUP6nJldZma/NbMDZrbXzH5jZnOijqs7M7vSzGpOsv/rZrY0TnmhmbWa2fl9GMt0M3vczOrD67bWzL5iZul99R49vO+DZva3yXwPGTyUMKRPmVkB8Czw/4DRwETg28CRKOPqzswyEjjsx8DHzGxKt/LbgHfcfV0fxXIWsALYCVzg7iOAW4AKYHhfvIdIn3B3bdr6bCP4ktt/kv3fAh6JeT4ZcCAjfP4a8HfASuAA8DQwutuxi4DdwB7gqzHnygb+Kdy3O3ycHe67EqgBvgZ8CDwOHAY6gUPhNiFOvC8Cf9WtbCXwJ+Hj64A1wH7gt8CFMcdNAn4G1AENwA96uCaPAM+d4rreALwbvs9rwDkx+xw4O+b5g8DfdvvcXwVqw2v2++G+RUAb0Bp+/mfC8q8Bu4BGYCNwdZx45oXXMT2m7CZgbfh4LlANHAQ+Av7xJJ/tnjCu3cAXu38ebamzqYYhfe19oMPMHjKzT5nZqNM4x50EXxwTgHbg3m77FwDTgE8A/8vMrgnLv0HwRTYTuIjgS+svYl43jqDWUxa+x6eA3e6eH26748TyEPD5ridmVh6ef4mZzQIeAP4QGAP8O/ALM8sOm5KeBbYTJLqJwGM9fN5rgCd6uhhmNh1YAnwZKAJ+CTxjZlk9vaabccCIMIa7gH8xs1Hufh+wGPi/4ee/Pvx8fwzMcffhwCeBbd1P6O7LgSbgqpjihcCj4eN/Bv7Z3QuAs4Cf9vDZrgX+DPg4wb/pNfGOk9SghCF9yt0PApcR/Er8D6DOzH5hZmN7cZofu/s6d28C/hL4XLe2/G+7e5O7vwP8CLg9LK8C/trda929jqAp7PMxr+sEvunuR9z9cIKxPAWMNbOPhc/vBJ4Pz/8HwL+7+wp373D3hwia3uYRJKsJwD1hrC3u/kYP7zGG4Bd2T24lqIG85O5twPeBYcDHTvKaWG0E16XN3X9JUJso7+HYDoKa2rlmlunu29x9cw/HLiG89mY2HPidsKzrPc82s0J3PxQmmHg+B/wo5t/7Wwl+JomAEob0OXdf7+5fcPcS4HyCL85/6sUpdsY83g5kAoUn2T8hfDwhfB5vH0Cdu7f0Ig7cvZmg+epOMzOCpPRQuLsM+KqZ7e/aCJqhJoR/t7t7ewJv0wCMP8n+4z6Xu3cSXIOJCX6Mhm5xNAP58Q50900ENZlvAbVm9piZTYh3LEFt4mYzywZuBla7e1ecdwHTgQ1m9qaZXdfDOSZw4r+npCglDEkqd99A0KbeNaKoCciNOWRcnJdNinlcSvBrtf4k+7uaknYTfInH2wdBrYeTPO/JQwS/hD9O0An9bFi+E/iOu4+M2XLdfUm4rzTBzvX/Aj57kv3Hfa4wcU0i6GeAIAGc6pr25IRr4O6Puvtl4Xs68PdxX+j+HsEX/Kc4vjkKd//A3W8HisPXP2FmeXFOs4cT/z0lRSlhSJ8ysxlm9lUzKwmfTyJotuhqklgDzDezUjMbAXw9zmnuMLNzzSwX+GvgCXfviNn/l2aWa2bnAb8P/CQsXwL8hZkVmVkh8FcEHco9+QgYE8ZxMq8TdDbfBzzm7q1h+X8AXzKzSgvkmdmnw+aZlQRfht8Ny3PM7NIezv9NgtFY3zOzcQBmdraZPWJmIwna/z9tZlebWSZBB/YRgk52CK7pQjNLD/sErjjF5+l+DaZ2PTGzcjO7Kqw1tBAMDOjo6cUESeJPgPkENbGu89xhZkVhbWh/WBzvPD8FvhDz7/3NXsQu/UwJQ/paI1AJrDCzJoJEsY7gSw53f4ngC34tsIpjv9Zj/ZigVvIhkEPwhRTr18Am4GXg++7+Ylj+twQjc9YC7wCrw7K4wtrPEmBL2KQUt+nF3R14mOAX98Mx5dUE/Rg/APaFMX0h3NcBXA+cDewgGKl0aw/n3wxcQtA5/q6ZHQCeDD9Lo7tvBO4gGKpcH573+pjE9adh2X6CJrOf9/SZ4/hPgv6K/Wb2c4L+i++G7/MhQQ3hf5/k9UsIRmK94u6xtcBrw89yiKAD/LZ4zYHu/jxBc+UrBNfvlV7ELv3Mgv8XRFKDmb1GMOz2/jj7JgNbgcwE+wZEpA+phiEiIglRwhARkYSoSUpERBKiGoaIiCQkkTHiA1JhYaFPnjw56jBERAaUVatW1bt7Ubx9gzZhTJ48merq6qjDEBEZUMysx7vt1SQlIiIJUcIQEZGEKGGIiEhCBm0fhojImWhra6OmpoaWll5NcDxg5OTkUFJSQmZmZsKvUcIQEYmjpqaG4cOHM3nyZIIJggcPd6ehoYGamhqmTOm+AnHP1CQlIhJHS0sLY8aMGXTJAsDMGDNmTK9rT0oYIiI9GIzJosvpfDYljG7W7TrA//nlejRliojI8ZKaMMzsATOrNbN1MWXfM7MNZrbWzJ4KF4jBzMaY2atmdsjMftDtPLPN7B0z22Rm91oS0/57uw9y39ItvLltX7LeQkSk1771rW/x/e9//5THNTQ0sGDBAvLz8/njP/7jPo0h2TWMBwkWUon1EnC+u18IvM+xFddagL8E/izOeX4ILAKmhVv3c/aZ6y4az/CcDBav0NLCIjLw5OTk8Dd/8zcJJZfeSmrCcPelwN5uZS/GLH6zHCgJy5vc/Q2CxHGUmY0HCtx9WczKZzcmK+bcrAw+O6uE59/5kIZDR5L1NiIip/Sd73yH8vJyrrnmGjZu3JjQa/Ly8rjsssvIycnp83iiHlb7RY6tx9yTiQTLW3apCctOYGaLCGoilJae/lryVZWlPPjbbTy+qoYvXXHWaZ9HRAaHbz/zLu/tPtin5zx3QgHfvP68HvevWrWKxx57jLfeeov29nZmzZrF7Nmz+d73vsfixYtPOH7+/Pnce++9fRpjd5ElDDP7BtAOnPjJux0apyxuj7S73wfcB1BRUXHavdbTxg5n7pTRPLpiB4sun0pa2uAdKSEiqen111/npptuIjc3F4AbbrgBgHvuuYd77rknkpgiSRhm9nvAdcDVfurhSDWEzVahEmB3smLrUlVZyp8+toY3NtUzf3rcmX5FZIg4WU0gmeKN74myhtHvw2rN7Frga8AN7t58quPdfQ/QaGbzwtFRdwJPJzlMrj1/HKPzstT5LSKRmD9/Pk899RSHDx+msbGRZ555BghqGGvWrDlhS3aygCTXMMxsCXAlUGhmNcA3CUZFZQMvhdlzubt/KTx+G1AAZJnZjcAn3P094G6CEVfDgOfDLamyM9K5paKE+1/fyocHWhg3ou87kEREejJr1ixuvfVWZs6cSVlZGZdffnnCr508eTIHDx6ktbWVn//857z44ouce+65ZxzToF3Tu6Kiws90AaXtDU1c8b3X+PI10/jyNdP7KDIRGQjWr1/POeecE3UYSRXvM5rZKneviHe87vQ+ibIxecyfXsRjK3fS3tEZdTgiIpFSwjiFqspSPjzYwisbaqMORUQkUkoYp3D1jGLGFmSzeMWOqEMRkX42WJvs4fQ+mxLGKWSkp3HbnFKWflDHjoZTDuoSkUEiJyeHhoaGQZk0utbD6O3d4FHf6T0g3DZ3Ev/vlQ9Y8uYOvnbtjKjDEZF+UFJSQk1NDXV1dVGHkhRdK+71hhJGAsaPGMbV54zlp2/u5MvXTCM7Iz3qkEQkyTIzM3u1Gt1QoCapBN0xr4yGplZeePejqEMREYmEEkaCLj+7kEmjh7F4ue78FpGhSQkjQWlpxsK5ZazYupdNtY1RhyMi0u+UMHrhlooSMtNNQ2xFZEhSwuiFwvxsrj1/PE+uquFwa0fU4YiI9CsljF6qqizlYEs7z6xN+gzrIiIpRQmjlyqnjObs4nw1S4nIkKOE0UtmRlVlKW/v3M+6XQeiDkdEpN8oYZyGmy8uISczTbUMERlSlDBOw4jcTK6/cAJPr9lFY0tb1OGIiPQLJYzTVDWvjObWDn6+Rp3fIjI0KGGcpotKRnDehAIWL98+KGezFBHpTgnjNJkZd8wrY8OHjazesS/qcEREkk4J4wzccNEE8rMzWLxcnd8iMvgpYZyBvOwMbrp4Is++s4d9Ta1RhyMiklRKGGdoYWUpre2dPLm6JupQRESSSgnjDJ0zvoDZZaNYvGKHOr9FZFBTwugDVZWlbK1v4rebG6IORUQkaZKaMMzsATOrNbN1MWXfM7MNZrbWzJ4ys5Ex+75uZpvMbKOZfTKmfLaZvRPuu9fMLJlx99bvXDCekbmZLF6hxZVEZPBKdg3jQeDabmUvAee7+4XA+8DXAczsXOA24LzwNf9qZl2LZ/8QWARMC7fu54xUTmY6t8wu4cV3P6L2YEvU4YiIJEVSE4a7LwX2dit70d3bw6fLgZLw8WeAx9z9iLtvBTYBc81sPFDg7ss86CR4GLgxmXGfjtvnltLe6fy0emfUoYiIJEXUfRhfBJ4PH08EYr9ta8KyieHj7uUnMLNFZlZtZtV1dXVJCLdnU4vyufTsMSxZuZOOTnV+i8jgE1nCMLNvAO3A4q6iOIf5ScpPLHS/z90r3L2iqKiobwLtharKMnbtP8yv36/t9/cWEUm2SBKGmf0ecB1Q5cfGotYAk2IOKwF2h+UlccpTzsfPHUvR8Gwe0Z3fIjII9XvCMLNrga8BN7h7c8yuXwC3mVm2mU0h6Nxe6e57gEYzmxeOjroTeLq/405EZnoat82ZxKsba6nZ13zqF4iIDCDJHla7BFgGlJtZjZndBfwAGA68ZGZrzOzfANz9XeCnwHvAr4A/cveO8FR3A/cTdIRv5li/R8q5bW4pBjy2Up3fIjK42GC9O7miosKrq6sjee+7HnyTt2sOsOzrV5GZHvW4AhGRxJnZKneviLdP32ZJUDWvlPpDR3jpvY+iDkVEpM8oYSTBFdOLmThymO78FpFBRQkjCdLTjNvnTuI3mxrYUnco6nBERPqEEkaSfG7OJDLSjEdXaIitiAwOShhJUjw8h0+eN44nVtfQ0tZx6heIiKQ4JYwkqqosZX9zG798Z0/UoYiInDEljCS65KwxTC3MY7GapURkEFDCSCIzY2FlKau272P9noNRhyMickaUMJLss7NKyMpI0xBbERnwlDCSbFReFtddOJ6nVu/i0JH2U79ARCRFKWH0g6rKMppaO/jFmpScZFdEJCFKGP1gVulIZowbzuIV2xmsc3eJyOCnhNEPzIyqeWW8u/sgb9cciDocEZHTooTRT26cOYHcrHQeWa7ObxEZmJQw+snwnExuvHgiz7y9mwPNbVGHIyLSa0oY/Wjh3FKOtHfy5OqaqEMREek1JYx+dP7EEcycNFKd3yIyIClh9LOqylI21zWxYuveqEMREekVJYx+dt2FEyjIydD8UiIy4Chh9LNhWel8dnYJv1q3h7rGI1GHIyKSMCWMCFRVltHW4Ty+amfUoYiIJEwJIwJnF+czb+poHl2xg85OdX6LyMCghBGRqsoyavYdZukHdVGHIiKSECWMiHzyvHGMyctS57eIDBhKGBHJykjjc3Mm8fL6j9hz4HDU4YiInFJSE4aZPWBmtWa2LqbsFjN718w6zawipjzLzH5kZu+Y2dtmdmXMvtlh+SYzu9fMLJlx95eFc0txYMlKdX6LSOpLdg3jQeDabmXrgJuBpd3K/wDA3S8APg78g5l1xfdDYBEwLdy6n3NAmjQ6lyumF/HYyh20dXRGHY6IyEklNWG4+1Jgb7ey9e6+Mc7h5wIvh8fUAvuBCjMbDxS4+zIP5tN4GLgxmXH3p6rKMmobj/Dy+tqoQxEROalU6sN4G/iMmWWY2RRgNjAJmAjEztZXE5adwMwWmVm1mVXX1Q2M0UcLyosYPyJHa36LSMpLpYTxAEEyqAb+Cfgt0A7E66+Ie/OCu9/n7hXuXlFUVJSsOPtURnoat80p5fUP6tne0BR1OCIiPUqZhOHu7e7+P919prt/BhgJfECQREpiDi0BBtXi2LfOmUR6mvHoSg2xFZHUlTIJw8xyzSwvfPxxoN3d33P3PUCjmc0LR0fdCTwdZax9bdyIHK45p5jHq2s40t4RdTgiInEle1jtEmAZUG5mNWZ2l5ndZGY1wCXAc2b2Qnh4MbDazNYDXwM+H3Oqu4H7gU3AZuD5ZMYdhTvmlbG3qZVfrfsw6lBEROLKSObJ3f32HnY9FefYbUB5D+epBs7vu8hSz6VnFVI2JpfFy3fwmZlx+/RFRCKVMk1SQ11amrFwbikrt+3l/Y8aow5HROQEShgp5Hdnl5CVnsajml9KRFLQKROGmf1fMysws0wze9nM6s3sjv4IbqgZk5/Npy4Yx5Ora2hubY86HBGR4yRSw/iEux8EriMY4joduCepUQ1hVZVlNLa088zbg2rksIgMAokkjMzw7+8AS9x978kOljMzZ/Iopo/N17TnIpJyEkkYz5jZBqACeNnMioCW5IY1dJkZVZVlrK05wNqa/VGHIyJyVCIJ45sE90xUuHsb0AzckNSohribZk1kWGa6Or9FJKUkkjCWufs+d+8AcPcmBuGNc6mkICeTGy6awNNrdnOwpS3qcEREgJMkDDMbZ2azgWFmdrGZzQq3K4Hc/gpwqKqaV8rhtg5+/tauqEMREQFOfqf3J4EvEEz2948x5Y3A/05iTAJcWDKSCyaO4JHl2/n8vDIGySKDIjKA9VjDcPeH3H0B8AV3XxCz3eDuP+vHGIesO+aV8v5Hh6jevi/qUEREEppL6lkzWwhMjj3e3f86WUFJ4PqLJvC3z65n8fLtzJk8OupwRGSIS6TT+2ngMwSLGTXFbJJkuVkZ3DxrIr9850P2NrVGHY6IDHGJ1DBK3P3apEcicS2sLOOhZdt5YtVOFs0/K+pwRGQIS6SG8VszuyDpkUhc5eOGM2fyKB5dsYPOzrgr04qI9ItEEsZlwCoz22hma83sHTNbm+zA5JiqyjK2NTTzm831UYciIkNYIk1Sn0p6FHJSn7pgHH/9bBaLl+/g8mlFUYcjIkNUIjUM72GTfpKdkc4ts0t4af1HfHRQ03iJSDQSSRjPAc+Gf18GtqCpQfrd7XNL6eh0fvLmzqhDEZEh6pQJw90vcPcLw7/TgLnAG8kPTWJNLszj8mmFLFm5g/aOzqjDEZEhqNdLtLr7amBOEmKRU6iqLGXPgRZe21gXdSgiMgSdstPbzL4S8zQNmAXoGysCV58zluLh2TyyYjvXnDs26nBEZIhJpIYxPGbLJujL+Ewyg5L4MtPTuG1uKb9+v46de5ujDkdEhphE+jC+7e7fJpix9p/dfbG7a6hORG6bMwkDlqzU4koi0r9OmTDM7HwzewtYB7xrZqvM7PxETm5mD5hZrZmtiym7xczeNbNOM6uIKc80s4fCGwPXm9nXY/bNDss3mdm9NoTn+p4wchhXzRjLT6t30tquzm8R6T+JNEndB3zF3cvcvQz4aliWiAeB7vNQrQNuBpZ2K78FyHb3C4DZwB+a2eRw3w+BRcC0cBvSc1tVzSul/lArL773YdShiMgQkkjCyHP3V7ueuPtrQF4iJ3f3pcDebmXr3X1jvMOBPDPLAIYBrcBBMxsPFLj7Mnd34GHgxkTef7CaP62IklHDWLxczVIi0n8SSRhbzOwvzWxyuP0FsDUJsTxBMG36HmAH8H133wtMBGpijqsJy05gZovMrNrMquvqBu9ArvQ0Y2FlKcu2NLCp9lDU4YjIEJFIwvgiUAT8LNwKgd9PQixzgQ5gAjAF+KqZTQXi9VfEnZrE3e9z9wp3rygqGtxzLt0yexKZ6cajK1TLEJH+kcgoqX3u/ifuPivcvuzuyVgzdCHwK3dvc/da4DdABUGNoiTmuBJgdxLef0ApGp7NJ88bxxOrdtLS1hF1OCIyBCQySuolMxsZ83yUmb2QhFh2AFdZIA+YB2xw9z1Ao5nNC0dH3UmwCuCQV1VZxsGWdp5duyfqUERkCEikSarQ3fd3PQlrF8WJnNzMlgDLgHIzqzGzu8zsJjOrAS4BnotJPv8C5BOMonoT+JG7d627cTdwP7AJ2IwmPwRg3tTRTC3KY/GK7VGHIiJDQCLrYXSaWam77wAwszISnN7c3W/vYddTcY49RDC0Nt55qoGE7v0YSsyMqsoy/ubZ93h39wHOmzAi6pBEZBBLpIbxDeANM/uxmf2Y4P6Jr5/iNdJPfndWCdkZaSxW57eIJFkind6/Iphw8CfAT4HZ7n60D8PMzkteeHIqI3Izuf6iCTz91i4OHWmPOhwRGcQSmt7c3evd/Vl3f8bduy8s/eMkxCW9UFVZSlNrBz9/a1fUoYjIINbr9TDiGLLzOqWKmZNGcu74Ahav2EFwM7yISN/ri4Shb6iImRlV80pZv+cgb+3cH3U4IjJI9UXCkBTwmZkTyctK55HlGmIrIsnRFwmjtQ/OIWcoPzuDm2ZN5Nm1e9jfrH8SEel7idzp/QszWxjefX0Cd5/X92HJ6Vg4t4zW9k6eWFVz6oNFRHopkRrGPwCXAe+Z2eNm9rtmlpPkuOQ0nDuhgFmlI3lUnd8ikgSJ3Ifxa3f/78BUgoWTPgfUJjswOT1VlWVsqW9i2ZaGqEMRkUEmoT4MMxsGfBb4EjAHeCiZQcnp+/SF4xkxLFN3fotIn0ukD+MnwHrgKuAHwFnu/j+SHZicnpzMdH53dgkvrPuQ2saWqMMRkUEkkRrGc8CF7v4lgr6MJ8zs4uSGJWdiYWUp7Z3O49Xq/BaRvpNIwvgzdz9oZpcBHydojvq35IYlZ+Ksonw+dtYYHl2xg45OdX6LSN9IJGF0Lef2aeDf3P1pICt5IUlfqKosY9f+wyx9f/CubS4i/SuRhLHLzP6dYHTUL80sO8HXSYQ+fu5YCvOztbiSiPSZRL74Pwe8AFwbrrw3GrgnmUHJmcvKSOPWOSW8sqGWXfsPRx2OiAwCidyH0ezuP3P3D8Lne9z9xeSHJmfqtjmlOPDYSg2xFZEzp6alQWzS6FwWlBfz2Js7aevojDocERnglDAGuarKUuoaj/Bf730UdSgiMsApYQxyV5YXM2FEju78FpEzpoQxyKWnGbfPLeWNTfVsrW+KOhwRGcCUMIaAW+dMIj3NWKLObxE5A0oYQ0BxQQ6fOHcsj1fvpKWt49QvEBGJI6kJw8weMLNaM1sXU3aLmb1rZp1mVhFTXmVma2K2TjObGe6bbWbvmNkmM7vXzCyZcQ9Gd8wrY19zG8+v2xN1KCIyQCW7hvEgcG23snXAzcDS2EJ3X+zuM919JvB5YJu7rwl3/xBYBEwLt+7nlFO4ZOoYphTmsXi5mqVE5PQkNWG4+1Jgb7ey9e6+8RQvvR1YAmBm44ECd1/mwTJyDwM3JiHcQS0tzVg4t5Tq7fvY8OHBqMMRkQEoVfswbiVMGMBEIHae7pqw7ARmtsjMqs2suq5Ok+5199nZJWRlpPGohtiKyGlIuYRhZpVAs7t39XvE66+IO2e3u9/n7hXuXlFUVJS0GAeq0XlZfPqC8fxs9S6ajrRHHY6IDDAplzCA2zhWu4CgRlES87wE2N2vEQ0iVZWlHDrSzi/e1iUUkd5JqYRhZmnALcBjXWXuvgdoNLN54eioO4GnIwpxwJtdNooZ44bzyPLtBF1CIiKJSfaw2iXAMqDczGrM7C4zu8nMaoBLgOfM7IWYl8wHatx9S7dT3Q3cD2wCNgPPJzPuwczMqKos5d3dB1lbcyDqcERkAMlI5snd/fYedj3Vw/GvAfPilFcD5/ddZEPbjRdP5O+e38DiFdu5aNLIqMMRkQEipZqkpH8Mz8nkMzMn8Iu3d3PgcFvU4YjIAKGEMUQtnFtGS1snT62uOfXBIiIoYQxZF5SM4KKSETyyYoc6v0UkIUoYQ1jVvDI21R5i5da9pz5YRIY8JYwh7PoLJzA8J0OLK4lIQpQwhrBhWel8dlYJz6/bQ/2hI1GHIyIpTgljiKuqLKWtw3lilTq/ReTklDCGuGljhzN3ymgeXbGDzk51fotIz5QwhKrKUnbsbeb1TfVRhyIiKUwJQ7j2/HGMycti8fLtUYciIilMCUPIzkjnlopJvLyhlj0HDkcdjoikKCUMAWDh3FI6Op2fvLkz6lBEJEUpYQgApWNymT+9iMdW7qS9ozPqcEQkBSlhyFFVlaV8eLCFVzbURh2KiKQgJQw56uoZxYwryOER3fktInEoYchRGelp3DZ3Ekvfr2NHQ3PU4YhIilHCkOPcNqeU9DTj0ZWqZYjI8ZQw5DjjRuRw9YxiHq/eyZH2jqjDEZEUooQhJ6iaV0ZDUysvvPtR1KGISApRwpATXH52IaWjc3lEd36LSAwlDDlBWpqxsLKUlVv38sFHjVGHIyIpQglD4rpldgmZ6abFlUTkKCUMiWtMfjafOn88T66u4XCrOr9FRAlDTqKqspTGlnaeWbs76lBEJAUoYUiP5k4ZzdnF+WqWEhEgyQnDzB4ws1ozWxdTdouZvWtmnWZW0e34C81sWbj/HTPLCctnh883mdm9ZmbJjFsCZkZVZSlv79zPul0Hog5HRCKW7BrGg8C13crWATcDS2MLzSwDeAT4krufB1wJtIW7fwgsAqaFW/dzSpLcPKuEnMw0Fq/QEFuRoS6pCcPdlwJ7u5Wtd/eNcQ7/BLDW3d8Oj2tw9w4zGw8UuPsyd3fgYeDGZMYtx4wYlskNF03g6TW7OdjSduoXiMiglUp9GNMBN7MXzGy1mf15WD4RqIk5riYsO4GZLTKzajOrrqurS3K4Q0dVZRnNrR08/dauqEMRkQilUsLIAC4DqsK/N5nZ1UC8/gqPdwJ3v8/dK9y9oqioKHmRDjEXlozg/IkFLF6xg6CSJyJDUSoljBrg1+5e7+7NwC+BWWF5ScxxJYDGefajoPO7jA0fNrJ6x76owxGRiKRSwngBuNDMcsMO8CuA99x9D9BoZvPC0VF3Ak9HGehQdMNFE8jPzuCR5RpiKzJUJXtY7RJgGVBuZjVmdpeZ3WRmNcAlwHNm9gKAu+8D/hF4E1gDrHb358JT3Q3cD2wCNgPPJzNuOVFedgY3z5rIc+/sYW9Ta9ThiEgEMpJ5cne/vYddT/Vw/CMEQ2u7l1cD5/dhaHIaFlaW8vCy7Ty5qoY/mD816nBSXntHJ7v2H2ZLfRNb65rYWh9sHx1sYeakkSyYUcxl0wopyMmMOlSRhCQ1YcjgMmNcARVlo3h05Q7uumwKaWm6f9LdqT/Uypa6Q0cTwpbw7/aGJto6jg0SGJ6TwdSifCaNzuWFdz/k8VU1ZKQZs8tGsWBGMVfNKGZacT66L1VSlRKG9ErVvFL+50/eZtmWBi49uzDqcPrNoSPtbAuTQWxy2FrXROOR9qPHZaWnMbkwl7OK8rjmnLFMLcxjSlEeUwvzGJ2XdTQZtHd08tbO/byyoZZXN9Ty3ec38N3nNzBx5DCuLC9iQXkxHzt7DLlZ+l9UUocN1mGSFRUVXl1dHXUYg05LWwfz/u5lPnbWGP61anbU4fSpto5OduxtPtp8FJscahuPHD3ODCaMGMbUMBFMKcxjSlE+UwvzmDByGOmnUfPac+Awr22s49UNtbyxqZ7m1g6y0tOonDqaq2YUs6C8mMmFeX35cUXiMrNV7l4Rd58ShvTWd557jx/9Zhu//V9XUVyQE3U4veLufHTwCFvqDh1tOuraduxtpqPz2P8Po/OygmRQmBeTHPIpG5NLTmZ60mI80t5B9bZ9Qe1jYy1b6poAmFKYd7T2MXfK6KTGIEOXEob0qa31TSz4/mt89ePT+R9XT4s6nLgOHG4LE8EhttQ1HdfxfLjt2PoeOZlpTCnMP1ZTCJPDlMI8RuZmRfgJjtne0BTUPjbWsmxzA0faOxmWmc6lZ49hwYxiriwvZuLIYVGHKYOEEob0uTvuX8GWukO8/rWrTqsJpi+0tHWwY28zW7qakGL6Fhpihv6mpxmTRg0LE0L+0T6FKYV5jCvIGVCd94dbO1i2pZ5XN9TxyoZadu0/DED52OFcOSOofcwuG0VmeirdYiUDiRKG9Lnn39nD3YtX85+/V8HV54xN2vt0djq79h8+rumoq29h1/7DxP7nWzQ8O6ghHK0l5DOlMI/S0blkZQy+L1B3Z3PdIV7dENQ+Vm7dS3unMzw7g8unF7KgvJgryosoHj6wmg0lWkoY0ufaOjr52Hdf4YKJI3jgC3PO6Fzuzr7mNrbWH2Jz3bHRR1vrm9ja0ERre+fRY/Oy0plalN+tbyGfyYW5DB/i9zM0trTxm031RxNIV0f9BRNHsKC8iCtnFHNRycjIaoQyMChhSFL8w4sb+cGrm3j9zxdQMir3lMc3t7azrb75xL6F+iYOHD42dXpmulE6OjfoWyiK6VsozKNoeLbuU0iAu/PenoNHR16t3rGPTg868q+YXsSV5UXMn1bEqLzU6KeR1KGEIUmxa/9hLv/7V7j7yrO455MzgOD+gpp9h2NuYDsU9i80sedAy3GvHz8iJ6aT+VjHc8moYWSoDb5P7W9uZekH9by6oZZfv1/H3qZW0gwuLh0V1D7KizlvQoGS8QDX1a83MjfztJsilTAkaf7bQ29SvX0fFWWj2Vp/iB17m4+7u7kgvLt56tEmpKA5aXJhrm5Ki0hHp7O2Zj+vbqzjtY21rK0Jlt8tHp7NgvJiFswo4tKzC4d8E1+q6uh0dnfr19scDvjo6tf7q+vO5YuXTTmt8ythSNIs39LAH/54FeMKcsIb2PJiOp7zGZWbqV+tKa6u8Qi/fj9oulr6QR2NLe1kpBlzJo9mwYwirppRzFlFmrKkP3X16x13v1BdE1vqD7Gtofm4fr387IwThoTPKh3FpNGnbiaORwlDRBLS1tHJ6u37jtY+NnzYCEDJqGFHax+XTC1kWJZuGuwLsf16XcPCE+3X65p2pii/b/v1lDBE5LTs3n+YVzfW8uqGOn6zqZ7DbR1kZaRxydQxLCgv4qoZYykdc3q/ZIeKeP16XfcOde/XmzAi52gtvatfb2pRHhNH9l+/nhKGiJyxI+0drNy6l1c3BLWPLfXBlCVTi/KC2kd5MXOmjCI7Y+jVPtydukNHjiaCroEeJ+3Xi5luJpX69ZQwRKTPbatvCmofG+tYvqWB1vZO8rLSufTswnDKkiLGjxhcU5Y0trSxrb6ZLTG1hK7tUOysxRlpTBlzfL9C12jAVO/XU8IQkaRqbm1n2eaGo81XXVOWzBg3nAXhbLuzSkcOiOHSre3hrMVx7heq6zZrccmoYccNCe9KDhNGDBtQU87EUsIQkX7j7myqPXR0tt3qbfto73QKcjK4fHoRV4VTlhTmZ0cWY2en81FjS7eJKYNO5537Dh83a3Fh/rFZi7uaj6YWBVPODMYZg5UwRCQyB1va+M0H9Uebr7p+pV9UMoIry4tZMKOYCyeOSMov8gPNbWyJuXm0q+N5W7dZi4dlph8/jX3XXGRj8hiRO7TuR1HCEJGU0NkZTFnyalj7eGvnftxhTF4WV4RrfcyfVtSrL+mWtg62NzQHzUdH71cIksPebrMWB0NTY5NCMBfZ2AJNOdNFCUNEUtLeplZe/6Du6JQl+5rbSDOYXTYqqH2UF3PO+OF0ejDEN0gIx9+v0H3W4rEF2ScMS51SmMek0bma9j0BShgikvI6Op23a/bz2oZaXtlYy7pdB4FgwsRDR9qPu7t5eHZGzMSUx9Y4mVyYR3529ENTB7KTJQxdWRFJCelpxqzSUcwqHcVXPlFO7cEWXnu/jje37mV0ftZx9ywU5mepCSkCShgikpKKC3L4XMUkPlcxKepQJJTUBj0ze8DMas1sXUzZLWb2rpl1mllFTPlkMztsZmvC7d9i9s02s3fMbJOZ3Wv6aSEi0u+S3QP0IHBtt7J1wM3A0jjHb3b3meH2pZjyHwKLgGnh1v2cIiKSZElNGO6+FNjbrWy9u29M9BxmNh4ocPdlHvTQPwzc2KeBiojIKaXaGLMpZvaWmf3azC4PyyYCNTHH1IRlJzCzRWZWbWbVdXV1yY5VRGRISaWEsQcodfeLga8Aj5pZARCvvyLuWGB3v8/dK9y9oqioKImhiogMPSkzSsrdjwBHwserzGwzMJ2gRlESc2gJsLv/IxQRGdpSpoZhZkVmlh4+nkrQub3F3fcAjWY2LxwddSfwdIShiogMSckeVrsEWAaUm1mNmd1lZjeZWQ1wCfCcmb0QHj4fWGtmbwNPAF9y964O87uB+4FNwGbg+WTGLSIiJxq0U4OYWR2w/TRfXgjU92E4g52uV+/pmvWOrlfvnMn1KnP3uJ3AgzZhnAkzq+5pLhU5ka5X7+ma9Y6uV+8k63qlTB+GiIikNiUMERFJiBJGfPdFHcAAo+vVe7pmvaPr1TtJuV7qwxARkYSohiEiIglRwhARkYQoYcSIt36H9MzMJpnZq2a2Plzj5E+jjimVmVmOma00s7fD6/XtqGMaCMwsPZyU9NmoY0l1ZrYtXDtojZn1+RrV6sOIYWbzgUPAw+5+ftTxpLpw6vnx7r7azIYDq4Ab3f29iENLSeHUNnnufsjMMoE3gD919+URh5bSzOwrQAXBMgfXRR1PKjOzbUCFuyflJkfVMGLEW79Deubue9x9dfi4EVhPD1PPC3jgUPg0M9z0i+0kzKwE+DTB1EASMSUM6RNmNhm4GFgRcSgpLWxeWQPUAi+5u67Xyf0T8OdAZ8RxDBQOvGhmq8xsUV+fXAlDzpiZ5QNPAl9294NRx5PK3L3D3WcSTNM/18zU9NkDM7sOqHX3VVHHMoBc6u6zgE8BfxQ2s/cZJQw5I2Fb/JPAYnf/WdTxDBTuvh94Da1PfzKXAjeE7fKPAVeZ2SPRhpTa3H13+LcWeAqY25fnV8KQ0xZ24v4nsN7d/zHqeFJduObLyPDxMOAaYEOkQaUwd/+6u5e4+2TgNuAVd78j4rBSlpnlhYNPMLM84BNAn474VMKIEW/9jqhjSnGXAp8n+OW3Jtx+J+qgUth44FUzWwu8SdCHoaGi0lfGAm+EawqtBJ5z91/15RtoWK2IiCRENQwREUmIEoaIiCRECUNERBKihCEiIglRwhARkYQoYYhExMy+ZWZ/FnUcIolSwhARkYQoYYj0IzP7hpltNLP/AsqjjkekNzKiDkBkqDCz2QRTXFxM8P/eaoI1REQGBCUMkf5zOfCUuzcDmNkvIo5HpFfUJCXSvzQXjwxYShgi/WcpcJOZDQtnFb0+6oBEekNNUiL9JFz7/CfAGmA78Hq0EYn0jmarFRGRhKhJSkREEqKEISIiCVHCEBGRhChhiIhIQpQwREQkIUoYIiKSECUMERFJyP8H8/v+x01QO+8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(sv_counts_res)\n",
    "plt.legend(['d=1', 'd=2', 'd=3', 'd=4', 'd=5'])\n",
    "plt.title('Support Vec Counts vs d')\n",
    "plt.xlabel('d')\n",
    "plt.ylabel('sv_counts')\n",
    "plt.xticks([i for i in range(5)],[f\"{i}\" for i in range(1,6)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb8fb3c",
   "metadata": {},
   "source": [
    "# C5\n",
    "Fix (C,d) to be (C ,d ). Plot the training and test errors as a function of the training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "e2b2d789",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_star = C[unravel_index(res.argmax(), res.shape)[1]]\n",
    "d_star = unravel_index(res.argmax(), res.shape)[0] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "f35117d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_scaled, train_x_scaled = svm_read_problem('./cached_datasets/abalone_train_scaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "f1429c30",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "drop_size: 0.9\n",
      ".....................*.............*...*\n",
      "optimization finished, #iter = 9260\n",
      "nu = 0.339069\n",
      "obj = -517634.262085, rho = -0.568996\n",
      "nSV = 100, nBSV = 76\n",
      "Total nSV = 100\n",
      "Accuracy = 71.4286% (45/63) (classification)\n",
      "Accuracy = 43.5824% (455/1044) (classification)\n",
      ".......................................*.......................................*....*\n",
      "optimization finished, #iter = 20471\n",
      "nu = 0.366815\n",
      "obj = -569021.651676, rho = -0.724279\n",
      "nSV = 110, nBSV = 78\n",
      "Total nSV = 110\n",
      "Accuracy = 76.1905% (48/63) (classification)\n",
      "Accuracy = 62.6437% (654/1044) (classification)\n",
      "......................................................*.............*\n",
      "optimization finished, #iter = 16843\n",
      "nu = 0.373257\n",
      "obj = -569142.246676, rho = 0.039270\n",
      "nSV = 111, nBSV = 81\n",
      "Total nSV = 111\n",
      "Accuracy = 80.9524% (51/63) (classification)\n",
      "Accuracy = 54.7893% (572/1044) (classification)\n",
      ".......................*...............*\n",
      "optimization finished, #iter = 9516\n",
      "nu = 0.427387\n",
      "obj = -654284.959109, rho = 0.216564\n",
      "nSV = 123, nBSV = 91\n",
      "Total nSV = 123\n",
      "Accuracy = 90.4762% (57/63) (classification)\n",
      "Accuracy = 68.3908% (714/1044) (classification)\n",
      "............................*...........*.............*\n",
      "optimization finished, #iter = 12998\n",
      "nu = 0.375451\n",
      "obj = -573126.251405, rho = 0.404532\n",
      "nSV = 107, nBSV = 79\n",
      "Total nSV = 107\n",
      "Accuracy = 84.127% (53/63) (classification)\n",
      "Accuracy = 59.4828% (621/1044) (classification)\n",
      "0.2\n",
      "drop_size: 0.8\n",
      "...............*.........*.......*\n",
      "optimization finished, #iter = 15698\n",
      "nu = 0.418575\n",
      "obj = -1323283.127628, rho = 1.373559\n",
      "nSV = 232, nBSV = 193\n",
      "Total nSV = 232\n",
      "Accuracy = 76.9841% (97/126) (classification)\n",
      "Accuracy = 65.4215% (683/1044) (classification)\n",
      ".................*...........*\n",
      "optimization finished, #iter = 14097\n",
      "nu = 0.425403\n",
      "obj = -1324194.507492, rho = -1.022950\n",
      "nSV = 240, nBSV = 194\n",
      "Total nSV = 240\n",
      "Accuracy = 78.5714% (99/126) (classification)\n",
      "Accuracy = 49.5211% (517/1044) (classification)\n",
      ".......................*......*\n",
      "optimization finished, #iter = 14992\n",
      "nu = 0.429046\n",
      "obj = -1348792.160260, rho = 1.217522\n",
      "nSV = 241, nBSV = 197\n",
      "Total nSV = 241\n",
      "Accuracy = 76.1905% (96/126) (classification)\n",
      "Accuracy = 49.2337% (514/1044) (classification)\n",
      ".........*....*......*\n",
      "optimization finished, #iter = 9208\n",
      "nu = 0.400212\n",
      "obj = -1245907.496124, rho = 1.824805\n",
      "nSV = 219, nBSV = 188\n",
      "Total nSV = 219\n",
      "Accuracy = 73.8095% (93/126) (classification)\n",
      "Accuracy = 68.9655% (720/1044) (classification)\n",
      "....................*............*\n",
      "optimization finished, #iter = 16181\n",
      "nu = 0.439046\n",
      "obj = -1374769.134138, rho = 1.452269\n",
      "nSV = 243, nBSV = 202\n",
      "Total nSV = 243\n",
      "Accuracy = 81.746% (103/126) (classification)\n",
      "Accuracy = 53.7356% (561/1044) (classification)\n",
      "0.30000000000000004\n",
      "drop_size: 0.7\n",
      "...................*.........*\n",
      "optimization finished, #iter = 21084\n",
      "nu = 0.423573\n",
      "obj = -2008433.622315, rho = 0.862424\n",
      "nSV = 343, nBSV = 300\n",
      "Total nSV = 343\n",
      "Accuracy = 77.1277% (145/188) (classification)\n",
      "Accuracy = 59.0038% (616/1044) (classification)\n",
      ".............................*.............*\n",
      "optimization finished, #iter = 31754\n",
      "nu = 0.424279\n",
      "obj = -2019866.321865, rho = 0.936642\n",
      "nSV = 345, nBSV = 293\n",
      "Total nSV = 345\n",
      "Accuracy = 78.7234% (148/188) (classification)\n",
      "Accuracy = 70.977% (741/1044) (classification)\n",
      ".................................*............*\n",
      "optimization finished, #iter = 34174\n",
      "nu = 0.440676\n",
      "obj = -2081858.079778, rho = -0.343915\n",
      "nSV = 356, nBSV = 311\n",
      "Total nSV = 356\n",
      "Accuracy = 80.8511% (152/188) (classification)\n",
      "Accuracy = 67.5287% (705/1044) (classification)\n",
      "............................*.........*.................*\n",
      "optimization finished, #iter = 40477\n",
      "nu = 0.430234\n",
      "obj = -2033913.141313, rho = 0.534931\n",
      "nSV = 354, nBSV = 301\n",
      "Total nSV = 354\n",
      "Accuracy = 77.1277% (145/188) (classification)\n",
      "Accuracy = 69.0613% (721/1044) (classification)\n",
      "............................*..............*.......................................................*...................*..................*\n",
      "optimization finished, #iter = 99677\n",
      "nu = 0.435539\n",
      "obj = -2056133.443496, rho = 0.504165\n",
      "nSV = 361, nBSV = 306\n",
      "Total nSV = 361\n",
      "Accuracy = 82.4468% (155/188) (classification)\n",
      "Accuracy = 64.0805% (669/1044) (classification)\n",
      "0.4\n",
      "drop_size: 0.6\n",
      "............................*.........*\n",
      "optimization finished, #iter = 37057\n",
      "nu = 0.476772\n",
      "obj = -3054431.491139, rho = 1.388616\n",
      "nSV = 503, nBSV = 454\n",
      "Total nSV = 503\n",
      "Accuracy = 80.4781% (202/251) (classification)\n",
      "Accuracy = 70.2107% (733/1044) (classification)\n",
      "................*............*.........*\n",
      "optimization finished, #iter = 37300\n",
      "nu = 0.486499\n",
      "obj = -3125439.343180, rho = 1.280167\n",
      "nSV = 518, nBSV = 468\n",
      "Total nSV = 518\n",
      "Accuracy = 82.8685% (208/251) (classification)\n",
      "Accuracy = 71.0728% (742/1044) (classification)\n",
      "...........................*...................*.*\n",
      "optimization finished, #iter = 46819\n",
      "nu = 0.478628\n",
      "obj = -3075309.713112, rho = -1.404141\n",
      "nSV = 513, nBSV = 450\n",
      "Total nSV = 513\n",
      "Accuracy = 80.0797% (201/251) (classification)\n",
      "Accuracy = 72.318% (755/1044) (classification)\n",
      "....................*.......*\n",
      "optimization finished, #iter = 27880\n",
      "nu = 0.464540\n",
      "obj = -2976091.230993, rho = -1.553926\n",
      "nSV = 494, nBSV = 442\n",
      "Total nSV = 494\n",
      "Accuracy = 78.4861% (197/251) (classification)\n",
      "Accuracy = 72.5096% (757/1044) (classification)\n",
      "...............*..........................*...........*\n",
      "optimization finished, #iter = 51578\n",
      "nu = 0.467377\n",
      "obj = -3006318.809750, rho = -1.368480\n",
      "nSV = 496, nBSV = 439\n",
      "Total nSV = 496\n",
      "Accuracy = 82.0717% (206/251) (classification)\n",
      "Accuracy = 71.8391% (750/1044) (classification)\n",
      "0.5\n",
      "drop_size: 0.5\n",
      "...........................................*........................*............................*\n",
      "optimization finished, #iter = 94203\n",
      "nu = 0.441924\n",
      "obj = -3556756.401234, rho = 0.387119\n",
      "nSV = 584, nBSV = 531\n",
      "Total nSV = 584\n",
      "Accuracy = 81.2102% (255/314) (classification)\n",
      "Accuracy = 60.8238% (635/1044) (classification)\n",
      "........................*..............*.................*\n",
      "optimization finished, #iter = 54460\n",
      "nu = 0.433286\n",
      "obj = -3479806.891456, rho = -0.164619\n",
      "nSV = 575, nBSV = 517\n",
      "Total nSV = 575\n",
      "Accuracy = 78.0255% (245/314) (classification)\n",
      "Accuracy = 51.341% (536/1044) (classification)\n",
      "................................*.................*\n",
      "optimization finished, #iter = 49713\n",
      "nu = 0.439671\n",
      "obj = -3535144.086263, rho = -0.119111\n",
      "nSV = 583, nBSV = 527\n",
      "Total nSV = 583\n",
      "Accuracy = 80.5732% (253/314) (classification)\n",
      "Accuracy = 50.2874% (525/1044) (classification)\n",
      ".............................*................*..*\n",
      "optimization finished, #iter = 47749\n",
      "nu = 0.451932\n",
      "obj = -3649234.345875, rho = -0.930843\n",
      "nSV = 603, nBSV = 542\n",
      "Total nSV = 603\n",
      "Accuracy = 80.2548% (252/314) (classification)\n",
      "Accuracy = 65.613% (685/1044) (classification)\n",
      ".....................*...............*..................*\n",
      "optimization finished, #iter = 53831\n",
      "nu = 0.442489\n",
      "obj = -3571377.950041, rho = -0.230912\n",
      "nSV = 584, nBSV = 528\n",
      "Total nSV = 584\n",
      "Accuracy = 81.5287% (256/314) (classification)\n",
      "Accuracy = 59.387% (620/1044) (classification)\n",
      "0.6\n",
      "drop_size: 0.4\n",
      ".................................*................................*......................*.*\n",
      "optimization finished, #iter = 87438\n",
      "nu = 0.424356\n",
      "obj = -4101117.632943, rho = 1.367461\n",
      "nSV = 677, nBSV = 613\n",
      "Total nSV = 677\n",
      "Accuracy = 77.1277% (290/376) (classification)\n",
      "Accuracy = 66.4751% (694/1044) (classification)\n",
      "..........................*................*..................................*.........................*................................*....*\n",
      "optimization finished, #iter = 135449\n",
      "nu = 0.429147\n",
      "obj = -4149544.723711, rho = 1.016719\n",
      "nSV = 682, nBSV = 617\n",
      "Total nSV = 682\n",
      "Accuracy = 77.6596% (292/376) (classification)\n",
      "Accuracy = 64.272% (671/1044) (classification)\n",
      "................................*.................................*.....................*\n",
      "optimization finished, #iter = 86061\n",
      "nu = 0.443527\n",
      "obj = -4291785.366720, rho = 0.832139\n",
      "nSV = 700, nBSV = 639\n",
      "Total nSV = 700\n",
      "Accuracy = 81.9149% (308/376) (classification)\n",
      "Accuracy = 54.1188% (565/1044) (classification)\n",
      "...............................*....................*..................*....*\n",
      "optimization finished, #iter = 72441\n",
      "nu = 0.439243\n",
      "obj = -4244110.748414, rho = -1.048905\n",
      "nSV = 695, nBSV = 634\n",
      "Total nSV = 695\n",
      "Accuracy = 77.9255% (293/376) (classification)\n",
      "Accuracy = 61.59% (643/1044) (classification)\n",
      "....................................*....................*\n",
      "optimization finished, #iter = 56627\n",
      "nu = 0.434201\n",
      "obj = -4197545.748883, rho = 0.821709\n",
      "nSV = 687, nBSV = 627\n",
      "Total nSV = 687\n",
      "Accuracy = 77.9255% (293/376) (classification)\n",
      "Accuracy = 61.2069% (639/1044) (classification)\n",
      "0.7000000000000001\n",
      "drop_size: 0.29999999999999993\n",
      ".........................*..................*\n",
      "optimization finished, #iter = 43556\n",
      "nu = 0.447376\n",
      "obj = -5059785.298962, rho = 1.047032\n",
      "nSV = 823, nBSV = 758\n",
      "Total nSV = 823\n",
      "Accuracy = 82.9157% (364/439) (classification)\n",
      "Accuracy = 68.5824% (716/1044) (classification)\n",
      "..................................*.................*...................*\n",
      "optimization finished, #iter = 69953\n",
      "nu = 0.432645\n",
      "obj = -4897419.168587, rho = -1.061285\n",
      "nSV = 796, nBSV = 733\n",
      "Total nSV = 796\n",
      "Accuracy = 77.4487% (340/439) (classification)\n",
      "Accuracy = 65.9962% (689/1044) (classification)\n",
      "................................*.....................*...............................*.....*\n",
      "optimization finished, #iter = 87959\n",
      "nu = 0.444684\n",
      "obj = -5013096.827228, rho = 0.593679\n",
      "nSV = 815, nBSV = 750\n",
      "Total nSV = 815\n",
      "Accuracy = 81.3212% (357/439) (classification)\n",
      "Accuracy = 63.0268% (658/1044) (classification)\n",
      ".................................*........................................*...................*\n",
      "optimization finished, #iter = 91805\n",
      "nu = 0.450006\n",
      "obj = -5093735.825923, rho = -1.015110\n",
      "nSV = 822, nBSV = 761\n",
      "Total nSV = 822\n",
      "Accuracy = 82.2323% (361/439) (classification)\n",
      "Accuracy = 64.1762% (670/1044) (classification)\n",
      ".....................................*..............................*\n",
      "optimization finished, #iter = 67834\n",
      "nu = 0.433707\n",
      "obj = -4905355.896625, rho = -0.910567\n",
      "nSV = 798, nBSV = 740\n",
      "Total nSV = 798\n",
      "Accuracy = 78.1321% (343/439) (classification)\n",
      "Accuracy = 63.9847% (668/1044) (classification)\n",
      "0.8\n",
      "drop_size: 0.19999999999999996\n",
      ".................................*...........................*\n",
      "optimization finished, #iter = 60495\n",
      "nu = 0.442649\n",
      "obj = -5721566.759560, rho = -1.184917\n",
      "nSV = 924, nBSV = 859\n",
      "Total nSV = 924\n",
      "Accuracy = 80.0797% (402/502) (classification)\n",
      "Accuracy = 68.9655% (720/1044) (classification)\n",
      "..........................*............................*.....................*........*\n",
      "optimization finished, #iter = 81681\n",
      "nu = 0.441248\n",
      "obj = -5709113.134890, rho = 1.193750\n",
      "nSV = 917, nBSV = 857\n",
      "Total nSV = 917\n",
      "Accuracy = 81.4741% (409/502) (classification)\n",
      "Accuracy = 68.6782% (717/1044) (classification)\n",
      "...................................*..................*.......*\n",
      "optimization finished, #iter = 59837\n",
      "nu = 0.446595\n",
      "obj = -5750587.030174, rho = 1.008174\n",
      "nSV = 932, nBSV = 869\n",
      "Total nSV = 932\n",
      "Accuracy = 81.0757% (407/502) (classification)\n",
      "Accuracy = 66.2835% (692/1044) (classification)\n",
      ".................................*....................*\n",
      "optimization finished, #iter = 53387\n",
      "nu = 0.451607\n",
      "obj = -5834236.015458, rho = -0.822740\n",
      "nSV = 937, nBSV = 875\n",
      "Total nSV = 937\n",
      "Accuracy = 83.2669% (418/502) (classification)\n",
      "Accuracy = 65.8046% (687/1044) (classification)\n",
      ".........................................*............................*.................*\n",
      "optimization finished, #iter = 86378\n",
      "nu = 0.429557\n",
      "obj = -5553881.187433, rho = 1.352620\n",
      "nSV = 896, nBSV = 834\n",
      "Total nSV = 896\n",
      "Accuracy = 77.49% (389/502) (classification)\n",
      "Accuracy = 70.3065% (734/1044) (classification)\n",
      "0.9\n",
      "drop_size: 0.09999999999999998\n",
      "......................................*.............................*\n",
      "optimization finished, #iter = 67055\n",
      "nu = 0.431152\n",
      "obj = -6289206.842925, rho = -1.186366\n",
      "nSV = 1010, nBSV = 940\n",
      "Total nSV = 1010\n",
      "Accuracy = 79.4326% (448/564) (classification)\n",
      "Accuracy = 65.9004% (688/1044) (classification)\n",
      "........................................*...................................*...............................................................*\n",
      "optimization finished, #iter = 138451\n",
      "nu = 0.429799\n",
      "obj = -6275467.460113, rho = -1.407179\n",
      "nSV = 1012, nBSV = 937\n",
      "Total nSV = 1012\n",
      "Accuracy = 78.0142% (440/564) (classification)\n",
      "Accuracy = 69.1571% (722/1044) (classification)\n",
      "....................................................................*....................................*..................................................*.......*\n",
      "optimization finished, #iter = 160953\n",
      "nu = 0.442104\n",
      "obj = -6457534.988983, rho = -1.046175\n",
      "nSV = 1041, nBSV = 967\n",
      "Total nSV = 1041\n",
      "Accuracy = 80.3191% (453/564) (classification)\n",
      "Accuracy = 67.2414% (702/1044) (classification)\n",
      "..........................*................*..........*\n",
      "optimization finished, #iter = 52381\n",
      "nu = 0.436440\n",
      "obj = -6362982.521101, rho = -0.860091\n",
      "nSV = 1024, nBSV = 960\n",
      "Total nSV = 1024\n",
      "Accuracy = 80.3191% (453/564) (classification)\n",
      "Accuracy = 59.0038% (616/1044) (classification)\n",
      "......................................*.........................................................*.........................................................*\n",
      "optimization finished, #iter = 152416\n",
      "nu = 0.436304\n",
      "obj = -6367081.255609, rho = 0.858197\n",
      "nSV = 1021, nBSV = 955\n",
      "Total nSV = 1021\n",
      "Accuracy = 81.383% (459/564) (classification)\n",
      "Accuracy = 68.0077% (710/1044) (classification)\n",
      "1.0\n",
      "drop_size: 0.0\n",
      "............................*................................................*\n",
      "optimization finished, #iter = 76511\n",
      "nu = 0.452608\n",
      "obj = -7333876.534338, rho = 1.100479\n",
      "nSV = 1173, nBSV = 1105\n",
      "Total nSV = 1173\n",
      "Accuracy = 83.4131% (523/627) (classification)\n",
      "Accuracy = 67.3372% (703/1044) (classification)\n",
      ".............................................*...........................................................*\n",
      "optimization finished, #iter = 104842\n",
      "nu = 0.426376\n",
      "obj = -6896452.569569, rho = 1.150346\n",
      "nSV = 1106, nBSV = 1040\n",
      "Total nSV = 1106\n",
      "Accuracy = 74.8006% (469/627) (classification)\n",
      "Accuracy = 63.5057% (663/1044) (classification)\n",
      "..............................................*.................................................*.............*\n",
      "optimization finished, #iter = 107818\n",
      "nu = 0.453478\n",
      "obj = -7359767.426589, rho = 1.248148\n",
      "nSV = 1178, nBSV = 1105\n",
      "Total nSV = 1178\n",
      "Accuracy = 83.4131% (523/627) (classification)\n",
      "Accuracy = 66.954% (699/1044) (classification)\n",
      "..................................................*.............................*...............................*\n",
      "optimization finished, #iter = 110663\n",
      "nu = 0.442648\n",
      "obj = -7193552.309436, rho = -1.132242\n",
      "nSV = 1143, nBSV = 1074\n",
      "Total nSV = 1143\n",
      "Accuracy = 80.7018% (506/627) (classification)\n",
      "Accuracy = 64.9425% (678/1044) (classification)\n",
      "...................................................*......................................*\n",
      "optimization finished, #iter = 89922\n",
      "nu = 0.439599\n",
      "obj = -7142708.252500, rho = -1.251615\n",
      "nSV = 1140, nBSV = 1072\n",
      "Total nSV = 1140\n",
      "Accuracy = 80.3828% (504/627) (classification)\n",
      "Accuracy = 65.3257% (682/1044) (classification)\n"
     ]
    }
   ],
   "source": [
    "ratio_val_res = []\n",
    "ratio_test_res = []\n",
    "for ratio in np.arange(0.1, 1.1, 0.1):\n",
    "    val_acc_list = []\n",
    "    test_acc_list = []\n",
    "    # ratio = round(i, 1)\n",
    "    # train_num = int(len(train_x_scaled)*ratio)\n",
    "    drop_size = 1 - ratio\n",
    "    print(ratio)\n",
    "    print(f\"drop_size: {drop_size}\")\n",
    "    if drop_size != 0:\n",
    "        part_train_x_scaled, _, part_train_y_scaled, _ = train_test_split(train_x_scaled, train_y_scaled, test_size=drop_size)\n",
    "    else:\n",
    "        part_train_x_scaled, part_train_y_scaled = train_x_scaled, train_y_scaled\n",
    "\n",
    "    for i in range(5):\n",
    "        train_x_scaled_, val_x_scaled, train_y_scaled_, val_y_scaled = train_test_split(part_train_x_scaled, part_train_y_scaled, test_size=0.2)\n",
    "        m = svm_train(train_y_scaled_, train_x_scaled_, f'-t 1 -c {c_star} -d {d_star}')\n",
    "        sv_len = m.get_sv_indices()\n",
    "        sv_counts_list += [len(sv_len)]\n",
    "        p_label, p_acc, p_val = svm_predict(val_y_scaled, val_x_scaled, m)\n",
    "        val_acc_list += [p_acc[0]]\n",
    "        p_label, p_acc, p_val = svm_predict(test_y_scaled, test_x_scaled, m)\n",
    "        test_acc_list += [p_acc[0]]\n",
    "    ratio_val_res += [sum(val_acc_list)/5]\n",
    "    ratio_test_res += [sum(test_acc_list)/5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "43272634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[80.63492063492063,\n",
       " 77.46031746031746,\n",
       " 79.25531914893618,\n",
       " 80.79681274900398,\n",
       " 80.31847133757962,\n",
       " 78.51063829787233,\n",
       " 80.41002277904327,\n",
       " 80.67729083665338,\n",
       " 79.8936170212766,\n",
       " 80.54226475279106]"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_val_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "b37b7b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'val_acc')"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ratio_val_res)\n",
    "plt.xticks([i for i in range(10)],[f\"{i}%\" for i in range(10,110,10)], rotation=20)\n",
    "plt.xlabel('train_num (by ratio)')\n",
    "plt.ylabel('val_acc')\n",
    "# ratio from 0.1 to 1.0 with 0.1 step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "360e6ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'test_acc')"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ratio_test_res)\n",
    "plt.xticks([i for i in range(10)],[f\"{i}%\" for i in range(10,110,10)], rotation=20)\n",
    "plt.xlabel('train_num (by ratio)')\n",
    "plt.ylabel('test_acc')\n",
    "# ratio from 0.1 to 1.0 with 0.1 step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e1b469",
   "metadata": {},
   "source": [
    "# C6(c)\n",
    "Apply Stochastic Gradient Descent to solve the optimization problem. Plot the five-fold crossvalidation training and test errors for the hypotheses obtained based on the solution  as a function of d, for the best value of C measured on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "86bed930",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1225\n",
      "nu = 0.974461\n",
      "obj = -0.041330, rho = -0.993988\n",
      "nSV = 2442, nBSV = 2442\n",
      "Total nSV = 2442\n",
      "Accuracy = 49.9203% (313/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1228\n",
      "nu = 0.980048\n",
      "obj = -0.041566, rho = 0.993737\n",
      "nSV = 2456, nBSV = 2456\n",
      "Total nSV = 2456\n",
      "Accuracy = 51.0367% (320/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1214\n",
      "nu = 0.965682\n",
      "obj = -0.040959, rho = -0.994293\n",
      "nSV = 2420, nBSV = 2420\n",
      "Total nSV = 2420\n",
      "Accuracy = 48.1659% (302/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1253\n",
      "nu = 1.000000\n",
      "obj = -0.042408, rho = 0.000777\n",
      "nSV = 2506, nBSV = 2506\n",
      "Total nSV = 2506\n",
      "Accuracy = 72.5678% (455/627) (classification)\n",
      "Accuracy = 58.7165% (613/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1213\n",
      "nu = 0.968077\n",
      "obj = -0.041059, rho = 0.994154\n",
      "nSV = 2426, nBSV = 2426\n",
      "Total nSV = 2426\n",
      "Accuracy = 48.6443% (305/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1225\n",
      "nu = 0.974461\n",
      "obj = -0.123811, rho = -0.981108\n",
      "nSV = 2442, nBSV = 2442\n",
      "Total nSV = 2442\n",
      "Accuracy = 49.9203% (313/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1221\n",
      "nu = 0.974461\n",
      "obj = -0.123823, rho = 0.981279\n",
      "nSV = 2442, nBSV = 2442\n",
      "Total nSV = 2442\n",
      "Accuracy = 49.9203% (313/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1223\n",
      "nu = 0.976057\n",
      "obj = -0.124037, rho = 0.981765\n",
      "nSV = 2446, nBSV = 2446\n",
      "Total nSV = 2446\n",
      "Accuracy = 50.2392% (315/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1235\n",
      "nu = 0.985634\n",
      "obj = -0.125228, rho = 0.980056\n",
      "nSV = 2470, nBSV = 2470\n",
      "Total nSV = 2470\n",
      "Accuracy = 52.1531% (327/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1232\n",
      "nu = 0.983240\n",
      "obj = -0.124925, rho = 0.980264\n",
      "nSV = 2464, nBSV = 2464\n",
      "Total nSV = 2464\n",
      "Accuracy = 51.6746% (324/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1226\n",
      "nu = 0.978452\n",
      "obj = -0.371531, rho = 0.943883\n",
      "nSV = 2452, nBSV = 2452\n",
      "Total nSV = 2452\n",
      "Accuracy = 50.7177% (318/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1227\n",
      "nu = 0.979250\n",
      "obj = -0.371740, rho = 0.942340\n",
      "nSV = 2454, nBSV = 2454\n",
      "Total nSV = 2454\n",
      "Accuracy = 50.8772% (319/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1226\n",
      "nu = 0.974461\n",
      "obj = -0.369986, rho = -0.944119\n",
      "nSV = 2442, nBSV = 2442\n",
      "Total nSV = 2442\n",
      "Accuracy = 49.9203% (313/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1253\n",
      "nu = 0.999202\n",
      "obj = -0.379096, rho = -0.951723\n",
      "nSV = 2504, nBSV = 2504\n",
      "Total nSV = 2504\n",
      "Accuracy = 44.8166% (281/627) (classification)\n",
      "Accuracy = 52.2989% (546/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1222\n",
      "nu = 0.975259\n",
      "obj = -0.370340, rho = 0.944306\n",
      "nSV = 2444, nBSV = 2444\n",
      "Total nSV = 2444\n",
      "Accuracy = 50.0797% (314/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1231\n",
      "nu = 0.982442\n",
      "obj = -1.104336, rho = 0.821912\n",
      "nSV = 2462, nBSV = 2462\n",
      "Total nSV = 2462\n",
      "Accuracy = 51.5152% (323/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1209\n",
      "nu = 0.962490\n",
      "obj = -1.084455, rho = -0.843124\n",
      "nSV = 2412, nBSV = 2412\n",
      "Total nSV = 2412\n",
      "Accuracy = 47.5279% (298/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1222\n",
      "nu = 0.975259\n",
      "obj = -1.097583, rho = 0.831005\n",
      "nSV = 2444, nBSV = 2444\n",
      "Total nSV = 2444\n",
      "Accuracy = 50.0797% (314/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1229\n",
      "nu = 0.980846\n",
      "obj = -1.103176, rho = 0.825298\n",
      "nSV = 2458, nBSV = 2458\n",
      "Total nSV = 2458\n",
      "Accuracy = 51.1962% (321/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1220\n",
      "nu = 0.973663\n",
      "obj = -1.096944, rho = 0.837804\n",
      "nSV = 2440, nBSV = 2440\n",
      "Total nSV = 2440\n",
      "Accuracy = 49.7608% (312/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1197\n",
      "nu = 0.955307\n",
      "obj = -3.137180, rho = 0.562964\n",
      "nSV = 2394, nBSV = 2394\n",
      "Total nSV = 2394\n",
      "Accuracy = 46.0925% (289/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1229\n",
      "nu = 0.980846\n",
      "obj = -3.191518, rho = 0.482836\n",
      "nSV = 2458, nBSV = 2458\n",
      "Total nSV = 2458\n",
      "Accuracy = 51.1962% (321/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1226\n",
      "nu = 0.975259\n",
      "obj = -3.178739, rho = -0.502080\n",
      "nSV = 2444, nBSV = 2444\n",
      "Total nSV = 2444\n",
      "Accuracy = 50.0797% (314/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1234\n",
      "nu = 0.981644\n",
      "obj = -3.197318, rho = -0.490730\n",
      "nSV = 2460, nBSV = 2460\n",
      "Total nSV = 2460\n",
      "Accuracy = 51.3557% (322/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1224\n",
      "nu = 0.976856\n",
      "obj = -3.177202, rho = 0.492064\n",
      "nSV = 2448, nBSV = 2448\n",
      "Total nSV = 2448\n",
      "Accuracy = 50.3987% (316/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1196\n",
      "nu = 0.945134\n",
      "obj = -8.567656, rho = -0.234062\n",
      "nSV = 2370, nBSV = 2368\n",
      "Total nSV = 2370\n",
      "Accuracy = 73.0463% (458/627) (classification)\n",
      "Accuracy = 62.4521% (652/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1201\n",
      "nu = 0.949721\n",
      "obj = -8.530775, rho = -0.311826\n",
      "nSV = 2380, nBSV = 2380\n",
      "Total nSV = 2380\n",
      "Accuracy = 72.0893% (452/627) (classification)\n",
      "Accuracy = 64.4636% (673/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1195\n",
      "nu = 0.945952\n",
      "obj = -8.445591, rho = -0.337477\n",
      "nSV = 2372, nBSV = 2370\n",
      "Total nSV = 2372\n",
      "Accuracy = 68.8995% (432/627) (classification)\n",
      "Accuracy = 64.6552% (675/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1187\n",
      "nu = 0.944079\n",
      "obj = -8.544688, rho = 0.252659\n",
      "nSV = 2366, nBSV = 2364\n",
      "Total nSV = 2366\n",
      "Accuracy = 73.2057% (459/627) (classification)\n",
      "Accuracy = 63.6973% (665/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1194\n",
      "nu = 0.945284\n",
      "obj = -8.569521, rho = -0.229329\n",
      "nSV = 2370, nBSV = 2368\n",
      "Total nSV = 2370\n",
      "Accuracy = 72.7273% (456/627) (classification)\n",
      "Accuracy = 62.5479% (653/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1047\n",
      "nu = 0.797949\n",
      "obj = -22.272809, rho = 0.550747\n",
      "nSV = 2000, nBSV = 1998\n",
      "Total nSV = 2000\n",
      "Accuracy = 74.3222% (466/627) (classification)\n",
      "Accuracy = 63.41% (662/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1040\n",
      "nu = 0.789064\n",
      "obj = -21.865868, rho = -0.550285\n",
      "nSV = 1978, nBSV = 1976\n",
      "Total nSV = 1978\n",
      "Accuracy = 70.0159% (439/627) (classification)\n",
      "Accuracy = 64.0805% (669/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1038\n",
      "nu = 0.783927\n",
      "obj = -21.662750, rho = 0.556136\n",
      "nSV = 1966, nBSV = 1964\n",
      "Total nSV = 1966\n",
      "Accuracy = 66.6667% (418/627) (classification)\n",
      "Accuracy = 63.6015% (664/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1040\n",
      "nu = 0.791700\n",
      "obj = -21.933171, rho = -0.553645\n",
      "nSV = 1984, nBSV = 1984\n",
      "Total nSV = 1984\n",
      "Accuracy = 68.2616% (428/627) (classification)\n",
      "Accuracy = 64.1762% (670/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1038\n",
      "nu = 0.791213\n",
      "obj = -22.014014, rho = -0.554684\n",
      "nSV = 1984, nBSV = 1982\n",
      "Total nSV = 1984\n",
      "Accuracy = 70.1754% (440/627) (classification)\n",
      "Accuracy = 63.6015% (664/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 949\n",
      "nu = 0.691766\n",
      "obj = -59.894725, rho = -0.728680\n",
      "nSV = 1735, nBSV = 1732\n",
      "Total nSV = 1735\n",
      "Accuracy = 73.5247% (461/627) (classification)\n",
      "Accuracy = 65.1341% (680/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 970\n",
      "nu = 0.692615\n",
      "obj = -59.923195, rho = 0.737327\n",
      "nSV = 1736, nBSV = 1734\n",
      "Total nSV = 1736\n",
      "Accuracy = 72.8868% (457/627) (classification)\n",
      "Accuracy = 64.0805% (669/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 970\n",
      "nu = 0.691400\n",
      "obj = -59.887752, rho = 0.723679\n",
      "nSV = 1734, nBSV = 1732\n",
      "Total nSV = 1734\n",
      "Accuracy = 72.2488% (453/627) (classification)\n",
      "Accuracy = 64.8467% (677/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 965\n",
      "nu = 0.695856\n",
      "obj = -60.159494, rho = 0.746336\n",
      "nSV = 1745, nBSV = 1742\n",
      "Total nSV = 1745\n",
      "Accuracy = 73.2057% (459/627) (classification)\n",
      "Accuracy = 64.8467% (677/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 972\n",
      "nu = 0.698762\n",
      "obj = -60.557823, rho = 0.721410\n",
      "nSV = 1752, nBSV = 1749\n",
      "Total nSV = 1752\n",
      "Accuracy = 74.3222% (466/627) (classification)\n",
      "Accuracy = 65.7088% (686/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 930\n",
      "nu = 0.630067\n",
      "obj = -167.520785, rho = 0.887137\n",
      "nSV = 1581, nBSV = 1578\n",
      "Total nSV = 1581\n",
      "Accuracy = 72.8868% (457/627) (classification)\n",
      "Accuracy = 65.0383% (679/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 921\n",
      "nu = 0.638347\n",
      "obj = -170.026559, rho = -0.850972\n",
      "nSV = 1601, nBSV = 1597\n",
      "Total nSV = 1601\n",
      "Accuracy = 74.0032% (464/627) (classification)\n",
      "Accuracy = 65.9004% (688/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 910\n",
      "nu = 0.635282\n",
      "obj = -169.032852, rho = -0.919397\n",
      "nSV = 1594, nBSV = 1591\n",
      "Total nSV = 1594\n",
      "Accuracy = 75.5981% (474/627) (classification)\n",
      "Accuracy = 66.1877% (691/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 918\n",
      "nu = 0.634226\n",
      "obj = -168.888414, rho = -0.908501\n",
      "nSV = 1590, nBSV = 1587\n",
      "Total nSV = 1590\n",
      "Accuracy = 73.5247% (461/627) (classification)\n",
      "Accuracy = 66.092% (690/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 912\n",
      "nu = 0.631197\n",
      "obj = -168.413514, rho = 0.877780\n",
      "nSV = 1582, nBSV = 1580\n",
      "Total nSV = 1582\n",
      "Accuracy = 73.2057% (459/627) (classification)\n",
      "Accuracy = 67.0498% (700/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 920\n",
      "nu = 0.611789\n",
      "obj = -491.220296, rho = 1.125578\n",
      "nSV = 1536, nBSV = 1531\n",
      "Total nSV = 1536\n",
      "Accuracy = 74.6411% (468/627) (classification)\n",
      "Accuracy = 68.295% (713/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 941\n",
      "nu = 0.612290\n",
      "obj = -491.304251, rho = -1.085654\n",
      "nSV = 1536, nBSV = 1533\n",
      "Total nSV = 1536\n",
      "Accuracy = 76.3955% (479/627) (classification)\n",
      "Accuracy = 67.9119% (709/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 903\n",
      "nu = 0.603820\n",
      "obj = -483.859632, rho = 1.183538\n",
      "nSV = 1515, nBSV = 1511\n",
      "Total nSV = 1515\n",
      "Accuracy = 74.9601% (470/627) (classification)\n",
      "Accuracy = 69.636% (727/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 906\n",
      "nu = 0.602541\n",
      "obj = -483.484212, rho = -1.250506\n",
      "nSV = 1511, nBSV = 1508\n",
      "Total nSV = 1511\n",
      "Accuracy = 74.1627% (465/627) (classification)\n",
      "Accuracy = 69.0613% (721/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 928\n",
      "nu = 0.603028\n",
      "obj = -482.228151, rho = -1.026625\n",
      "nSV = 1513, nBSV = 1508\n",
      "Total nSV = 1513\n",
      "Accuracy = 74.0032% (464/627) (classification)\n",
      "Accuracy = 68.6782% (717/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 997\n",
      "nu = 0.584077\n",
      "obj = -1394.965312, rho = -1.816067\n",
      "nSV = 1466, nBSV = 1461\n",
      "Total nSV = 1466\n",
      "Accuracy = 78.1499% (490/627) (classification)\n",
      "Accuracy = 70.5939% (737/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 931\n",
      "nu = 0.569564\n",
      "obj = -1363.860645, rho = 1.675684\n",
      "nSV = 1429, nBSV = 1425\n",
      "Total nSV = 1429\n",
      "Accuracy = 74.9601% (470/627) (classification)\n",
      "Accuracy = 70.4981% (736/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 886\n",
      "nu = 0.571897\n",
      "obj = -1371.844395, rho = 1.432789\n",
      "nSV = 1437, nBSV = 1429\n",
      "Total nSV = 1437\n",
      "Accuracy = 75.1196% (471/627) (classification)\n",
      "Accuracy = 68.1992% (712/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 921\n",
      "nu = 0.573924\n",
      "obj = -1376.128640, rho = 1.416843\n",
      "nSV = 1441, nBSV = 1436\n",
      "Total nSV = 1441\n",
      "Accuracy = 75.1196% (471/627) (classification)\n",
      "Accuracy = 69.7318% (728/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 948\n",
      "nu = 0.578610\n",
      "obj = -1386.390211, rho = 1.644472\n",
      "nSV = 1452, nBSV = 1447\n",
      "Total nSV = 1452\n",
      "Accuracy = 77.193% (484/627) (classification)\n",
      "Accuracy = 69.5402% (726/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1199\n",
      "nu = 0.544833\n",
      "obj = -3931.892093, rho = -2.731453\n",
      "nSV = 1368, nBSV = 1363\n",
      "Total nSV = 1368\n",
      "Accuracy = 76.874% (482/627) (classification)\n",
      "Accuracy = 69.2529% (723/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1222\n",
      "nu = 0.544752\n",
      "obj = -3926.516671, rho = -2.588346\n",
      "nSV = 1368, nBSV = 1364\n",
      "Total nSV = 1368\n",
      "Accuracy = 79.9043% (501/627) (classification)\n",
      "Accuracy = 69.4444% (725/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1500\n",
      "nu = 0.543942\n",
      "obj = -3904.838013, rho = -2.983514\n",
      "nSV = 1366, nBSV = 1361\n",
      "Total nSV = 1366\n",
      "Accuracy = 78.3094% (491/627) (classification)\n",
      "Accuracy = 69.3487% (724/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "**.*\n",
      "optimization finished, #iter = 1651\n",
      "nu = 0.543804\n",
      "obj = -3918.655226, rho = -3.187122\n",
      "nSV = 1365, nBSV = 1360\n",
      "Total nSV = 1365\n",
      "Accuracy = 79.2663% (497/627) (classification)\n",
      "Accuracy = 73.0843% (763/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*.\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1391\n",
      "nu = 0.556958\n",
      "obj = -4003.296735, rho = -2.812605\n",
      "nSV = 1400, nBSV = 1394\n",
      "Total nSV = 1400\n",
      "Accuracy = 80.5423% (505/627) (classification)\n",
      "Accuracy = 71.0728% (742/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*.\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 2291\n",
      "nu = 0.506084\n",
      "obj = -11066.311457, rho = -3.854312\n",
      "nSV = 1271, nBSV = 1265\n",
      "Total nSV = 1271\n",
      "Accuracy = 77.0335% (483/627) (classification)\n",
      "Accuracy = 64.9425% (678/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1906\n",
      "nu = 0.510239\n",
      "obj = -11198.423760, rho = 3.593153\n",
      "nSV = 1282, nBSV = 1275\n",
      "Total nSV = 1282\n",
      "Accuracy = 78.1499% (490/627) (classification)\n",
      "Accuracy = 64.1762% (670/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*.\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 2355\n",
      "nu = 0.516124\n",
      "obj = -11334.810665, rho = 3.030034\n",
      "nSV = 1296, nBSV = 1291\n",
      "Total nSV = 1296\n",
      "Accuracy = 79.2663% (497/627) (classification)\n",
      "Accuracy = 59.7701% (624/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*.\n",
      "WARNING: using -h 0 may be faster\n",
      "*.*\n",
      "optimization finished, #iter = 2627\n",
      "nu = 0.511874\n",
      "obj = -11200.891301, rho = -4.125183\n",
      "nSV = 1286, nBSV = 1278\n",
      "Total nSV = 1286\n",
      "Accuracy = 78.1499% (490/627) (classification)\n",
      "Accuracy = 70.5939% (737/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*.*\n",
      "optimization finished, #iter = 2756\n",
      "nu = 0.517715\n",
      "obj = -11375.107030, rho = -2.828431\n",
      "nSV = 1301, nBSV = 1293\n",
      "Total nSV = 1301\n",
      "Accuracy = 77.512% (486/627) (classification)\n",
      "Accuracy = 57.8544% (604/1044) (classification)\n",
      "..\n",
      "WARNING: using -h 0 may be faster\n",
      "*..*\n",
      "optimization finished, #iter = 4862\n",
      "nu = 0.501646\n",
      "obj = -33388.794858, rho = 3.108718\n",
      "nSV = 1261, nBSV = 1252\n",
      "Total nSV = 1261\n",
      "Accuracy = 78.6284% (493/627) (classification)\n",
      "Accuracy = 50.9579% (532/1044) (classification)\n",
      "...\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 3656\n",
      "nu = 0.492008\n",
      "obj = -32783.393994, rho = -4.748972\n",
      "nSV = 1236, nBSV = 1230\n",
      "Total nSV = 1236\n",
      "Accuracy = 78.7879% (494/627) (classification)\n",
      "Accuracy = 68.7739% (718/1044) (classification)\n",
      "....\n",
      "WARNING: using -h 0 may be faster\n",
      "*.\n",
      "WARNING: using -h 0 may be faster\n",
      "*.......*\n",
      "optimization finished, #iter = 12021\n",
      "nu = 0.496736\n",
      "obj = -33177.755505, rho = -3.201651\n",
      "nSV = 1248, nBSV = 1240\n",
      "Total nSV = 1248\n",
      "Accuracy = 78.6284% (493/627) (classification)\n",
      "Accuracy = 54.3103% (567/1044) (classification)\n",
      "....\n",
      "WARNING: using -h 0 may be faster\n",
      "*.......*\n",
      "optimization finished, #iter = 11423\n",
      "nu = 0.497920\n",
      "obj = -33231.413382, rho = -3.287296\n",
      "nSV = 1253, nBSV = 1243\n",
      "Total nSV = 1253\n",
      "Accuracy = 78.9474% (495/627) (classification)\n",
      "Accuracy = 56.9923% (595/1044) (classification)\n",
      "...\n",
      "WARNING: using -h 0 may be faster\n",
      "*..*\n",
      "optimization finished, #iter = 5629\n",
      "nu = 0.502901\n",
      "obj = -33572.398063, rho = 3.101748\n",
      "nSV = 1266, nBSV = 1256\n",
      "Total nSV = 1266\n",
      "Accuracy = 80.0638% (502/627) (classification)\n",
      "Accuracy = 51.8199% (541/1044) (classification)\n",
      "........\n",
      "WARNING: using -h 0 may be faster\n",
      "*.*........*.*\n",
      "optimization finished, #iter = 17430\n",
      "nu = 0.492188\n",
      "obj = -99241.093347, rho = 2.808057\n",
      "nSV = 1237, nBSV = 1227\n",
      "Total nSV = 1237\n",
      "Accuracy = 80.0638% (502/627) (classification)\n",
      "Accuracy = 51.341% (536/1044) (classification)\n",
      "...........*....*...*\n",
      "optimization finished, #iter = 17100\n",
      "nu = 0.489871\n",
      "obj = -98727.719203, rho = -2.734839\n",
      "nSV = 1232, nBSV = 1223\n",
      "Total nSV = 1232\n",
      "Accuracy = 77.8309% (488/627) (classification)\n",
      "Accuracy = 48.7548% (509/1044) (classification)\n",
      "........*.........*......*\n",
      "optimization finished, #iter = 23030\n",
      "nu = 0.495356\n",
      "obj = -99854.586811, rho = -2.668685\n",
      "nSV = 1247, nBSV = 1237\n",
      "Total nSV = 1247\n",
      "Accuracy = 80.7018% (506/627) (classification)\n",
      "Accuracy = 48.1801% (503/1044) (classification)\n",
      "........\n",
      "WARNING: using -h 0 may be faster\n",
      "*..\n",
      "WARNING: using -h 0 may be faster\n",
      "*..*..*\n",
      "optimization finished, #iter = 14010\n",
      "nu = 0.500580\n",
      "obj = -100975.730891, rho = 2.529622\n",
      "nSV = 1259, nBSV = 1250\n",
      "Total nSV = 1259\n",
      "Accuracy = 80.7018% (506/627) (classification)\n",
      "Accuracy = 50.4789% (527/1044) (classification)\n",
      "..........*...........*....*\n",
      "optimization finished, #iter = 25061\n",
      "nu = 0.500851\n",
      "obj = -100909.467530, rho = -1.958406\n",
      "nSV = 1261, nBSV = 1251\n",
      "Total nSV = 1261\n",
      "Accuracy = 80.5423% (505/627) (classification)\n",
      "Accuracy = 47.0307% (491/1044) (classification)\n",
      "...............*......*..................................*..........*\n",
      "optimization finished, #iter = 63707\n",
      "nu = 0.490267\n",
      "obj = -297569.922713, rho = 2.442575\n",
      "nSV = 1232, nBSV = 1223\n",
      "Total nSV = 1232\n",
      "Accuracy = 80.3828% (504/627) (classification)\n",
      "Accuracy = 47.4138% (495/1044) (classification)\n",
      "..................*........*.....................*.....................................*\n",
      "optimization finished, #iter = 83853\n",
      "nu = 0.502159\n",
      "obj = -304878.760970, rho = 1.957029\n",
      "nSV = 1263, nBSV = 1253\n",
      "Total nSV = 1263\n",
      "Accuracy = 82.1372% (515/627) (classification)\n",
      "Accuracy = 47.2222% (493/1044) (classification)\n",
      "...........\n",
      "WARNING: using -h 0 may be faster\n",
      "*..*...............*...............*.............*......*.*\n",
      "optimization finished, #iter = 59630\n",
      "nu = 0.492834\n",
      "obj = -299316.433459, rho = 2.334973\n",
      "nSV = 1238, nBSV = 1228\n",
      "Total nSV = 1238\n",
      "Accuracy = 80.2233% (503/627) (classification)\n",
      "Accuracy = 47.7969% (499/1044) (classification)\n",
      "..................*..........*..................................*................*.....*\n",
      "optimization finished, #iter = 82277\n",
      "nu = 0.487442\n",
      "obj = -295909.625879, rho = 2.474804\n",
      "nSV = 1227, nBSV = 1218\n",
      "Total nSV = 1227\n",
      "Accuracy = 79.2663% (497/627) (classification)\n",
      "Accuracy = 47.0307% (491/1044) (classification)\n",
      ".............*.....*.....................*.*\n",
      "optimization finished, #iter = 39620\n",
      "nu = 0.470052\n",
      "obj = -285347.453102, rho = -4.827534\n",
      "nSV = 1184, nBSV = 1174\n",
      "Total nSV = 1184\n",
      "Accuracy = 76.7145% (481/627) (classification)\n",
      "Accuracy = 54.5019% (569/1044) (classification)\n",
      ".............................*..........*.................................................................................*............................*...............*\n",
      "optimization finished, #iter = 161966\n",
      "nu = 0.482710\n",
      "obj = -881116.899072, rho = 5.283578\n",
      "nSV = 1216, nBSV = 1207\n",
      "Total nSV = 1216\n",
      "Accuracy = 78.9474% (495/627) (classification)\n",
      "Accuracy = 65.8046% (687/1044) (classification)\n",
      ".....................................*...............*...........................................................*.................................................................................*.*\n",
      "optimization finished, #iter = 191681\n",
      "nu = 0.480708\n",
      "obj = -877367.343153, rho = 4.413107\n",
      "nSV = 1210, nBSV = 1198\n",
      "Total nSV = 1210\n",
      "Accuracy = 79.2663% (497/627) (classification)\n",
      "Accuracy = 53.5441% (559/1044) (classification)\n",
      "................................*....*..................................................................................................................................................*.........................................*...........................*......................................................................................*\n",
      "optimization finished, #iter = 334057\n",
      "nu = 0.472541\n",
      "obj = -862518.014349, rho = -2.873319\n",
      "nSV = 1189, nBSV = 1180\n",
      "Total nSV = 1189\n",
      "Accuracy = 78.4689% (492/627) (classification)\n",
      "Accuracy = 48.8506% (510/1044) (classification)\n",
      "..........................................*.....*................................................*..................................*....................................................................................................*....................*.*\n",
      "optimization finished, #iter = 247749\n",
      "nu = 0.481848\n",
      "obj = -879346.401821, rho = -2.389233\n",
      "nSV = 1212, nBSV = 1201\n",
      "Total nSV = 1212\n",
      "Accuracy = 78.7879% (494/627) (classification)\n",
      "Accuracy = 46.8391% (489/1044) (classification)\n",
      ".........................................*.......*...............................................................*............................................................................................................................*................................................*\n",
      "optimization finished, #iter = 281460\n",
      "nu = 0.478151\n",
      "obj = -872413.574960, rho = 2.261671\n",
      "nSV = 1203, nBSV = 1193\n",
      "Total nSV = 1203\n",
      "Accuracy = 77.512% (486/627) (classification)\n",
      "Accuracy = 47.0307% (491/1044) (classification)\n",
      ".......................................................................................*..............*..........................................................................................*.................................................................................................................................*......................................................................................*.................................................................*\n",
      "optimization finished, #iter = 469711\n",
      "nu = 0.481361\n",
      "obj = -2637063.713775, rho = 2.149540\n",
      "nSV = 1211, nBSV = 1201\n",
      "Total nSV = 1211\n",
      "Accuracy = 78.7879% (494/627) (classification)\n",
      "Accuracy = 46.7433% (488/1044) (classification)\n",
      ".................................................................*......*.........................................................................*...........................................................................................................................*.............................................................................................................................................................*..............................................................................................................................................*..........................................................*\n",
      "optimization finished, #iter = 621579\n",
      "nu = 0.488041\n",
      "obj = -2673840.812838, rho = -2.247600\n",
      "nSV = 1227, nBSV = 1218\n",
      "Total nSV = 1227\n",
      "Accuracy = 80.7018% (506/627) (classification)\n",
      "Accuracy = 47.2222% (493/1044) (classification)\n",
      "...............................................................*........................................................................................*......................................................*...............................................................................*.................*............................................................................................................*........*\n",
      "optimization finished, #iter = 415854\n",
      "nu = 0.480908\n",
      "obj = -2634468.102441, rho = -2.048836\n",
      "nSV = 1208, nBSV = 1199\n",
      "Total nSV = 1208\n",
      "Accuracy = 79.1069% (496/627) (classification)\n",
      "Accuracy = 47.2222% (493/1044) (classification)\n",
      "...........................................................*..................*............................................................................*......................................................................................*..............................................................................................*..............................................................................................................................................................................*..............................................................................................................................................................................*..............................*\n",
      "optimization finished, #iter = 707958\n",
      "nu = 0.490922\n",
      "obj = -2689685.275821, rho = -2.159156\n",
      "nSV = 1234, nBSV = 1225\n",
      "Total nSV = 1234\n",
      "Accuracy = 81.1802% (509/627) (classification)\n",
      "Accuracy = 46.8391% (489/1044) (classification)\n",
      "....................................................*........................................................................*........................................................................*..................................................................................................................................*..........................................................................................................................*...............................................................................................................................................................................................*\n",
      "optimization finished, #iter = 637997\n",
      "nu = 0.480393\n",
      "obj = -2631901.131084, rho = -2.752308\n",
      "nSV = 1208, nBSV = 1199\n",
      "Total nSV = 1208\n",
      "Accuracy = 78.6284% (493/627) (classification)\n",
      "Accuracy = 47.2222% (493/1044) (classification)\n",
      "................................................................................................*.........*.......................................................................................................................................*..........................................................................................................................................................................................................................................................................................................................................................................................................*...................................................................................................................................................................................................................................................................*........................................................................................................................................................................................................................................................................................................................................................................................................................*................................................................................................................................................................................................................................................................................................................................*............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*.....................................................*...........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*\n",
      "optimization finished, #iter = 3357846\n",
      "nu = 0.472541\n",
      "obj = -7768128.711168, rho = -4.722948\n",
      "nSV = 1189, nBSV = 1179\n",
      "Total nSV = 1189\n",
      "Accuracy = 76.3955% (479/627) (classification)\n",
      "Accuracy = 60.0575% (627/1044) (classification)\n",
      "..................................................................................................................................................................................*........................................................................*...............................................................................................................................................................................................................................................................*.......................................................................................................................................................................................................*..............................................................................................................................................*..................................................................................................................................................................................................................................................................................................................................*....................................................................................................................................................................................................................................*......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*...............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*.*\n",
      "optimization finished, #iter = 2468853\n",
      "nu = 0.482510\n",
      "obj = -7931804.760140, rho = 2.327981\n",
      "nSV = 1214, nBSV = 1204\n",
      "Total nSV = 1214\n",
      "Accuracy = 79.4258% (498/627) (classification)\n",
      "Accuracy = 47.1264% (492/1044) (classification)\n",
      ".................................................................................................................................................................................................*...........................................................*......................................................................................................................................................................................................................................*........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*..........................................................................................................................................................................................................................................................................................................*............................................................................................................................................................................*.....................................................................................................................................................................................................................*................................................................................................................................*......................................*...........................................................................................................................................................................................*\n",
      "optimization finished, #iter = 2018716\n",
      "nu = 0.482617\n",
      "obj = -7933725.305061, rho = 2.269214\n",
      "nSV = 1214, nBSV = 1205\n",
      "Total nSV = 1214\n",
      "Accuracy = 79.1069% (496/627) (classification)\n",
      "Accuracy = 46.4559% (485/1044) (classification)\n",
      "..........................................................................................................*...................*...........................................................................................................................................................................................................................................................*................................................................................................................................................*........................................................................................................................................................................................................................................................................................................................................................................*.................................................................................................................................................................................................................*...............................................................................................................................................................................................................................................................................................................................................................................................................................*............................................................................................................................................................*.............................................................................................................................................................................................................................................................*.......................................................................................*..................................................................................................................................................................*......................................................................................................................................................................................................................................................................................................................................................*.......................*\n",
      "optimization finished, #iter = 2521743\n",
      "nu = 0.476845\n",
      "obj = -7838995.722677, rho = -4.642677\n",
      "nSV = 1199, nBSV = 1189\n",
      "Total nSV = 1199\n",
      "Accuracy = 78.3094% (491/627) (classification)\n",
      "Accuracy = 57.4713% (600/1044) (classification)\n",
      "....................................................................................*............................................................................*........................................................*...........................................................................................................................................................................*..................................................................................................*...............................................................................................................................................................................................................................................................................*..........................................................................................................................................................................................................................*.........................................................................................................................................................................................................*...............................................................................................................................................................................................................................................................*\n",
      "optimization finished, #iter = 1426100\n",
      "nu = 0.481311\n",
      "obj = -7912518.019540, rho = -4.587731\n",
      "nSV = 1211, nBSV = 1201\n",
      "Total nSV = 1211\n",
      "Accuracy = 80.2233% (503/627) (classification)\n",
      "Accuracy = 59.9617% (626/1044) (classification)\n",
      ".................................................................................................................................................................................................................................................................*..................................................................................................*..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*.......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*................................................................................................................................................................................................................................................................................................................................................................................*........................................................................................................................................................................................................................................................*...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*......................................................................................................................................................................*\n",
      "optimization finished, #iter = 5792030\n",
      "nu = 0.492673\n",
      "obj = -24298343.208613, rho = 2.159983\n",
      "nSV = 1239, nBSV = 1229\n",
      "Total nSV = 1239\n",
      "Accuracy = 81.4992% (511/627) (classification)\n",
      "Accuracy = 46.7433% (488/1044) (classification)\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*................................................*.................................................................................................................................................................................................................................................................................................................................................*....................................................................................................................................................................................................................................................................*.............................................................................................................................................................................................................................................................................................................................................................................................................................................*.....................................................................................................................................................................................................................................................................................................................................................................................................................................................................*.................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*.........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*......................................................................................................................................................................................................................................................................................................................................*...................................*................................................................................................*\n",
      "optimization finished, #iter = 4853838\n",
      "nu = 0.493092\n",
      "obj = -24317653.843592, rho = 2.444272\n",
      "nSV = 1244, nBSV = 1232\n",
      "Total nSV = 1244\n",
      "Accuracy = 81.8182% (513/627) (classification)\n",
      "Accuracy = 47.4138% (495/1044) (classification)\n",
      "...........................................................................................................................................................................................................................................................................................................................................................................................*.........................................................................*.....................................................................................................................................................................................................................................................................................................................................................................................................*.........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*...............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*.............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*................................................................................................................................................................................................*.............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*\n",
      "optimization finished, #iter = 5986557\n",
      "nu = 0.478843\n",
      "obj = -23614300.169597, rho = -3.594593\n",
      "nSV = 1204, nBSV = 1195\n",
      "Total nSV = 1204\n",
      "Accuracy = 78.6284% (493/627) (classification)\n",
      "Accuracy = 50.3831% (526/1044) (classification)\n",
      ".............................................................................................................................................................................................................................................................................................*........................................................................................*............................................................................................................................................................................................*...............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*...........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*...............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*...............................................................................................................................................................................................................*\n",
      "optimization finished, #iter = 6218827\n",
      "nu = 0.487712\n",
      "obj = -24054163.334169, rho = 2.646414\n",
      "nSV = 1227, nBSV = 1217\n",
      "Total nSV = 1227\n",
      "Accuracy = 80.8612% (507/627) (classification)\n",
      "Accuracy = 47.1264% (492/1044) (classification)\n",
      "...................................................................................................................................................................................................................................................................................................*.........*.................................................................................................................................................................................................................................................................................................................................................................................................................*........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*...............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*.....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*.......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*....*\n",
      "optimization finished, #iter = 8141285\n",
      "nu = 0.479291\n",
      "obj = -23638696.823249, rho = 3.758907\n",
      "nSV = 1207, nBSV = 1197\n",
      "Total nSV = 1207\n",
      "Accuracy = 78.6284% (493/627) (classification)\n",
      "Accuracy = 48.5632% (507/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1231\n",
      "nu = 0.982442\n",
      "obj = -0.041679, rho = -0.995514\n",
      "nSV = 2462, nBSV = 2462\n",
      "Total nSV = 2462\n",
      "Accuracy = 51.5152% (323/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1229\n",
      "nu = 0.975259\n",
      "obj = -0.041375, rho = 0.995870\n",
      "nSV = 2444, nBSV = 2444\n",
      "Total nSV = 2444\n",
      "Accuracy = 50.0797% (314/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1225\n",
      "nu = 0.972067\n",
      "obj = -0.041240, rho = 0.995942\n",
      "nSV = 2436, nBSV = 2436\n",
      "Total nSV = 2436\n",
      "Accuracy = 49.4418% (310/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1249\n",
      "nu = 0.996808\n",
      "obj = -0.042286, rho = -0.994750\n",
      "nSV = 2498, nBSV = 2498\n",
      "Total nSV = 2498\n",
      "Accuracy = 54.386% (341/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1233\n",
      "nu = 0.984038\n",
      "obj = -0.041746, rho = -0.995205\n",
      "nSV = 2466, nBSV = 2466\n",
      "Total nSV = 2466\n",
      "Accuracy = 51.8341% (325/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1225\n",
      "nu = 0.976856\n",
      "obj = -0.124246, rho = -0.987599\n",
      "nSV = 2448, nBSV = 2448\n",
      "Total nSV = 2448\n",
      "Accuracy = 50.3987% (316/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1229\n",
      "nu = 0.980846\n",
      "obj = -0.124746, rho = -0.986857\n",
      "nSV = 2458, nBSV = 2458\n",
      "Total nSV = 2458\n",
      "Accuracy = 51.1962% (321/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1206\n",
      "nu = 0.961692\n",
      "obj = -0.122331, rho = -0.988927\n",
      "nSV = 2410, nBSV = 2410\n",
      "Total nSV = 2410\n",
      "Accuracy = 47.3684% (297/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1251\n",
      "nu = 0.998404\n",
      "obj = -0.126967, rho = -0.984509\n",
      "nSV = 2502, nBSV = 2502\n",
      "Total nSV = 2502\n",
      "Accuracy = 54.7049% (343/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1241\n",
      "nu = 0.989625\n",
      "obj = -0.125848, rho = -0.985311\n",
      "nSV = 2480, nBSV = 2480\n",
      "Total nSV = 2480\n",
      "Accuracy = 52.9506% (332/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1236\n",
      "nu = 0.984836\n",
      "obj = -0.374899, rho = 0.958803\n",
      "nSV = 2468, nBSV = 2468\n",
      "Total nSV = 2468\n",
      "Accuracy = 51.9936% (326/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1226\n",
      "nu = 0.978452\n",
      "obj = -0.372563, rho = -0.962146\n",
      "nSV = 2452, nBSV = 2452\n",
      "Total nSV = 2452\n",
      "Accuracy = 50.7177% (318/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1226\n",
      "nu = 0.978452\n",
      "obj = -0.372558, rho = -0.961854\n",
      "nSV = 2452, nBSV = 2452\n",
      "Total nSV = 2452\n",
      "Accuracy = 50.7177% (318/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1246\n",
      "nu = 0.993615\n",
      "obj = -0.378174, rho = 0.955739\n",
      "nSV = 2490, nBSV = 2490\n",
      "Total nSV = 2490\n",
      "Accuracy = 53.748% (337/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1235\n",
      "nu = 0.984836\n",
      "obj = -0.374847, rho = -0.957790\n",
      "nSV = 2468, nBSV = 2468\n",
      "Total nSV = 2468\n",
      "Accuracy = 51.9936% (326/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1227\n",
      "nu = 0.976057\n",
      "obj = -1.107352, rho = 0.882793\n",
      "nSV = 2446, nBSV = 2446\n",
      "Total nSV = 2446\n",
      "Accuracy = 50.2392% (315/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1233\n",
      "nu = 0.983240\n",
      "obj = -1.116051, rho = -0.882709\n",
      "nSV = 2464, nBSV = 2464\n",
      "Total nSV = 2464\n",
      "Accuracy = 51.6746% (324/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1228\n",
      "nu = 0.977654\n",
      "obj = -1.110031, rho = 0.886017\n",
      "nSV = 2450, nBSV = 2450\n",
      "Total nSV = 2450\n",
      "Accuracy = 50.5582% (317/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1221\n",
      "nu = 0.972865\n",
      "obj = -1.105121, rho = -0.893506\n",
      "nSV = 2438, nBSV = 2438\n",
      "Total nSV = 2438\n",
      "Accuracy = 49.6013% (311/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1212\n",
      "nu = 0.966480\n",
      "obj = -1.098111, rho = -0.896623\n",
      "nSV = 2422, nBSV = 2422\n",
      "Total nSV = 2422\n",
      "Accuracy = 48.3254% (303/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1239\n",
      "nu = 0.988029\n",
      "obj = -3.289944, rho = -0.617273\n",
      "nSV = 2476, nBSV = 2476\n",
      "Total nSV = 2476\n",
      "Accuracy = 52.6316% (330/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1233\n",
      "nu = 0.984038\n",
      "obj = -3.284762, rho = 0.635572\n",
      "nSV = 2466, nBSV = 2466\n",
      "Total nSV = 2466\n",
      "Accuracy = 51.8341% (325/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1243\n",
      "nu = 0.992019\n",
      "obj = -3.298924, rho = -0.594492\n",
      "nSV = 2486, nBSV = 2486\n",
      "Total nSV = 2486\n",
      "Accuracy = 53.429% (335/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1229\n",
      "nu = 0.980846\n",
      "obj = -3.270116, rho = 0.638240\n",
      "nSV = 2458, nBSV = 2458\n",
      "Total nSV = 2458\n",
      "Accuracy = 51.1962% (321/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1201\n",
      "nu = 0.957702\n",
      "obj = -3.213206, rho = -0.711744\n",
      "nSV = 2400, nBSV = 2400\n",
      "Total nSV = 2400\n",
      "Accuracy = 46.571% (292/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1242\n",
      "nu = 0.991221\n",
      "obj = -9.224224, rho = 0.204372\n",
      "nSV = 2484, nBSV = 2484\n",
      "Total nSV = 2484\n",
      "Accuracy = 69.5375% (436/627) (classification)\n",
      "Accuracy = 68.6782% (717/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1245\n",
      "nu = 0.993615\n",
      "obj = -9.217341, rho = -0.234923\n",
      "nSV = 2490, nBSV = 2490\n",
      "Total nSV = 2490\n",
      "Accuracy = 69.378% (435/627) (classification)\n",
      "Accuracy = 67.3372% (703/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1222\n",
      "nu = 0.975259\n",
      "obj = -9.234975, rho = -0.007717\n",
      "nSV = 2444, nBSV = 2444\n",
      "Total nSV = 2444\n",
      "Accuracy = 69.5375% (436/627) (classification)\n",
      "Accuracy = 59.0038% (616/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1231\n",
      "nu = 0.982442\n",
      "obj = -9.235150, rho = -0.080879\n",
      "nSV = 2462, nBSV = 2462\n",
      "Total nSV = 2462\n",
      "Accuracy = 70.0159% (439/627) (classification)\n",
      "Accuracy = 70.1149% (732/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1233\n",
      "nu = 0.984038\n",
      "obj = -9.225264, rho = -0.107526\n",
      "nSV = 2466, nBSV = 2466\n",
      "Total nSV = 2466\n",
      "Accuracy = 71.2919% (447/627) (classification)\n",
      "Accuracy = 70.5939% (737/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1135\n",
      "nu = 0.879489\n",
      "obj = -23.626562, rho = 0.822931\n",
      "nSV = 2204, nBSV = 2204\n",
      "Total nSV = 2204\n",
      "Accuracy = 72.7273% (456/627) (classification)\n",
      "Accuracy = 59.387% (620/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1137\n",
      "nu = 0.881519\n",
      "obj = -23.633960, rho = 0.833106\n",
      "nSV = 2210, nBSV = 2208\n",
      "Total nSV = 2210\n",
      "Accuracy = 72.4083% (454/627) (classification)\n",
      "Accuracy = 59.0038% (616/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1131\n",
      "nu = 0.878691\n",
      "obj = -23.559042, rho = -0.814227\n",
      "nSV = 2202, nBSV = 2202\n",
      "Total nSV = 2202\n",
      "Accuracy = 72.7273% (456/627) (classification)\n",
      "Accuracy = 59.2912% (619/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1132\n",
      "nu = 0.884597\n",
      "obj = -23.733425, rho = -0.788610\n",
      "nSV = 2218, nBSV = 2216\n",
      "Total nSV = 2218\n",
      "Accuracy = 74.1627% (465/627) (classification)\n",
      "Accuracy = 59.7701% (624/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1127\n",
      "nu = 0.877095\n",
      "obj = -23.539478, rho = -0.817628\n",
      "nSV = 2198, nBSV = 2198\n",
      "Total nSV = 2198\n",
      "Accuracy = 70.1754% (440/627) (classification)\n",
      "Accuracy = 58.908% (615/1044) (classification)\n",
      "*.\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1006\n",
      "nu = 0.732030\n",
      "obj = -61.953203, rho = 0.892378\n",
      "nSV = 1836, nBSV = 1833\n",
      "Total nSV = 1836\n",
      "Accuracy = 73.0463% (458/627) (classification)\n",
      "Accuracy = 62.6437% (654/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 997\n",
      "nu = 0.732919\n",
      "obj = -61.758672, rho = 0.895181\n",
      "nSV = 1838, nBSV = 1836\n",
      "Total nSV = 1838\n",
      "Accuracy = 73.8437% (463/627) (classification)\n",
      "Accuracy = 63.41% (662/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 984\n",
      "nu = 0.726257\n",
      "obj = -61.314762, rho = -0.903756\n",
      "nSV = 1822, nBSV = 1817\n",
      "Total nSV = 1822\n",
      "Accuracy = 72.0893% (452/627) (classification)\n",
      "Accuracy = 62.4521% (652/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 996\n",
      "nu = 0.725557\n",
      "obj = -61.396285, rho = -0.911559\n",
      "nSV = 1820, nBSV = 1817\n",
      "Total nSV = 1820\n",
      "Accuracy = 71.6108% (449/627) (classification)\n",
      "Accuracy = 62.6437% (654/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 974\n",
      "nu = 0.728374\n",
      "obj = -61.230419, rho = -0.895133\n",
      "nSV = 1827, nBSV = 1823\n",
      "Total nSV = 1827\n",
      "Accuracy = 72.5678% (455/627) (classification)\n",
      "Accuracy = 63.6973% (665/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 930\n",
      "nu = 0.643156\n",
      "obj = -168.942377, rho = 1.008069\n",
      "nSV = 1614, nBSV = 1609\n",
      "Total nSV = 1614\n",
      "Accuracy = 73.8437% (463/627) (classification)\n",
      "Accuracy = 65.3257% (682/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 933\n",
      "nu = 0.652405\n",
      "obj = -171.658781, rho = 1.026812\n",
      "nSV = 1638, nBSV = 1634\n",
      "Total nSV = 1638\n",
      "Accuracy = 75.4386% (473/627) (classification)\n",
      "Accuracy = 64.751% (676/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 906\n",
      "nu = 0.639625\n",
      "obj = -167.536028, rho = -1.026572\n",
      "nSV = 1606, nBSV = 1599\n",
      "Total nSV = 1606\n",
      "Accuracy = 72.2488% (453/627) (classification)\n",
      "Accuracy = 65.3257% (682/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 912\n",
      "nu = 0.643502\n",
      "obj = -168.988617, rho = 1.018088\n",
      "nSV = 1616, nBSV = 1610\n",
      "Total nSV = 1616\n",
      "Accuracy = 72.7273% (456/627) (classification)\n",
      "Accuracy = 65.2299% (681/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 914\n",
      "nu = 0.634594\n",
      "obj = -166.587393, rho = -1.025197\n",
      "nSV = 1592, nBSV = 1588\n",
      "Total nSV = 1592\n",
      "Accuracy = 71.2919% (447/627) (classification)\n",
      "Accuracy = 65.2299% (681/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 906\n",
      "nu = 0.606862\n",
      "obj = -488.276393, rho = 1.182202\n",
      "nSV = 1524, nBSV = 1517\n",
      "Total nSV = 1524\n",
      "Accuracy = 76.3955% (479/627) (classification)\n",
      "Accuracy = 65.3257% (682/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 872\n",
      "nu = 0.597308\n",
      "obj = -479.530243, rho = -1.140012\n",
      "nSV = 1500, nBSV = 1491\n",
      "Total nSV = 1500\n",
      "Accuracy = 74.0032% (464/627) (classification)\n",
      "Accuracy = 66.6667% (696/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 910\n",
      "nu = 0.596147\n",
      "obj = -479.157483, rho = 1.126694\n",
      "nSV = 1500, nBSV = 1490\n",
      "Total nSV = 1500\n",
      "Accuracy = 74.6411% (468/627) (classification)\n",
      "Accuracy = 65.9962% (689/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 922\n",
      "nu = 0.609095\n",
      "obj = -489.581178, rho = 1.158351\n",
      "nSV = 1529, nBSV = 1524\n",
      "Total nSV = 1529\n",
      "Accuracy = 75.9171% (476/627) (classification)\n",
      "Accuracy = 65.5172% (684/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 884\n",
      "nu = 0.592839\n",
      "obj = -476.538043, rho = -1.196516\n",
      "nSV = 1488, nBSV = 1483\n",
      "Total nSV = 1488\n",
      "Accuracy = 73.0463% (458/627) (classification)\n",
      "Accuracy = 66.1877% (691/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 897\n",
      "nu = 0.568335\n",
      "obj = -1375.849363, rho = -1.348866\n",
      "nSV = 1430, nBSV = 1419\n",
      "Total nSV = 1430\n",
      "Accuracy = 71.6108% (449/627) (classification)\n",
      "Accuracy = 64.9425% (678/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 877\n",
      "nu = 0.571387\n",
      "obj = -1375.388365, rho = 1.332499\n",
      "nSV = 1435, nBSV = 1428\n",
      "Total nSV = 1435\n",
      "Accuracy = 74.0032% (464/627) (classification)\n",
      "Accuracy = 66.6667% (696/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 841\n",
      "nu = 0.564327\n",
      "obj = -1361.045238, rho = -1.293403\n",
      "nSV = 1418, nBSV = 1410\n",
      "Total nSV = 1418\n",
      "Accuracy = 72.2488% (453/627) (classification)\n",
      "Accuracy = 65.613% (685/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 889\n",
      "nu = 0.576182\n",
      "obj = -1390.045355, rho = -1.349809\n",
      "nSV = 1450, nBSV = 1439\n",
      "Total nSV = 1450\n",
      "Accuracy = 74.4817% (467/627) (classification)\n",
      "Accuracy = 66.7625% (697/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 872\n",
      "nu = 0.582844\n",
      "obj = -1406.176197, rho = 1.322866\n",
      "nSV = 1464, nBSV = 1458\n",
      "Total nSV = 1464\n",
      "Accuracy = 76.3955% (479/627) (classification)\n",
      "Accuracy = 68.1034% (711/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 908\n",
      "nu = 0.556212\n",
      "obj = -3985.096962, rho = -1.637071\n",
      "nSV = 1399, nBSV = 1392\n",
      "Total nSV = 1399\n",
      "Accuracy = 77.8309% (488/627) (classification)\n",
      "Accuracy = 72.6054% (758/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 909\n",
      "nu = 0.553722\n",
      "obj = -3958.693632, rho = 1.545216\n",
      "nSV = 1393, nBSV = 1384\n",
      "Total nSV = 1393\n",
      "Accuracy = 76.7145% (481/627) (classification)\n",
      "Accuracy = 72.8927% (761/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 920\n",
      "nu = 0.551839\n",
      "obj = -3958.600901, rho = 1.457602\n",
      "nSV = 1387, nBSV = 1377\n",
      "Total nSV = 1387\n",
      "Accuracy = 76.236% (478/627) (classification)\n",
      "Accuracy = 72.0307% (752/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 997\n",
      "nu = 0.552514\n",
      "obj = -3967.414100, rho = 1.572361\n",
      "nSV = 1391, nBSV = 1382\n",
      "Total nSV = 1391\n",
      "Accuracy = 77.0335% (483/627) (classification)\n",
      "Accuracy = 72.2222% (754/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 922\n",
      "nu = 0.558863\n",
      "obj = -4006.203965, rho = 1.476329\n",
      "nSV = 1405, nBSV = 1395\n",
      "Total nSV = 1405\n",
      "Accuracy = 79.7448% (500/627) (classification)\n",
      "Accuracy = 72.8927% (761/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1095\n",
      "nu = 0.528505\n",
      "obj = -11441.893337, rho = -2.010872\n",
      "nSV = 1329, nBSV = 1320\n",
      "Total nSV = 1329\n",
      "Accuracy = 78.9474% (495/627) (classification)\n",
      "Accuracy = 72.7011% (759/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1145\n",
      "nu = 0.514818\n",
      "obj = -11124.955833, rho = 1.724764\n",
      "nSV = 1295, nBSV = 1284\n",
      "Total nSV = 1295\n",
      "Accuracy = 77.6715% (487/627) (classification)\n",
      "Accuracy = 71.8391% (750/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1265\n",
      "nu = 0.523563\n",
      "obj = -11334.613961, rho = 1.883317\n",
      "nSV = 1318, nBSV = 1308\n",
      "Total nSV = 1318\n",
      "Accuracy = 78.6284% (493/627) (classification)\n",
      "Accuracy = 71.6475% (748/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1655\n",
      "nu = 0.525304\n",
      "obj = -11377.972437, rho = -1.548226\n",
      "nSV = 1326, nBSV = 1311\n",
      "Total nSV = 1326\n",
      "Accuracy = 80.3828% (504/627) (classification)\n",
      "Accuracy = 71.2644% (744/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "**.\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1837\n",
      "nu = 0.513926\n",
      "obj = -11165.200364, rho = -1.727356\n",
      "nSV = 1294, nBSV = 1282\n",
      "Total nSV = 1294\n",
      "Accuracy = 77.3525% (485/627) (classification)\n",
      "Accuracy = 72.1264% (753/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*.\n",
      "WARNING: using -h 0 may be faster\n",
      "*.\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 3131\n",
      "nu = 0.501145\n",
      "obj = -33022.014390, rho = -1.980807\n",
      "nSV = 1263, nBSV = 1248\n",
      "Total nSV = 1263\n",
      "Accuracy = 80.5423% (505/627) (classification)\n",
      "Accuracy = 70.8812% (740/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*.*..*\n",
      "optimization finished, #iter = 3830\n",
      "nu = 0.497647\n",
      "obj = -32792.489677, rho = 1.926800\n",
      "nSV = 1255, nBSV = 1239\n",
      "Total nSV = 1255\n",
      "Accuracy = 79.4258% (498/627) (classification)\n",
      "Accuracy = 69.636% (727/1044) (classification)\n",
      "..*..*\n",
      "optimization finished, #iter = 4598\n",
      "nu = 0.490286\n",
      "obj = -32248.375046, rho = -1.800092\n",
      "nSV = 1239, nBSV = 1220\n",
      "Total nSV = 1239\n",
      "Accuracy = 79.1069% (496/627) (classification)\n",
      "Accuracy = 66.8582% (698/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*.*\n",
      "optimization finished, #iter = 2626\n",
      "nu = 0.488167\n",
      "obj = -32032.905669, rho = 2.076697\n",
      "nSV = 1233, nBSV = 1215\n",
      "Total nSV = 1233\n",
      "Accuracy = 78.3094% (491/627) (classification)\n",
      "Accuracy = 68.7739% (718/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*.*\n",
      "optimization finished, #iter = 2321\n",
      "nu = 0.490415\n",
      "obj = -32252.764790, rho = -2.103778\n",
      "nSV = 1238, nBSV = 1220\n",
      "Total nSV = 1238\n",
      "Accuracy = 78.4689% (492/627) (classification)\n",
      "Accuracy = 69.5402% (726/1044) (classification)\n",
      "...*.*\n",
      "optimization finished, #iter = 4008\n",
      "nu = 0.477042\n",
      "obj = -95306.524033, rho = -2.179285\n",
      "nSV = 1205, nBSV = 1189\n",
      "Total nSV = 1205\n",
      "Accuracy = 80.2233% (503/627) (classification)\n",
      "Accuracy = 69.7318% (728/1044) (classification)\n",
      "....*...*.*\n",
      "optimization finished, #iter = 8410\n",
      "nu = 0.478754\n",
      "obj = -95639.534071, rho = 1.665288\n",
      "nSV = 1211, nBSV = 1190\n",
      "Total nSV = 1211\n",
      "Accuracy = 80.7018% (506/627) (classification)\n",
      "Accuracy = 59.1954% (618/1044) (classification)\n",
      ".....*...*...*\n",
      "optimization finished, #iter = 10271\n",
      "nu = 0.470794\n",
      "obj = -93775.203293, rho = -1.612287\n",
      "nSV = 1189, nBSV = 1170\n",
      "Total nSV = 1189\n",
      "Accuracy = 77.9904% (489/627) (classification)\n",
      "Accuracy = 59.4828% (621/1044) (classification)\n",
      "...\n",
      "WARNING: using -h 0 may be faster\n",
      "*.....*\n",
      "optimization finished, #iter = 8717\n",
      "nu = 0.474107\n",
      "obj = -94596.993729, rho = 1.879396\n",
      "nSV = 1197, nBSV = 1175\n",
      "Total nSV = 1197\n",
      "Accuracy = 78.4689% (492/627) (classification)\n",
      "Accuracy = 55.1724% (576/1044) (classification)\n",
      "...\n",
      "WARNING: using -h 0 may be faster\n",
      "*..*....*\n",
      "optimization finished, #iter = 9392\n",
      "nu = 0.477521\n",
      "obj = -95412.161784, rho = 1.747767\n",
      "nSV = 1208, nBSV = 1190\n",
      "Total nSV = 1208\n",
      "Accuracy = 79.4258% (498/627) (classification)\n",
      "Accuracy = 52.4904% (548/1044) (classification)\n",
      ".........*.............*...*\n",
      "optimization finished, #iter = 25746\n",
      "nu = 0.481454\n",
      "obj = -290063.886705, rho = -1.408711\n",
      "nSV = 1221, nBSV = 1195\n",
      "Total nSV = 1221\n",
      "Accuracy = 83.2536% (522/627) (classification)\n",
      "Accuracy = 48.5632% (507/1044) (classification)\n",
      ".........*.......*........*\n",
      "optimization finished, #iter = 23033\n",
      "nu = 0.463632\n",
      "obj = -279325.203736, rho = -2.191650\n",
      "nSV = 1172, nBSV = 1152\n",
      "Total nSV = 1172\n",
      "Accuracy = 79.2663% (497/627) (classification)\n",
      "Accuracy = 49.3295% (515/1044) (classification)\n",
      ".........*...*.......*....*\n",
      "optimization finished, #iter = 23013\n",
      "nu = 0.467852\n",
      "obj = -281871.451224, rho = 1.437436\n",
      "nSV = 1182, nBSV = 1159\n",
      "Total nSV = 1182\n",
      "Accuracy = 81.3397% (510/627) (classification)\n",
      "Accuracy = 48.659% (508/1044) (classification)\n",
      "...........*.....*\n",
      "optimization finished, #iter = 16485\n",
      "nu = 0.478192\n",
      "obj = -287934.607126, rho = -1.798509\n",
      "nSV = 1206, nBSV = 1184\n",
      "Total nSV = 1206\n",
      "Accuracy = 82.4561% (517/627) (classification)\n",
      "Accuracy = 49.8084% (520/1044) (classification)\n",
      "......*..........*......*\n",
      "optimization finished, #iter = 21392\n",
      "nu = 0.470338\n",
      "obj = -283357.399645, rho = -1.513147\n",
      "nSV = 1192, nBSV = 1170\n",
      "Total nSV = 1192\n",
      "Accuracy = 80.3828% (504/627) (classification)\n",
      "Accuracy = 50.7663% (530/1044) (classification)\n",
      "..........................*...........................*..............*\n",
      "optimization finished, #iter = 66847\n",
      "nu = 0.471790\n",
      "obj = -855489.553162, rho = -1.499875\n",
      "nSV = 1193, nBSV = 1169\n",
      "Total nSV = 1193\n",
      "Accuracy = 81.4992% (511/627) (classification)\n",
      "Accuracy = 47.0307% (491/1044) (classification)\n",
      "..................*.....................*.......................................*...*\n",
      "optimization finished, #iter = 80633\n",
      "nu = 0.466321\n",
      "obj = -847318.495049, rho = 1.330374\n",
      "nSV = 1182, nBSV = 1156\n",
      "Total nSV = 1182\n",
      "Accuracy = 80.0638% (502/627) (classification)\n",
      "Accuracy = 51.341% (536/1044) (classification)\n",
      "......................*.................*........................*.........*.........*\n",
      "optimization finished, #iter = 80087\n",
      "nu = 0.463852\n",
      "obj = -841283.974537, rho = 1.952684\n",
      "nSV = 1174, nBSV = 1148\n",
      "Total nSV = 1174\n",
      "Accuracy = 80.8612% (507/627) (classification)\n",
      "Accuracy = 47.6054% (497/1044) (classification)\n",
      ".........................*............................*...................................*......................*\n",
      "optimization finished, #iter = 109910\n",
      "nu = 0.458640\n",
      "obj = -833087.021464, rho = -1.664185\n",
      "nSV = 1162, nBSV = 1134\n",
      "Total nSV = 1162\n",
      "Accuracy = 80.0638% (502/627) (classification)\n",
      "Accuracy = 51.1494% (534/1044) (classification)\n",
      "......................*...........*..........................................*...........*\n",
      "optimization finished, #iter = 85862\n",
      "nu = 0.474910\n",
      "obj = -861640.944948, rho = -1.742278\n",
      "nSV = 1202, nBSV = 1179\n",
      "Total nSV = 1202\n",
      "Accuracy = 83.4131% (523/627) (classification)\n",
      "Accuracy = 48.4674% (506/1044) (classification)\n",
      "........................................................*...............................*.....................................................................................................................*.*\n",
      "optimization finished, #iter = 204020\n",
      "nu = 0.456869\n",
      "obj = -2493301.651902, rho = 2.488135\n",
      "nSV = 1160, nBSV = 1130\n",
      "Total nSV = 1160\n",
      "Accuracy = 79.4258% (498/627) (classification)\n",
      "Accuracy = 50.8621% (531/1044) (classification)\n",
      "...............................*.......................*..................................*.......................................................................................................*......................................................................................................*.............................*.....*\n",
      "optimization finished, #iter = 325425\n",
      "nu = 0.454149\n",
      "obj = -2481351.282320, rho = 1.660218\n",
      "nSV = 1156, nBSV = 1125\n",
      "Total nSV = 1156\n",
      "Accuracy = 80.5423% (505/627) (classification)\n",
      "Accuracy = 48.5632% (507/1044) (classification)\n",
      ".......................................................*.......................................................*...........................................................................*.........*\n",
      "optimization finished, #iter = 193638\n",
      "nu = 0.453932\n",
      "obj = -2478107.294589, rho = -1.539897\n",
      "nSV = 1156, nBSV = 1125\n",
      "Total nSV = 1156\n",
      "Accuracy = 77.6715% (487/627) (classification)\n",
      "Accuracy = 63.8889% (667/1044) (classification)\n",
      "........................................................*.............................*.................................................................*\n",
      "optimization finished, #iter = 149414\n",
      "nu = 0.457519\n",
      "obj = -2499089.238700, rho = 1.093081\n",
      "nSV = 1163, nBSV = 1134\n",
      "Total nSV = 1163\n",
      "Accuracy = 80.8612% (507/627) (classification)\n",
      "Accuracy = 47.6054% (497/1044) (classification)\n",
      ".................................................*.................................*.........................................................*.................................................*....................................*\n",
      "optimization finished, #iter = 223833\n",
      "nu = 0.461821\n",
      "obj = -2519832.271287, rho = -1.781641\n",
      "nSV = 1167, nBSV = 1142\n",
      "Total nSV = 1167\n",
      "Accuracy = 79.1069% (496/627) (classification)\n",
      "Accuracy = 50.6705% (529/1044) (classification)\n",
      ".....................................................................................................................................................................*....................................................................*..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*..................................................................................*................*\n",
      "optimization finished, #iter = 871829\n",
      "nu = 0.462312\n",
      "obj = -7577231.904669, rho = -1.675946\n",
      "nSV = 1179, nBSV = 1143\n",
      "Total nSV = 1179\n",
      "Accuracy = 81.4992% (511/627) (classification)\n",
      "Accuracy = 50.0958% (523/1044) (classification)\n",
      ".......................................................................................................................................................................................................*...................................................................*...............................................................................................................................................................*...........................................................................................................................................................................................*....................................*\n",
      "optimization finished, #iter = 646888\n",
      "nu = 0.459216\n",
      "obj = -7537208.305229, rho = 1.514976\n",
      "nSV = 1168, nBSV = 1137\n",
      "Total nSV = 1168\n",
      "Accuracy = 81.6587% (512/627) (classification)\n",
      "Accuracy = 49.8084% (520/1044) (classification)\n",
      ".............................................................................................................................................................*..........................................................................................................................*................................................................................................................................................................................................................................................*.......................................................................................................*\n",
      "optimization finished, #iter = 621077\n",
      "nu = 0.455740\n",
      "obj = -7472556.676616, rho = -1.160335\n",
      "nSV = 1161, nBSV = 1128\n",
      "Total nSV = 1161\n",
      "Accuracy = 78.9474% (495/627) (classification)\n",
      "Accuracy = 47.7969% (499/1044) (classification)\n",
      "...................................................................................................................................*..............................................*.....................................................................................................................................................................................................................................................*...........................................................................................................................................................................................*......................................................................................................................................................*.......*\n",
      "optimization finished, #iter = 764394\n",
      "nu = 0.447675\n",
      "obj = -7342933.700922, rho = 2.133194\n",
      "nSV = 1143, nBSV = 1107\n",
      "Total nSV = 1143\n",
      "Accuracy = 77.8309% (488/627) (classification)\n",
      "Accuracy = 47.9885% (501/1044) (classification)\n",
      "......................................................................................................................................................*..............................................................*..........................................................................................................................................................................................................................................*............................................................................................................................*..........................*\n",
      "optimization finished, #iter = 594974\n",
      "nu = 0.460764\n",
      "obj = -7563749.798033, rho = -1.766622\n",
      "nSV = 1169, nBSV = 1140\n",
      "Total nSV = 1169\n",
      "Accuracy = 80.7018% (506/627) (classification)\n",
      "Accuracy = 49.0421% (512/1044) (classification)\n",
      "..............................................................................................................................................................................................................................................................................*.............................................................................................................................................................................................*............................................................................................................................................................................................................................................................................................................................................................................................................................*................................................................................................................................................................................................................................................................................................................................................................*....................................................................................................................................................................................................................................................................................................................................................................................................................*...........................................................................................................................................................................................................................................................................................................................................................................................*\n",
      "optimization finished, #iter = 2004163\n",
      "nu = 0.457229\n",
      "obj = -22527097.712890, rho = -1.973540\n",
      "nSV = 1164, nBSV = 1129\n",
      "Total nSV = 1164\n",
      "Accuracy = 80.2233% (503/627) (classification)\n",
      "Accuracy = 46.6475% (487/1044) (classification)\n",
      "..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*...............................................................................................................................................................................................................*...........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*...............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*............................................................................................................................................................................................................................................................................................................................................................................................*...............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*\n",
      "optimization finished, #iter = 3168204\n",
      "nu = 0.455409\n",
      "obj = -22443554.874549, rho = -1.960182\n",
      "nSV = 1159, nBSV = 1123\n",
      "Total nSV = 1159\n",
      "Accuracy = 80.5423% (505/627) (classification)\n",
      "Accuracy = 47.2222% (493/1044) (classification)\n",
      "...................................................................................................................................................................................................................................................*.............................................................................................................................................*...............................................................................................................................................................................................................................................................................................................................................................................................................................................................................*.....................................................................................................................................................................................................................................................................................................................................................................................................................................................*...............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*.................................................................................................................*....*\n",
      "optimization finished, #iter = 1877792\n",
      "nu = 0.454264\n",
      "obj = -22370310.504517, rho = 1.888984\n",
      "nSV = 1155, nBSV = 1122\n",
      "Total nSV = 1155\n",
      "Accuracy = 79.7448% (500/627) (classification)\n",
      "Accuracy = 47.6054% (497/1044) (classification)\n",
      ".....................................................................................................................................................................................................................................................................................................................................................................................*............................................................................................................*.......................................................................................................................................................................................................................................................................................................................................................*..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*...................................................................................................................................................................................................................................................................................................................................................................................*\n",
      "optimization finished, #iter = 1704717\n",
      "nu = 0.449273\n",
      "obj = -22140360.705106, rho = 1.811117\n",
      "nSV = 1145, nBSV = 1112\n",
      "Total nSV = 1145\n",
      "Accuracy = 79.1069% (496/627) (classification)\n",
      "Accuracy = 46.4559% (485/1044) (classification)\n",
      ".......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*............................................................................................................................................................................*.....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*.............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*...............................................................................................................................................................................*.....................................................................................................................................................................................................*\n",
      "optimization finished, #iter = 2949165\n",
      "nu = 0.460695\n",
      "obj = -22699598.164613, rho = -1.643879\n",
      "nSV = 1172, nBSV = 1139\n",
      "Total nSV = 1172\n",
      "Accuracy = 80.8612% (507/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1222\n",
      "nu = 0.975259\n",
      "obj = -0.041380, rho = -0.996431\n",
      "nSV = 2444, nBSV = 2444\n",
      "Total nSV = 2444\n",
      "Accuracy = 50.0797% (314/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1220\n",
      "nu = 0.973663\n",
      "obj = -0.041313, rho = -0.996385\n",
      "nSV = 2440, nBSV = 2440\n",
      "Total nSV = 2440\n",
      "Accuracy = 49.7608% (312/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1245\n",
      "nu = 0.993615\n",
      "obj = -0.042157, rho = 0.995822\n",
      "nSV = 2490, nBSV = 2490\n",
      "Total nSV = 2490\n",
      "Accuracy = 53.748% (337/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1240\n",
      "nu = 0.988827\n",
      "obj = -0.041955, rho = 0.996246\n",
      "nSV = 2478, nBSV = 2478\n",
      "Total nSV = 2478\n",
      "Accuracy = 52.7911% (331/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1229\n",
      "nu = 0.980846\n",
      "obj = -0.041617, rho = -0.996803\n",
      "nSV = 2458, nBSV = 2458\n",
      "Total nSV = 2458\n",
      "Accuracy = 51.1962% (321/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1226\n",
      "nu = 0.977654\n",
      "obj = -0.124389, rho = -0.990330\n",
      "nSV = 2450, nBSV = 2450\n",
      "Total nSV = 2450\n",
      "Accuracy = 50.5582% (317/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1231\n",
      "nu = 0.978452\n",
      "obj = -0.124486, rho = 0.990022\n",
      "nSV = 2452, nBSV = 2452\n",
      "Total nSV = 2452\n",
      "Accuracy = 50.7177% (318/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1238\n",
      "nu = 0.985634\n",
      "obj = -0.125399, rho = 0.989468\n",
      "nSV = 2470, nBSV = 2470\n",
      "Total nSV = 2470\n",
      "Accuracy = 52.1531% (327/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1216\n",
      "nu = 0.970471\n",
      "obj = -0.123481, rho = -0.991168\n",
      "nSV = 2432, nBSV = 2432\n",
      "Total nSV = 2432\n",
      "Accuracy = 49.1228% (308/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1218\n",
      "nu = 0.971269\n",
      "obj = -0.123580, rho = -0.990999\n",
      "nSV = 2434, nBSV = 2434\n",
      "Total nSV = 2434\n",
      "Accuracy = 49.2823% (309/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1236\n",
      "nu = 0.986433\n",
      "obj = -0.375921, rho = 0.967286\n",
      "nSV = 2472, nBSV = 2472\n",
      "Total nSV = 2472\n",
      "Accuracy = 52.3126% (328/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1248\n",
      "nu = 0.996010\n",
      "obj = -0.379456, rho = -0.960271\n",
      "nSV = 2496, nBSV = 2496\n",
      "Total nSV = 2496\n",
      "Accuracy = 54.2265% (340/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1224\n",
      "nu = 0.976057\n",
      "obj = -0.372062, rho = -0.971480\n",
      "nSV = 2446, nBSV = 2446\n",
      "Total nSV = 2446\n",
      "Accuracy = 50.2392% (315/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1241\n",
      "nu = 0.990423\n",
      "obj = -0.377430, rho = -0.965819\n",
      "nSV = 2482, nBSV = 2482\n",
      "Total nSV = 2482\n",
      "Accuracy = 53.11% (333/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1219\n",
      "nu = 0.972067\n",
      "obj = -0.370564, rho = -0.972346\n",
      "nSV = 2436, nBSV = 2436\n",
      "Total nSV = 2436\n",
      "Accuracy = 49.4418% (310/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1221\n",
      "nu = 0.968875\n",
      "obj = -1.104250, rho = 0.922995\n",
      "nSV = 2428, nBSV = 2428\n",
      "Total nSV = 2428\n",
      "Accuracy = 48.8038% (306/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1249\n",
      "nu = 0.996010\n",
      "obj = -1.133022, rho = -0.888127\n",
      "nSV = 2496, nBSV = 2496\n",
      "Total nSV = 2496\n",
      "Accuracy = 54.2265% (340/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1217\n",
      "nu = 0.969673\n",
      "obj = -1.105040, rho = -0.922112\n",
      "nSV = 2430, nBSV = 2430\n",
      "Total nSV = 2430\n",
      "Accuracy = 48.9633% (307/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1242\n",
      "nu = 0.991221\n",
      "obj = -1.127888, rho = -0.895820\n",
      "nSV = 2484, nBSV = 2484\n",
      "Total nSV = 2484\n",
      "Accuracy = 53.2695% (334/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1236\n",
      "nu = 0.985634\n",
      "obj = -1.121943, rho = -0.902491\n",
      "nSV = 2470, nBSV = 2470\n",
      "Total nSV = 2470\n",
      "Accuracy = 52.1531% (327/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1226\n",
      "nu = 0.977654\n",
      "obj = -3.297821, rho = -0.733514\n",
      "nSV = 2450, nBSV = 2450\n",
      "Total nSV = 2450\n",
      "Accuracy = 50.5582% (317/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1230\n",
      "nu = 0.980846\n",
      "obj = -3.309424, rho = 0.728449\n",
      "nSV = 2458, nBSV = 2458\n",
      "Total nSV = 2458\n",
      "Accuracy = 51.1962% (321/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1232\n",
      "nu = 0.983240\n",
      "obj = -3.313998, rho = 0.720641\n",
      "nSV = 2464, nBSV = 2464\n",
      "Total nSV = 2464\n",
      "Accuracy = 51.6746% (324/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1223\n",
      "nu = 0.976057\n",
      "obj = -3.295213, rho = -0.748399\n",
      "nSV = 2446, nBSV = 2446\n",
      "Total nSV = 2446\n",
      "Accuracy = 50.2392% (315/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1247\n",
      "nu = 0.994413\n",
      "obj = -3.341238, rho = -0.657563\n",
      "nSV = 2492, nBSV = 2492\n",
      "Total nSV = 2492\n",
      "Accuracy = 53.9075% (338/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1215\n",
      "nu = 0.969673\n",
      "obj = -9.495787, rho = 0.290896\n",
      "nSV = 2430, nBSV = 2430\n",
      "Total nSV = 2430\n",
      "Accuracy = 48.9633% (307/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1231\n",
      "nu = 0.982442\n",
      "obj = -9.549181, rho = -0.182349\n",
      "nSV = 2462, nBSV = 2462\n",
      "Total nSV = 2462\n",
      "Accuracy = 51.5152% (323/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1229\n",
      "nu = 0.980846\n",
      "obj = -9.489746, rho = 0.146723\n",
      "nSV = 2458, nBSV = 2458\n",
      "Total nSV = 2458\n",
      "Accuracy = 51.3557% (322/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1249\n",
      "nu = 0.996010\n",
      "obj = -9.565030, rho = 0.060611\n",
      "nSV = 2496, nBSV = 2496\n",
      "Total nSV = 2496\n",
      "Accuracy = 71.7703% (450/627) (classification)\n",
      "Accuracy = 70.1149% (732/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1231\n",
      "nu = 0.982442\n",
      "obj = -9.565840, rho = -0.174629\n",
      "nSV = 2462, nBSV = 2462\n",
      "Total nSV = 2462\n",
      "Accuracy = 51.5152% (323/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1181\n",
      "nu = 0.937688\n",
      "obj = -25.169570, rho = -0.902555\n",
      "nSV = 2350, nBSV = 2348\n",
      "Total nSV = 2350\n",
      "Accuracy = 65.7097% (412/627) (classification)\n",
      "Accuracy = 53.2567% (556/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1176\n",
      "nu = 0.936367\n",
      "obj = -25.004370, rho = 0.893745\n",
      "nSV = 2348, nBSV = 2346\n",
      "Total nSV = 2348\n",
      "Accuracy = 65.2313% (409/627) (classification)\n",
      "Accuracy = 53.3525% (557/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1179\n",
      "nu = 0.932660\n",
      "obj = -25.108958, rho = 0.907481\n",
      "nSV = 2338, nBSV = 2336\n",
      "Total nSV = 2338\n",
      "Accuracy = 64.9123% (407/627) (classification)\n",
      "Accuracy = 52.9693% (553/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1189\n",
      "nu = 0.944932\n",
      "obj = -25.436846, rho = 0.885787\n",
      "nSV = 2368, nBSV = 2368\n",
      "Total nSV = 2368\n",
      "Accuracy = 67.7831% (425/627) (classification)\n",
      "Accuracy = 53.2567% (556/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1189\n",
      "nu = 0.948068\n",
      "obj = -25.535375, rho = -0.802465\n",
      "nSV = 2376, nBSV = 2374\n",
      "Total nSV = 2376\n",
      "Accuracy = 71.2919% (447/627) (classification)\n",
      "Accuracy = 54.2146% (566/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1020\n",
      "nu = 0.785651\n",
      "obj = -64.804574, rho = -0.952265\n",
      "nSV = 1970, nBSV = 1968\n",
      "Total nSV = 1970\n",
      "Accuracy = 71.6108% (449/627) (classification)\n",
      "Accuracy = 56.9923% (595/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1048\n",
      "nu = 0.794236\n",
      "obj = -65.740414, rho = 0.956596\n",
      "nSV = 1992, nBSV = 1990\n",
      "Total nSV = 1992\n",
      "Accuracy = 72.7273% (456/627) (classification)\n",
      "Accuracy = 56.5134% (590/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1046\n",
      "nu = 0.781325\n",
      "obj = -64.412787, rho = 0.959061\n",
      "nSV = 1959, nBSV = 1957\n",
      "Total nSV = 1959\n",
      "Accuracy = 69.697% (437/627) (classification)\n",
      "Accuracy = 56.4176% (589/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1050\n",
      "nu = 0.783074\n",
      "obj = -64.551317, rho = 0.957409\n",
      "nSV = 1965, nBSV = 1960\n",
      "Total nSV = 1965\n",
      "Accuracy = 70.4944% (442/627) (classification)\n",
      "Accuracy = 56.5134% (590/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1045\n",
      "nu = 0.794892\n",
      "obj = -65.867575, rho = 0.950695\n",
      "nSV = 1992, nBSV = 1992\n",
      "Total nSV = 1992\n",
      "Accuracy = 74.0032% (464/627) (classification)\n",
      "Accuracy = 56.4176% (589/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 957\n",
      "nu = 0.685560\n",
      "obj = -176.648700, rho = 0.984457\n",
      "nSV = 1721, nBSV = 1715\n",
      "Total nSV = 1721\n",
      "Accuracy = 74.1627% (465/627) (classification)\n",
      "Accuracy = 59.7701% (624/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 924\n",
      "nu = 0.679961\n",
      "obj = -175.414561, rho = -0.979220\n",
      "nSV = 1705, nBSV = 1702\n",
      "Total nSV = 1705\n",
      "Accuracy = 73.5247% (461/627) (classification)\n",
      "Accuracy = 59.6743% (623/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 939\n",
      "nu = 0.678617\n",
      "obj = -174.698982, rho = 0.978646\n",
      "nSV = 1704, nBSV = 1700\n",
      "Total nSV = 1704\n",
      "Accuracy = 73.8437% (463/627) (classification)\n",
      "Accuracy = 60.4406% (631/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 922\n",
      "nu = 0.672118\n",
      "obj = -173.372549, rho = -0.990467\n",
      "nSV = 1687, nBSV = 1683\n",
      "Total nSV = 1687\n",
      "Accuracy = 71.7703% (450/627) (classification)\n",
      "Accuracy = 59.7701% (624/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 927\n",
      "nu = 0.680723\n",
      "obj = -175.918811, rho = 0.981360\n",
      "nSV = 1708, nBSV = 1703\n",
      "Total nSV = 1708\n",
      "Accuracy = 74.3222% (466/627) (classification)\n",
      "Accuracy = 59.5785% (622/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 893\n",
      "nu = 0.611195\n",
      "obj = -487.052255, rho = 1.020947\n",
      "nSV = 1535, nBSV = 1528\n",
      "Total nSV = 1535\n",
      "Accuracy = 73.3652% (460/627) (classification)\n",
      "Accuracy = 60.6322% (633/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 905\n",
      "nu = 0.611365\n",
      "obj = -488.214603, rho = -1.032593\n",
      "nSV = 1535, nBSV = 1529\n",
      "Total nSV = 1535\n",
      "Accuracy = 71.9298% (451/627) (classification)\n",
      "Accuracy = 59.9617% (626/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 909\n",
      "nu = 0.619638\n",
      "obj = -493.754705, rho = -1.024243\n",
      "nSV = 1557, nBSV = 1549\n",
      "Total nSV = 1557\n",
      "Accuracy = 75.1196% (471/627) (classification)\n",
      "Accuracy = 61.3985% (641/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 903\n",
      "nu = 0.611739\n",
      "obj = -487.512298, rho = -1.029897\n",
      "nSV = 1536, nBSV = 1529\n",
      "Total nSV = 1536\n",
      "Accuracy = 73.0463% (458/627) (classification)\n",
      "Accuracy = 61.3985% (641/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 914\n",
      "nu = 0.625000\n",
      "obj = -498.793193, rho = -1.022428\n",
      "nSV = 1570, nBSV = 1565\n",
      "Total nSV = 1570\n",
      "Accuracy = 75.7576% (475/627) (classification)\n",
      "Accuracy = 61.3027% (640/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 873\n",
      "nu = 0.572969\n",
      "obj = -1385.395532, rho = -1.088766\n",
      "nSV = 1441, nBSV = 1432\n",
      "Total nSV = 1441\n",
      "Accuracy = 71.7703% (450/627) (classification)\n",
      "Accuracy = 61.7816% (645/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 884\n",
      "nu = 0.578948\n",
      "obj = -1404.306728, rho = -1.075352\n",
      "nSV = 1457, nBSV = 1448\n",
      "Total nSV = 1457\n",
      "Accuracy = 74.9601% (470/627) (classification)\n",
      "Accuracy = 63.8889% (667/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 893\n",
      "nu = 0.586690\n",
      "obj = -1425.036543, rho = 1.100793\n",
      "nSV = 1474, nBSV = 1467\n",
      "Total nSV = 1474\n",
      "Accuracy = 75.4386% (473/627) (classification)\n",
      "Accuracy = 62.069% (648/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 878\n",
      "nu = 0.579845\n",
      "obj = -1402.660800, rho = -1.094339\n",
      "nSV = 1458, nBSV = 1450\n",
      "Total nSV = 1458\n",
      "Accuracy = 73.3652% (460/627) (classification)\n",
      "Accuracy = 60.9195% (636/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 882\n",
      "nu = 0.586708\n",
      "obj = -1422.970868, rho = 1.089986\n",
      "nSV = 1474, nBSV = 1466\n",
      "Total nSV = 1474\n",
      "Accuracy = 76.3955% (479/627) (classification)\n",
      "Accuracy = 63.41% (662/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 949\n",
      "nu = 0.569616\n",
      "obj = -4115.587598, rho = 1.152872\n",
      "nSV = 1433, nBSV = 1420\n",
      "Total nSV = 1433\n",
      "Accuracy = 74.1627% (465/627) (classification)\n",
      "Accuracy = 62.069% (648/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 908\n",
      "nu = 0.564136\n",
      "obj = -4082.503946, rho = 1.141000\n",
      "nSV = 1420, nBSV = 1408\n",
      "Total nSV = 1420\n",
      "Accuracy = 76.3955% (479/627) (classification)\n",
      "Accuracy = 64.9425% (678/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 943\n",
      "nu = 0.569729\n",
      "obj = -4142.064313, rho = -1.130142\n",
      "nSV = 1436, nBSV = 1421\n",
      "Total nSV = 1436\n",
      "Accuracy = 75.4386% (473/627) (classification)\n",
      "Accuracy = 61.2069% (639/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 975\n",
      "nu = 0.570680\n",
      "obj = -4131.171013, rho = 1.185120\n",
      "nSV = 1435, nBSV = 1423\n",
      "Total nSV = 1435\n",
      "Accuracy = 76.0766% (477/627) (classification)\n",
      "Accuracy = 65.0383% (679/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 878\n",
      "nu = 0.540182\n",
      "obj = -3899.653787, rho = -1.183199\n",
      "nSV = 1359, nBSV = 1347\n",
      "Total nSV = 1359\n",
      "Accuracy = 70.6539% (443/627) (classification)\n",
      "Accuracy = 62.1648% (649/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1241\n",
      "nu = 0.546458\n",
      "obj = -11741.102341, rho = 1.270472\n",
      "nSV = 1377, nBSV = 1360\n",
      "Total nSV = 1377\n",
      "Accuracy = 79.5853% (499/627) (classification)\n",
      "Accuracy = 69.0613% (721/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1211\n",
      "nu = 0.549087\n",
      "obj = -11800.308415, rho = 1.288563\n",
      "nSV = 1382, nBSV = 1366\n",
      "Total nSV = 1382\n",
      "Accuracy = 79.2663% (497/627) (classification)\n",
      "Accuracy = 72.2222% (754/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1209\n",
      "nu = 0.542809\n",
      "obj = -11657.921395, rho = -1.305922\n",
      "nSV = 1367, nBSV = 1352\n",
      "Total nSV = 1367\n",
      "Accuracy = 80.0638% (502/627) (classification)\n",
      "Accuracy = 71.4559% (746/1044) (classification)\n",
      "*.*\n",
      "optimization finished, #iter = 1027\n",
      "nu = 0.542904\n",
      "obj = -11704.080250, rho = -1.230576\n",
      "nSV = 1370, nBSV = 1353\n",
      "Total nSV = 1370\n",
      "Accuracy = 78.4689% (492/627) (classification)\n",
      "Accuracy = 70.0192% (731/1044) (classification)\n",
      "*.*\n",
      "optimization finished, #iter = 1016\n",
      "nu = 0.530041\n",
      "obj = -11406.010892, rho = -1.432526\n",
      "nSV = 1331, nBSV = 1320\n",
      "Total nSV = 1331\n",
      "Accuracy = 75.4386% (473/627) (classification)\n",
      "Accuracy = 70.8812% (740/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*.*\n",
      "optimization finished, #iter = 2113\n",
      "nu = 0.500620\n",
      "obj = -32557.975222, rho = -1.117306\n",
      "nSV = 1265, nBSV = 1243\n",
      "Total nSV = 1265\n",
      "Accuracy = 77.6715% (487/627) (classification)\n",
      "Accuracy = 72.318% (755/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1784\n",
      "nu = 0.513854\n",
      "obj = -33388.616879, rho = 1.189850\n",
      "nSV = 1299, nBSV = 1280\n",
      "Total nSV = 1299\n",
      "Accuracy = 78.3094% (491/627) (classification)\n",
      "Accuracy = 71.7433% (749/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1917\n",
      "nu = 0.506355\n",
      "obj = -32749.201265, rho = -1.381618\n",
      "nSV = 1279, nBSV = 1259\n",
      "Total nSV = 1279\n",
      "Accuracy = 79.4258% (498/627) (classification)\n",
      "Accuracy = 72.318% (755/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1550\n",
      "nu = 0.508350\n",
      "obj = -32926.802110, rho = -1.400123\n",
      "nSV = 1283, nBSV = 1265\n",
      "Total nSV = 1283\n",
      "Accuracy = 80.0638% (502/627) (classification)\n",
      "Accuracy = 72.5096% (757/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1823\n",
      "nu = 0.513504\n",
      "obj = -33235.916420, rho = -1.479651\n",
      "nSV = 1297, nBSV = 1277\n",
      "Total nSV = 1297\n",
      "Accuracy = 80.2233% (503/627) (classification)\n",
      "Accuracy = 72.318% (755/1044) (classification)\n",
      "..\n",
      "WARNING: using -h 0 may be faster\n",
      "*..*.*\n",
      "optimization finished, #iter = 4861\n",
      "nu = 0.481740\n",
      "obj = -94873.432902, rho = 1.098982\n",
      "nSV = 1225, nBSV = 1195\n",
      "Total nSV = 1225\n",
      "Accuracy = 80.0638% (502/627) (classification)\n",
      "Accuracy = 67.5287% (705/1044) (classification)\n",
      "..*\n",
      "optimization finished, #iter = 2895\n",
      "nu = 0.477482\n",
      "obj = -94147.264599, rho = -1.316356\n",
      "nSV = 1213, nBSV = 1187\n",
      "Total nSV = 1213\n",
      "Accuracy = 79.5853% (499/627) (classification)\n",
      "Accuracy = 69.7318% (728/1044) (classification)\n",
      "..\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 2879\n",
      "nu = 0.476003\n",
      "obj = -93696.044035, rho = 1.151639\n",
      "nSV = 1208, nBSV = 1185\n",
      "Total nSV = 1208\n",
      "Accuracy = 77.9904% (489/627) (classification)\n",
      "Accuracy = 68.1992% (712/1044) (classification)\n",
      "..*.*\n",
      "optimization finished, #iter = 3486\n",
      "nu = 0.483770\n",
      "obj = -95618.403981, rho = -1.308243\n",
      "nSV = 1228, nBSV = 1200\n",
      "Total nSV = 1228\n",
      "Accuracy = 80.5423% (505/627) (classification)\n",
      "Accuracy = 69.3487% (724/1044) (classification)\n",
      "....*.*\n",
      "optimization finished, #iter = 5626\n",
      "nu = 0.487889\n",
      "obj = -96060.677830, rho = 1.602258\n",
      "nSV = 1237, nBSV = 1208\n",
      "Total nSV = 1237\n",
      "Accuracy = 80.7018% (506/627) (classification)\n",
      "Accuracy = 70.5939% (737/1044) (classification)\n",
      "......*...*\n",
      "optimization finished, #iter = 9303\n",
      "nu = 0.478732\n",
      "obj = -285480.280310, rho = -1.402630\n",
      "nSV = 1219, nBSV = 1187\n",
      "Total nSV = 1219\n",
      "Accuracy = 82.6156% (518/627) (classification)\n",
      "Accuracy = 67.8161% (708/1044) (classification)\n",
      "...*..*\n",
      "optimization finished, #iter = 5201\n",
      "nu = 0.460005\n",
      "obj = -274411.129073, rho = -1.344914\n",
      "nSV = 1169, nBSV = 1138\n",
      "Total nSV = 1169\n",
      "Accuracy = 79.2663% (497/627) (classification)\n",
      "Accuracy = 67.3372% (703/1044) (classification)\n",
      ".....*....*.....*\n",
      "optimization finished, #iter = 13581\n",
      "nu = 0.453519\n",
      "obj = -270579.640791, rho = 1.172407\n",
      "nSV = 1153, nBSV = 1121\n",
      "Total nSV = 1153\n",
      "Accuracy = 79.4258% (498/627) (classification)\n",
      "Accuracy = 67.433% (704/1044) (classification)\n",
      ".....*....*\n",
      "optimization finished, #iter = 9911\n",
      "nu = 0.455812\n",
      "obj = -272183.929460, rho = -1.414161\n",
      "nSV = 1158, nBSV = 1122\n",
      "Total nSV = 1158\n",
      "Accuracy = 78.4689% (492/627) (classification)\n",
      "Accuracy = 66.3793% (693/1044) (classification)\n",
      ".....*..*........*\n",
      "optimization finished, #iter = 15073\n",
      "nu = 0.463508\n",
      "obj = -276917.855325, rho = 1.427362\n",
      "nSV = 1181, nBSV = 1146\n",
      "Total nSV = 1181\n",
      "Accuracy = 79.5853% (499/627) (classification)\n",
      "Accuracy = 69.2529% (723/1044) (classification)\n",
      "...............*............*.................*\n",
      "optimization finished, #iter = 44235\n",
      "nu = 0.457938\n",
      "obj = -824855.420100, rho = 1.051298\n",
      "nSV = 1168, nBSV = 1127\n",
      "Total nSV = 1168\n",
      "Accuracy = 82.1372% (515/627) (classification)\n",
      "Accuracy = 60.8238% (635/1044) (classification)\n",
      "..........*........*.*\n",
      "optimization finished, #iter = 19140\n",
      "nu = 0.456069\n",
      "obj = -822317.640501, rho = -1.185263\n",
      "nSV = 1164, nBSV = 1124\n",
      "Total nSV = 1164\n",
      "Accuracy = 80.2233% (503/627) (classification)\n",
      "Accuracy = 64.0805% (669/1044) (classification)\n",
      ".............*..........*......*\n",
      "optimization finished, #iter = 28991\n",
      "nu = 0.473875\n",
      "obj = -855112.480166, rho = 1.201268\n",
      "nSV = 1209, nBSV = 1170\n",
      "Total nSV = 1209\n",
      "Accuracy = 83.5726% (524/627) (classification)\n",
      "Accuracy = 60.3448% (630/1044) (classification)\n",
      ".................*........*\n",
      "optimization finished, #iter = 25638\n",
      "nu = 0.462832\n",
      "obj = -834433.551961, rho = 1.255960\n",
      "nSV = 1177, nBSV = 1139\n",
      "Total nSV = 1177\n",
      "Accuracy = 82.6156% (518/627) (classification)\n",
      "Accuracy = 61.9732% (647/1044) (classification)\n",
      "................*....*\n",
      "optimization finished, #iter = 20878\n",
      "nu = 0.461054\n",
      "obj = -830548.844362, rho = -1.292799\n",
      "nSV = 1175, nBSV = 1136\n",
      "Total nSV = 1175\n",
      "Accuracy = 81.4992% (511/627) (classification)\n",
      "Accuracy = 63.1226% (659/1044) (classification)\n",
      ".................................*.........*..............................*\n",
      "optimization finished, #iter = 71402\n",
      "nu = 0.444892\n",
      "obj = -2415005.767448, rho = 0.942829\n",
      "nSV = 1139, nBSV = 1093\n",
      "Total nSV = 1139\n",
      "Accuracy = 79.2663% (497/627) (classification)\n",
      "Accuracy = 55.9387% (584/1044) (classification)\n",
      "...........................*.......................*..................*\n",
      "optimization finished, #iter = 68257\n",
      "nu = 0.442891\n",
      "obj = -2406213.314832, rho = 1.717516\n",
      "nSV = 1129, nBSV = 1088\n",
      "Total nSV = 1129\n",
      "Accuracy = 79.4258% (498/627) (classification)\n",
      "Accuracy = 63.1226% (659/1044) (classification)\n",
      "..............................*..............*......................*\n",
      "optimization finished, #iter = 65175\n",
      "nu = 0.445577\n",
      "obj = -2415652.142203, rho = -0.940056\n",
      "nSV = 1143, nBSV = 1096\n",
      "Total nSV = 1143\n",
      "Accuracy = 79.7448% (500/627) (classification)\n",
      "Accuracy = 56.8966% (594/1044) (classification)\n",
      "................................*.................*...................*.........*\n",
      "optimization finished, #iter = 76994\n",
      "nu = 0.449090\n",
      "obj = -2439350.580998, rho = -1.091298\n",
      "nSV = 1154, nBSV = 1103\n",
      "Total nSV = 1154\n",
      "Accuracy = 81.6587% (512/627) (classification)\n",
      "Accuracy = 59.2912% (619/1044) (classification)\n",
      "..........................*...........*\n",
      "optimization finished, #iter = 37072\n",
      "nu = 0.441668\n",
      "obj = -2400173.353837, rho = -1.107111\n",
      "nSV = 1138, nBSV = 1090\n",
      "Total nSV = 1138\n",
      "Accuracy = 80.8612% (507/627) (classification)\n",
      "Accuracy = 59.8659% (625/1044) (classification)\n",
      "....................................................................................................................*..................................................................*..............................................................................................................*\n",
      "optimization finished, #iter = 291566\n",
      "nu = 0.439895\n",
      "obj = -7167867.595118, rho = 1.454883\n",
      "nSV = 1131, nBSV = 1076\n",
      "Total nSV = 1131\n",
      "Accuracy = 78.7879% (494/627) (classification)\n",
      "Accuracy = 52.5862% (549/1044) (classification)\n",
      ".................................................................................*.........................................................*........................................................................................................*..................................................*.....................*\n",
      "optimization finished, #iter = 312326\n",
      "nu = 0.437347\n",
      "obj = -7130722.188190, rho = 1.428885\n",
      "nSV = 1127, nBSV = 1074\n",
      "Total nSV = 1127\n",
      "Accuracy = 79.4258% (498/627) (classification)\n",
      "Accuracy = 57.7586% (603/1044) (classification)\n",
      "....................................................................*........................................................................................*..................................................................................*......*\n",
      "optimization finished, #iter = 243895\n",
      "nu = 0.441977\n",
      "obj = -7194636.812573, rho = -0.859361\n",
      "nSV = 1137, nBSV = 1085\n",
      "Total nSV = 1137\n",
      "Accuracy = 80.7018% (506/627) (classification)\n",
      "Accuracy = 58.8123% (614/1044) (classification)\n",
      "................................................................*.......................................................................*...........................................................*..........................................................................................................*.......................*\n",
      "optimization finished, #iter = 322570\n",
      "nu = 0.431030\n",
      "obj = -7028220.705114, rho = 0.926589\n",
      "nSV = 1110, nBSV = 1057\n",
      "Total nSV = 1110\n",
      "Accuracy = 79.1069% (496/627) (classification)\n",
      "Accuracy = 54.9808% (574/1044) (classification)\n",
      ".........................................................................*...............................................................*.....................*\n",
      "optimization finished, #iter = 157035\n",
      "nu = 0.440274\n",
      "obj = -7176131.613259, rho = -1.451603\n",
      "nSV = 1136, nBSV = 1074\n",
      "Total nSV = 1136\n",
      "Accuracy = 80.3828% (504/627) (classification)\n",
      "Accuracy = 53.9272% (563/1044) (classification)\n",
      "...........................................................................................................................................................................................................................................................................................................................*.......................................................................................................................................................................................*..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*................................................................................................................................................................*................................................*\n",
      "optimization finished, #iter = 1299027\n",
      "nu = 0.439824\n",
      "obj = -21560682.837501, rho = 1.315230\n",
      "nSV = 1139, nBSV = 1072\n",
      "Total nSV = 1139\n",
      "Accuracy = 80.5423% (505/627) (classification)\n",
      "Accuracy = 51.5326% (538/1044) (classification)\n",
      ".........................................................................................................................................................................................................................................................*........................................................................................................................................*......................................................................................................................................................................................................................................................................................................................................................................................................*..........................................................................................................................*..............................................................................................................*\n",
      "optimization finished, #iter = 1007189\n",
      "nu = 0.438987\n",
      "obj = -21464586.050749, rho = 1.496987\n",
      "nSV = 1138, nBSV = 1071\n",
      "Total nSV = 1138\n",
      "Accuracy = 80.7018% (506/627) (classification)\n",
      "Accuracy = 52.682% (550/1044) (classification)\n",
      "..................................................................................................................................................................................*......................................................................................................................*.........................................................................................................................................*.................................................................................*\n",
      "optimization finished, #iter = 514235\n",
      "nu = 0.446548\n",
      "obj = -21837908.678690, rho = -1.258322\n",
      "nSV = 1153, nBSV = 1088\n",
      "Total nSV = 1153\n",
      "Accuracy = 80.8612% (507/627) (classification)\n",
      "Accuracy = 50.4789% (527/1044) (classification)\n",
      "..........................................................................................................................................................*.............................................................................................................................................................*.................................................................................................................................*\n",
      "optimization finished, #iter = 440063\n",
      "nu = 0.439047\n",
      "obj = -21504231.590899, rho = -1.238672\n",
      "nSV = 1130, nBSV = 1073\n",
      "Total nSV = 1130\n",
      "Accuracy = 80.8612% (507/627) (classification)\n",
      "Accuracy = 52.1073% (544/1044) (classification)\n",
      "....................................................................................................................................................................................................................................*.....................................................................................................................................................................*..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................*...........................................*\n",
      "optimization finished, #iter = 957460\n",
      "nu = 0.440406\n",
      "obj = -21507507.427992, rho = 1.294971\n",
      "nSV = 1134, nBSV = 1071\n",
      "Total nSV = 1134\n",
      "Accuracy = 81.3397% (510/627) (classification)\n",
      "Accuracy = 54.023% (564/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1229\n",
      "nu = 0.980846\n",
      "obj = -0.041621, rho = -0.997363\n",
      "nSV = 2458, nBSV = 2458\n",
      "Total nSV = 2458\n",
      "Accuracy = 51.1962% (321/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1247\n",
      "nu = 0.995211\n",
      "obj = -0.042229, rho = -0.996470\n",
      "nSV = 2494, nBSV = 2494\n",
      "Total nSV = 2494\n",
      "Accuracy = 54.067% (339/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1226\n",
      "nu = 0.978452\n",
      "obj = -0.041519, rho = -0.997360\n",
      "nSV = 2452, nBSV = 2452\n",
      "Total nSV = 2452\n",
      "Accuracy = 50.7177% (318/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1229\n",
      "nu = 0.980846\n",
      "obj = -0.041620, rho = -0.997554\n",
      "nSV = 2458, nBSV = 2458\n",
      "Total nSV = 2458\n",
      "Accuracy = 51.1962% (321/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1240\n",
      "nu = 0.983240\n",
      "obj = -0.041721, rho = 0.997359\n",
      "nSV = 2464, nBSV = 2464\n",
      "Total nSV = 2464\n",
      "Accuracy = 51.6746% (324/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1235\n",
      "nu = 0.980846\n",
      "obj = -0.124824, rho = 0.992693\n",
      "nSV = 2458, nBSV = 2458\n",
      "Total nSV = 2458\n",
      "Accuracy = 51.1962% (321/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1240\n",
      "nu = 0.988827\n",
      "obj = -0.125833, rho = -0.991489\n",
      "nSV = 2478, nBSV = 2478\n",
      "Total nSV = 2478\n",
      "Accuracy = 52.7911% (331/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1232\n",
      "nu = 0.976057\n",
      "obj = -0.124220, rho = 0.993264\n",
      "nSV = 2446, nBSV = 2446\n",
      "Total nSV = 2446\n",
      "Accuracy = 50.2392% (315/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1251\n",
      "nu = 0.998404\n",
      "obj = -0.127043, rho = -0.999833\n",
      "nSV = 2502, nBSV = 2502\n",
      "Total nSV = 2502\n",
      "Accuracy = 44.6571% (280/627) (classification)\n",
      "Accuracy = 52.2989% (546/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1245\n",
      "nu = 0.987231\n",
      "obj = -0.125634, rho = 0.991883\n",
      "nSV = 2474, nBSV = 2474\n",
      "Total nSV = 2474\n",
      "Accuracy = 52.4721% (329/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1231\n",
      "nu = 0.975259\n",
      "obj = -0.372055, rho = 0.980062\n",
      "nSV = 2444, nBSV = 2444\n",
      "Total nSV = 2444\n",
      "Accuracy = 50.0797% (314/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1239\n",
      "nu = 0.983240\n",
      "obj = -0.375060, rho = 0.977434\n",
      "nSV = 2464, nBSV = 2464\n",
      "Total nSV = 2464\n",
      "Accuracy = 51.6746% (324/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1237\n",
      "nu = 0.978452\n",
      "obj = -0.373247, rho = 0.978706\n",
      "nSV = 2452, nBSV = 2452\n",
      "Total nSV = 2452\n",
      "Accuracy = 50.7177% (318/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1237\n",
      "nu = 0.987231\n",
      "obj = -0.376535, rho = -0.975358\n",
      "nSV = 2474, nBSV = 2474\n",
      "Total nSV = 2474\n",
      "Accuracy = 52.4721% (329/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1226\n",
      "nu = 0.977654\n",
      "obj = -0.372929, rho = -0.978364\n",
      "nSV = 2450, nBSV = 2450\n",
      "Total nSV = 2450\n",
      "Accuracy = 50.5582% (317/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1207\n",
      "nu = 0.962490\n",
      "obj = -1.099306, rho = -0.948217\n",
      "nSV = 2412, nBSV = 2412\n",
      "Total nSV = 2412\n",
      "Accuracy = 47.5279% (298/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1233\n",
      "nu = 0.984038\n",
      "obj = -1.122794, rho = -0.928803\n",
      "nSV = 2466, nBSV = 2466\n",
      "Total nSV = 2466\n",
      "Accuracy = 51.8341% (325/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1234\n",
      "nu = 0.976856\n",
      "obj = -1.115070, rho = 0.936867\n",
      "nSV = 2448, nBSV = 2448\n",
      "Total nSV = 2448\n",
      "Accuracy = 50.3987% (316/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1219\n",
      "nu = 0.966480\n",
      "obj = -1.103581, rho = 0.944405\n",
      "nSV = 2422, nBSV = 2422\n",
      "Total nSV = 2422\n",
      "Accuracy = 48.3254% (303/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1248\n",
      "nu = 0.996010\n",
      "obj = -1.135560, rho = 0.906638\n",
      "nSV = 2496, nBSV = 2496\n",
      "Total nSV = 2496\n",
      "Accuracy = 54.2265% (340/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1219\n",
      "nu = 0.965682\n",
      "obj = -3.287305, rho = 0.841379\n",
      "nSV = 2420, nBSV = 2420\n",
      "Total nSV = 2420\n",
      "Accuracy = 48.1659% (302/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1204\n",
      "nu = 0.960096\n",
      "obj = -3.269901, rho = -0.849294\n",
      "nSV = 2406, nBSV = 2406\n",
      "Total nSV = 2406\n",
      "Accuracy = 47.0494% (295/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1235\n",
      "nu = 0.979250\n",
      "obj = -3.324528, rho = 0.799417\n",
      "nSV = 2454, nBSV = 2454\n",
      "Total nSV = 2454\n",
      "Accuracy = 50.8772% (319/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1220\n",
      "nu = 0.973663\n",
      "obj = -3.310985, rho = -0.822888\n",
      "nSV = 2440, nBSV = 2440\n",
      "Total nSV = 2440\n",
      "Accuracy = 49.7608% (312/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1206\n",
      "nu = 0.961692\n",
      "obj = -3.274484, rho = -0.845037\n",
      "nSV = 2410, nBSV = 2410\n",
      "Total nSV = 2410\n",
      "Accuracy = 47.3684% (297/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1224\n",
      "nu = 0.976856\n",
      "obj = -9.718477, rho = -0.419717\n",
      "nSV = 2448, nBSV = 2448\n",
      "Total nSV = 2448\n",
      "Accuracy = 50.3987% (316/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1231\n",
      "nu = 0.982442\n",
      "obj = -9.751049, rho = -0.373325\n",
      "nSV = 2462, nBSV = 2462\n",
      "Total nSV = 2462\n",
      "Accuracy = 51.5152% (323/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1224\n",
      "nu = 0.974461\n",
      "obj = -9.707486, rho = 0.447734\n",
      "nSV = 2442, nBSV = 2442\n",
      "Total nSV = 2442\n",
      "Accuracy = 49.9203% (313/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1224\n",
      "nu = 0.976856\n",
      "obj = -9.735329, rho = -0.445007\n",
      "nSV = 2448, nBSV = 2448\n",
      "Total nSV = 2448\n",
      "Accuracy = 50.3987% (316/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1233\n",
      "nu = 0.983240\n",
      "obj = -9.776716, rho = -0.385449\n",
      "nSV = 2464, nBSV = 2464\n",
      "Total nSV = 2464\n",
      "Accuracy = 51.6746% (324/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1221\n",
      "nu = 0.974461\n",
      "obj = -27.111677, rho = 0.651059\n",
      "nSV = 2442, nBSV = 2442\n",
      "Total nSV = 2442\n",
      "Accuracy = 66.3477% (416/627) (classification)\n",
      "Accuracy = 52.5862% (549/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1216\n",
      "nu = 0.968875\n",
      "obj = -27.123941, rho = 0.528991\n",
      "nSV = 2428, nBSV = 2428\n",
      "Total nSV = 2428\n",
      "Accuracy = 69.2185% (434/627) (classification)\n",
      "Accuracy = 53.5441% (559/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1224\n",
      "nu = 0.976856\n",
      "obj = -27.003960, rho = -0.739813\n",
      "nSV = 2448, nBSV = 2448\n",
      "Total nSV = 2448\n",
      "Accuracy = 64.7528% (406/627) (classification)\n",
      "Accuracy = 52.3946% (547/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1231\n",
      "nu = 0.982442\n",
      "obj = -26.921472, rho = 0.925923\n",
      "nSV = 2462, nBSV = 2462\n",
      "Total nSV = 2462\n",
      "Accuracy = 60.1276% (377/627) (classification)\n",
      "Accuracy = 52.3946% (547/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1217\n",
      "nu = 0.970471\n",
      "obj = -27.092800, rho = 0.557221\n",
      "nSV = 2432, nBSV = 2432\n",
      "Total nSV = 2432\n",
      "Accuracy = 69.059% (433/627) (classification)\n",
      "Accuracy = 53.4483% (558/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1110\n",
      "nu = 0.857431\n",
      "obj = -70.457991, rho = 0.970967\n",
      "nSV = 2150, nBSV = 2148\n",
      "Total nSV = 2150\n",
      "Accuracy = 69.059% (433/627) (classification)\n",
      "Accuracy = 53.3525% (557/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1098\n",
      "nu = 0.851318\n",
      "obj = -69.823066, rho = -0.972265\n",
      "nSV = 2134, nBSV = 2132\n",
      "Total nSV = 2134\n",
      "Accuracy = 67.7831% (425/627) (classification)\n",
      "Accuracy = 53.3525% (557/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1096\n",
      "nu = 0.854749\n",
      "obj = -70.057036, rho = -0.972151\n",
      "nSV = 2142, nBSV = 2142\n",
      "Total nSV = 2142\n",
      "Accuracy = 67.7831% (425/627) (classification)\n",
      "Accuracy = 53.7356% (561/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1114\n",
      "nu = 0.861177\n",
      "obj = -70.475809, rho = 0.972245\n",
      "nSV = 2160, nBSV = 2158\n",
      "Total nSV = 2160\n",
      "Accuracy = 70.0159% (439/627) (classification)\n",
      "Accuracy = 53.7356% (561/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1106\n",
      "nu = 0.865124\n",
      "obj = -70.999109, rho = 0.967115\n",
      "nSV = 2168, nBSV = 2168\n",
      "Total nSV = 2168\n",
      "Accuracy = 71.4514% (448/627) (classification)\n",
      "Accuracy = 53.5441% (559/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 988\n",
      "nu = 0.731844\n",
      "obj = -184.853174, rho = 0.990953\n",
      "nSV = 1835, nBSV = 1833\n",
      "Total nSV = 1835\n",
      "Accuracy = 71.6108% (449/627) (classification)\n",
      "Accuracy = 55.8429% (583/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 989\n",
      "nu = 0.742386\n",
      "obj = -188.230348, rho = -0.988190\n",
      "nSV = 1863, nBSV = 1858\n",
      "Total nSV = 1863\n",
      "Accuracy = 74.8006% (469/627) (classification)\n",
      "Accuracy = 55.8429% (583/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 985\n",
      "nu = 0.733897\n",
      "obj = -186.009931, rho = 0.988727\n",
      "nSV = 1840, nBSV = 1838\n",
      "Total nSV = 1840\n",
      "Accuracy = 73.5247% (461/627) (classification)\n",
      "Accuracy = 55.364% (578/1044) (classification)\n",
      "*.\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1003\n",
      "nu = 0.734945\n",
      "obj = -185.686658, rho = 0.989757\n",
      "nSV = 1843, nBSV = 1840\n",
      "Total nSV = 1843\n",
      "Accuracy = 74.6411% (468/627) (classification)\n",
      "Accuracy = 56.3218% (588/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 994\n",
      "nu = 0.722761\n",
      "obj = -183.275747, rho = 0.992150\n",
      "nSV = 1814, nBSV = 1808\n",
      "Total nSV = 1814\n",
      "Accuracy = 70.4944% (442/627) (classification)\n",
      "Accuracy = 55.2682% (577/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 902\n",
      "nu = 0.633797\n",
      "obj = -498.287904, rho = 1.004501\n",
      "nSV = 1592, nBSV = 1587\n",
      "Total nSV = 1592\n",
      "Accuracy = 71.1324% (446/627) (classification)\n",
      "Accuracy = 57.567% (601/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 919\n",
      "nu = 0.644421\n",
      "obj = -504.921386, rho = 1.002138\n",
      "nSV = 1618, nBSV = 1611\n",
      "Total nSV = 1618\n",
      "Accuracy = 74.0032% (464/627) (classification)\n",
      "Accuracy = 58.908% (615/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 904\n",
      "nu = 0.640190\n",
      "obj = -503.200125, rho = -1.005571\n",
      "nSV = 1608, nBSV = 1602\n",
      "Total nSV = 1608\n",
      "Accuracy = 72.8868% (457/627) (classification)\n",
      "Accuracy = 57.8544% (604/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 915\n",
      "nu = 0.654163\n",
      "obj = -514.956749, rho = -1.001905\n",
      "nSV = 1642, nBSV = 1636\n",
      "Total nSV = 1642\n",
      "Accuracy = 75.9171% (476/627) (classification)\n",
      "Accuracy = 57.3755% (599/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 917\n",
      "nu = 0.656255\n",
      "obj = -516.860899, rho = -1.005997\n",
      "nSV = 1647, nBSV = 1642\n",
      "Total nSV = 1647\n",
      "Accuracy = 76.0766% (477/627) (classification)\n",
      "Accuracy = 57.6628% (602/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 897\n",
      "nu = 0.588981\n",
      "obj = -1420.285813, rho = -1.022692\n",
      "nSV = 1480, nBSV = 1473\n",
      "Total nSV = 1480\n",
      "Accuracy = 71.6108% (449/627) (classification)\n",
      "Accuracy = 59.1954% (618/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 915\n",
      "nu = 0.601776\n",
      "obj = -1452.768245, rho = 1.022904\n",
      "nSV = 1513, nBSV = 1503\n",
      "Total nSV = 1513\n",
      "Accuracy = 74.3222% (466/627) (classification)\n",
      "Accuracy = 58.908% (615/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 927\n",
      "nu = 0.609123\n",
      "obj = -1472.682723, rho = 1.025559\n",
      "nSV = 1531, nBSV = 1522\n",
      "Total nSV = 1531\n",
      "Accuracy = 76.874% (482/627) (classification)\n",
      "Accuracy = 59.0996% (617/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 879\n",
      "nu = 0.584477\n",
      "obj = -1407.942063, rho = -1.030604\n",
      "nSV = 1470, nBSV = 1461\n",
      "Total nSV = 1470\n",
      "Accuracy = 71.1324% (446/627) (classification)\n",
      "Accuracy = 58.7165% (613/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 902\n",
      "nu = 0.589816\n",
      "obj = -1423.414464, rho = -1.028591\n",
      "nSV = 1484, nBSV = 1474\n",
      "Total nSV = 1484\n",
      "Accuracy = 72.7273% (456/627) (classification)\n",
      "Accuracy = 58.7165% (613/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 924\n",
      "nu = 0.568223\n",
      "obj = -4134.907169, rho = 1.043997\n",
      "nSV = 1431, nBSV = 1417\n",
      "Total nSV = 1431\n",
      "Accuracy = 73.5247% (461/627) (classification)\n",
      "Accuracy = 57.7586% (603/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 897\n",
      "nu = 0.575096\n",
      "obj = -4202.553365, rho = -1.041263\n",
      "nSV = 1447, nBSV = 1438\n",
      "Total nSV = 1447\n",
      "Accuracy = 75.1196% (471/627) (classification)\n",
      "Accuracy = 60.1533% (628/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 898\n",
      "nu = 0.572005\n",
      "obj = -4174.701069, rho = -1.061450\n",
      "nSV = 1439, nBSV = 1427\n",
      "Total nSV = 1439\n",
      "Accuracy = 74.9601% (470/627) (classification)\n",
      "Accuracy = 60.0575% (627/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 941\n",
      "nu = 0.565127\n",
      "obj = -4101.383007, rho = 1.036609\n",
      "nSV = 1423, nBSV = 1410\n",
      "Total nSV = 1423\n",
      "Accuracy = 70.9729% (445/627) (classification)\n",
      "Accuracy = 54.5977% (570/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 945\n",
      "nu = 0.573842\n",
      "obj = -4172.761269, rho = 1.060031\n",
      "nSV = 1446, nBSV = 1433\n",
      "Total nSV = 1446\n",
      "Accuracy = 75.5981% (474/627) (classification)\n",
      "Accuracy = 62.5479% (653/1044) (classification)\n",
      "*.*\n",
      "optimization finished, #iter = 1133\n",
      "nu = 0.568446\n",
      "obj = -12247.555195, rho = 1.074691\n",
      "nSV = 1434, nBSV = 1417\n",
      "Total nSV = 1434\n",
      "Accuracy = 78.9474% (495/627) (classification)\n",
      "Accuracy = 62.931% (657/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1123\n",
      "nu = 0.553377\n",
      "obj = -11991.592836, rho = -1.071427\n",
      "nSV = 1396, nBSV = 1377\n",
      "Total nSV = 1396\n",
      "Accuracy = 75.9171% (476/627) (classification)\n",
      "Accuracy = 60.6322% (633/1044) (classification)\n",
      "*.*\n",
      "optimization finished, #iter = 1026\n",
      "nu = 0.552364\n",
      "obj = -11925.075210, rho = 1.132115\n",
      "nSV = 1392, nBSV = 1375\n",
      "Total nSV = 1392\n",
      "Accuracy = 75.1196% (471/627) (classification)\n",
      "Accuracy = 63.5057% (663/1044) (classification)\n",
      "*.*\n",
      "optimization finished, #iter = 1013\n",
      "nu = 0.554831\n",
      "obj = -12028.520649, rho = -1.063332\n",
      "nSV = 1397, nBSV = 1382\n",
      "Total nSV = 1397\n",
      "Accuracy = 75.5981% (474/627) (classification)\n",
      "Accuracy = 61.2069% (639/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 988\n",
      "nu = 0.557393\n",
      "obj = -12068.234272, rho = 1.058097\n",
      "nSV = 1406, nBSV = 1392\n",
      "Total nSV = 1406\n",
      "Accuracy = 76.236% (478/627) (classification)\n",
      "Accuracy = 60.8238% (635/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1258\n",
      "nu = 0.526020\n",
      "obj = -34178.045823, rho = -1.084414\n",
      "nSV = 1331, nBSV = 1308\n",
      "Total nSV = 1331\n",
      "Accuracy = 75.4386% (473/627) (classification)\n",
      "Accuracy = 61.3985% (641/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1516\n",
      "nu = 0.520221\n",
      "obj = -33571.675024, rho = 1.069715\n",
      "nSV = 1322, nBSV = 1293\n",
      "Total nSV = 1322\n",
      "Accuracy = 75.2791% (472/627) (classification)\n",
      "Accuracy = 67.0498% (700/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1301\n",
      "nu = 0.521323\n",
      "obj = -33729.694757, rho = 1.183897\n",
      "nSV = 1318, nBSV = 1298\n",
      "Total nSV = 1318\n",
      "Accuracy = 75.5981% (474/627) (classification)\n",
      "Accuracy = 63.5057% (663/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1247\n",
      "nu = 0.525140\n",
      "obj = -33883.687230, rho = 1.073920\n",
      "nSV = 1326, nBSV = 1307\n",
      "Total nSV = 1326\n",
      "Accuracy = 76.236% (478/627) (classification)\n",
      "Accuracy = 65.0383% (679/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1514\n",
      "nu = 0.529336\n",
      "obj = -34091.052295, rho = 1.123304\n",
      "nSV = 1340, nBSV = 1317\n",
      "Total nSV = 1340\n",
      "Accuracy = 77.0335% (483/627) (classification)\n",
      "Accuracy = 67.433% (704/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*..*\n",
      "optimization finished, #iter = 3306\n",
      "nu = 0.502026\n",
      "obj = -97939.334568, rho = -1.170533\n",
      "nSV = 1272, nBSV = 1247\n",
      "Total nSV = 1272\n",
      "Accuracy = 78.7879% (494/627) (classification)\n",
      "Accuracy = 72.318% (755/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*.*\n",
      "optimization finished, #iter = 2140\n",
      "nu = 0.507176\n",
      "obj = -99275.216167, rho = 1.069565\n",
      "nSV = 1287, nBSV = 1259\n",
      "Total nSV = 1287\n",
      "Accuracy = 80.8612% (507/627) (classification)\n",
      "Accuracy = 72.2222% (754/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*.*\n",
      "optimization finished, #iter = 2345\n",
      "nu = 0.491348\n",
      "obj = -96197.787697, rho = -1.109524\n",
      "nSV = 1246, nBSV = 1219\n",
      "Total nSV = 1246\n",
      "Accuracy = 76.3955% (479/627) (classification)\n",
      "Accuracy = 72.318% (755/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*.*\n",
      "optimization finished, #iter = 2227\n",
      "nu = 0.502228\n",
      "obj = -98009.748482, rho = -1.051351\n",
      "nSV = 1274, nBSV = 1249\n",
      "Total nSV = 1274\n",
      "Accuracy = 78.9474% (495/627) (classification)\n",
      "Accuracy = 72.318% (755/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1932\n",
      "nu = 0.490077\n",
      "obj = -95480.740995, rho = 1.131677\n",
      "nSV = 1244, nBSV = 1215\n",
      "Total nSV = 1244\n",
      "Accuracy = 77.3525% (485/627) (classification)\n",
      "Accuracy = 71.3602% (745/1044) (classification)\n",
      "....*..*\n",
      "optimization finished, #iter = 6231\n",
      "nu = 0.471856\n",
      "obj = -278855.364662, rho = -1.119831\n",
      "nSV = 1206, nBSV = 1166\n",
      "Total nSV = 1206\n",
      "Accuracy = 76.7145% (481/627) (classification)\n",
      "Accuracy = 71.7433% (749/1044) (classification)\n",
      "....*.*\n",
      "optimization finished, #iter = 5961\n",
      "nu = 0.481382\n",
      "obj = -284682.716314, rho = -1.056600\n",
      "nSV = 1228, nBSV = 1186\n",
      "Total nSV = 1228\n",
      "Accuracy = 80.7018% (506/627) (classification)\n",
      "Accuracy = 71.5517% (747/1044) (classification)\n",
      "..\n",
      "WARNING: using -h 0 may be faster\n",
      "*.*\n",
      "optimization finished, #iter = 3834\n",
      "nu = 0.466595\n",
      "obj = -275148.799551, rho = 1.247092\n",
      "nSV = 1187, nBSV = 1155\n",
      "Total nSV = 1187\n",
      "Accuracy = 77.6715% (487/627) (classification)\n",
      "Accuracy = 72.0307% (752/1044) (classification)\n",
      "...*.*\n",
      "optimization finished, #iter = 4496\n",
      "nu = 0.481896\n",
      "obj = -285533.188134, rho = 0.918015\n",
      "nSV = 1230, nBSV = 1192\n",
      "Total nSV = 1230\n",
      "Accuracy = 80.7018% (506/627) (classification)\n",
      "Accuracy = 71.8391% (750/1044) (classification)\n",
      "...*..*\n",
      "optimization finished, #iter = 5227\n",
      "nu = 0.475103\n",
      "obj = -281695.746986, rho = -1.048999\n",
      "nSV = 1212, nBSV = 1172\n",
      "Total nSV = 1212\n",
      "Accuracy = 79.4258% (498/627) (classification)\n",
      "Accuracy = 72.0307% (752/1044) (classification)\n",
      "......*...........*.*\n",
      "optimization finished, #iter = 17507\n",
      "nu = 0.449492\n",
      "obj = -805791.417917, rho = 1.495454\n",
      "nSV = 1156, nBSV = 1105\n",
      "Total nSV = 1156\n",
      "Accuracy = 77.9904% (489/627) (classification)\n",
      "Accuracy = 71.5517% (747/1044) (classification)\n",
      ".......*....*\n",
      "optimization finished, #iter = 11460\n",
      "nu = 0.462634\n",
      "obj = -828426.317909, rho = -1.018669\n",
      "nSV = 1185, nBSV = 1137\n",
      "Total nSV = 1185\n",
      "Accuracy = 81.1802% (509/627) (classification)\n",
      "Accuracy = 70.977% (741/1044) (classification)\n",
      ".....*....*.......*\n",
      "optimization finished, #iter = 16203\n",
      "nu = 0.464865\n",
      "obj = -833285.647368, rho = 1.132509\n",
      "nSV = 1188, nBSV = 1143\n",
      "Total nSV = 1188\n",
      "Accuracy = 80.5423% (505/627) (classification)\n",
      "Accuracy = 71.2644% (744/1044) (classification)\n",
      "...........*.....*\n",
      "optimization finished, #iter = 16826\n",
      "nu = 0.459929\n",
      "obj = -822090.478346, rho = 1.231065\n",
      "nSV = 1182, nBSV = 1136\n",
      "Total nSV = 1182\n",
      "Accuracy = 80.8612% (507/627) (classification)\n",
      "Accuracy = 71.5517% (747/1044) (classification)\n",
      "......*....*\n",
      "optimization finished, #iter = 10869\n",
      "nu = 0.453670\n",
      "obj = -812035.100599, rho = 1.151953\n",
      "nSV = 1162, nBSV = 1118\n",
      "Total nSV = 1162\n",
      "Accuracy = 78.9474% (495/627) (classification)\n",
      "Accuracy = 71.3602% (745/1044) (classification)\n",
      "....................*....................*...*\n",
      "optimization finished, #iter = 42623\n",
      "nu = 0.448845\n",
      "obj = -2423039.985998, rho = 1.013805\n",
      "nSV = 1161, nBSV = 1101\n",
      "Total nSV = 1161\n",
      "Accuracy = 80.0638% (502/627) (classification)\n",
      "Accuracy = 69.4444% (725/1044) (classification)\n",
      ".............*.........*\n",
      "optimization finished, #iter = 22128\n",
      "nu = 0.447705\n",
      "obj = -2414313.030035, rho = 1.141885\n",
      "nSV = 1150, nBSV = 1099\n",
      "Total nSV = 1150\n",
      "Accuracy = 78.7879% (494/627) (classification)\n",
      "Accuracy = 70.8812% (740/1044) (classification)\n",
      "..................*............*......*\n",
      "optimization finished, #iter = 36412\n",
      "nu = 0.452277\n",
      "obj = -2443479.623017, rho = 1.267445\n",
      "nSV = 1167, nBSV = 1111\n",
      "Total nSV = 1167\n",
      "Accuracy = 79.7448% (500/627) (classification)\n",
      "Accuracy = 70.8812% (740/1044) (classification)\n",
      "...............*....*\n",
      "optimization finished, #iter = 19835\n",
      "nu = 0.440203\n",
      "obj = -2374606.622598, rho = 1.071887\n",
      "nSV = 1133, nBSV = 1079\n",
      "Total nSV = 1133\n",
      "Accuracy = 78.4689% (492/627) (classification)\n",
      "Accuracy = 69.636% (727/1044) (classification)\n",
      "....................*................*..................*\n",
      "optimization finished, #iter = 53744\n",
      "nu = 0.452628\n",
      "obj = -2444876.928107, rho = 1.187742\n",
      "nSV = 1172, nBSV = 1109\n",
      "Total nSV = 1172\n",
      "Accuracy = 80.7018% (506/627) (classification)\n",
      "Accuracy = 70.8812% (740/1044) (classification)\n",
      ".........................................................*.............................................*....*\n",
      "optimization finished, #iter = 106480\n",
      "nu = 0.447602\n",
      "obj = -7257424.620330, rho = 0.951240\n",
      "nSV = 1163, nBSV = 1094\n",
      "Total nSV = 1163\n",
      "Accuracy = 80.7018% (506/627) (classification)\n",
      "Accuracy = 69.5402% (726/1044) (classification)\n",
      "...................................................*...............................*.............................*........*\n",
      "optimization finished, #iter = 118444\n",
      "nu = 0.445094\n",
      "obj = -7220007.110322, rho = 0.980074\n",
      "nSV = 1150, nBSV = 1086\n",
      "Total nSV = 1150\n",
      "Accuracy = 80.7018% (506/627) (classification)\n",
      "Accuracy = 67.5287% (705/1044) (classification)\n",
      ".............................*........................*..*\n",
      "optimization finished, #iter = 55124\n",
      "nu = 0.442224\n",
      "obj = -7158800.905961, rho = 1.245400\n",
      "nSV = 1142, nBSV = 1079\n",
      "Total nSV = 1142\n",
      "Accuracy = 79.2663% (497/627) (classification)\n",
      "Accuracy = 66.8582% (698/1044) (classification)\n",
      ".......................................................*......................................................................................*.................................................................*...................*\n",
      "optimization finished, #iter = 223652\n",
      "nu = 0.451209\n",
      "obj = -7319579.861787, rho = 0.817149\n",
      "nSV = 1169, nBSV = 1097\n",
      "Total nSV = 1169\n",
      "Accuracy = 82.2967% (516/627) (classification)\n",
      "Accuracy = 62.4521% (652/1044) (classification)\n",
      ".........................................................*...........................*............................*..*\n",
      "optimization finished, #iter = 113542\n",
      "nu = 0.445935\n",
      "obj = -7230246.969768, rho = 0.627941\n",
      "nSV = 1159, nBSV = 1085\n",
      "Total nSV = 1159\n",
      "Accuracy = 80.3828% (504/627) (classification)\n",
      "Accuracy = 63.9847% (668/1044) (classification)\n",
      "...........................................................................................................................................*................................................................................*....................................................................................................................................................................*\n",
      "optimization finished, #iter = 383670\n",
      "nu = 0.427765\n",
      "obj = -20772695.399812, rho = -1.402430\n",
      "nSV = 1118, nBSV = 1037\n",
      "Total nSV = 1118\n",
      "Accuracy = 76.555% (480/627) (classification)\n",
      "Accuracy = 58.2375% (608/1044) (classification)\n",
      "............................................................................*............................................................*...................................................................................................................................*...............................................................*\n",
      "optimization finished, #iter = 329984\n",
      "nu = 0.436122\n",
      "obj = -21136084.185004, rho = 1.377075\n",
      "nSV = 1142, nBSV = 1057\n",
      "Total nSV = 1142\n",
      "Accuracy = 79.5853% (499/627) (classification)\n",
      "Accuracy = 61.59% (643/1044) (classification)\n",
      "...........................................................................................................................*............................................................................................*......................................................................................................................................................................................................*.......................................................*\n",
      "optimization finished, #iter = 467622\n",
      "nu = 0.439830\n",
      "obj = -21386624.188643, rho = -1.200904\n",
      "nSV = 1151, nBSV = 1068\n",
      "Total nSV = 1151\n",
      "Accuracy = 80.3828% (504/627) (classification)\n",
      "Accuracy = 58.8123% (614/1044) (classification)\n",
      "......................................................................................................................*......................................................................................*..................................................................................................*................................*\n",
      "optimization finished, #iter = 333306\n",
      "nu = 0.435147\n",
      "obj = -21199675.991354, rho = -1.382526\n",
      "nSV = 1139, nBSV = 1055\n",
      "Total nSV = 1139\n",
      "Accuracy = 80.0638% (502/627) (classification)\n",
      "Accuracy = 60.4406% (631/1044) (classification)\n",
      ".................................................................................*..............................................................................................*........................................................................................................................................................................*......................................................................*\n",
      "optimization finished, #iter = 412906\n",
      "nu = 0.431977\n",
      "obj = -20986223.634048, rho = 0.931258\n",
      "nSV = 1128, nBSV = 1048\n",
      "Total nSV = 1128\n",
      "Accuracy = 80.0638% (502/627) (classification)\n",
      "Accuracy = 58.1418% (607/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1238\n",
      "nu = 0.984038\n",
      "obj = -0.041758, rho = 0.998040\n",
      "nSV = 2466, nBSV = 2466\n",
      "Total nSV = 2466\n",
      "Accuracy = 51.8341% (325/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1237\n",
      "nu = 0.981644\n",
      "obj = -0.041656, rho = 0.998150\n",
      "nSV = 2460, nBSV = 2460\n",
      "Total nSV = 2460\n",
      "Accuracy = 51.3557% (322/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1225\n",
      "nu = 0.977654\n",
      "obj = -0.041487, rho = -0.998393\n",
      "nSV = 2450, nBSV = 2450\n",
      "Total nSV = 2450\n",
      "Accuracy = 50.5582% (317/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1243\n",
      "nu = 0.988827\n",
      "obj = -0.041961, rho = 0.997784\n",
      "nSV = 2478, nBSV = 2478\n",
      "Total nSV = 2478\n",
      "Accuracy = 52.7911% (331/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1218\n",
      "nu = 0.968875\n",
      "obj = -0.041115, rho = 0.998941\n",
      "nSV = 2428, nBSV = 2428\n",
      "Total nSV = 2428\n",
      "Accuracy = 48.8038% (306/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1226\n",
      "nu = 0.978452\n",
      "obj = -0.124541, rho = -0.994839\n",
      "nSV = 2452, nBSV = 2452\n",
      "Total nSV = 2452\n",
      "Accuracy = 50.7177% (318/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1218\n",
      "nu = 0.972067\n",
      "obj = -0.123727, rho = -0.994866\n",
      "nSV = 2436, nBSV = 2436\n",
      "Total nSV = 2436\n",
      "Accuracy = 49.4418% (310/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1234\n",
      "nu = 0.984038\n",
      "obj = -0.125248, rho = -0.994066\n",
      "nSV = 2466, nBSV = 2466\n",
      "Total nSV = 2466\n",
      "Accuracy = 51.8341% (325/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1224\n",
      "nu = 0.976856\n",
      "obj = -0.124336, rho = -0.994699\n",
      "nSV = 2448, nBSV = 2448\n",
      "Total nSV = 2448\n",
      "Accuracy = 50.3987% (316/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1240\n",
      "nu = 0.989625\n",
      "obj = -0.125956, rho = -0.993146\n",
      "nSV = 2480, nBSV = 2480\n",
      "Total nSV = 2480\n",
      "Accuracy = 52.9506% (332/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1230\n",
      "nu = 0.976057\n",
      "obj = -0.372490, rho = 0.984023\n",
      "nSV = 2446, nBSV = 2446\n",
      "Total nSV = 2446\n",
      "Accuracy = 50.2392% (315/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1232\n",
      "nu = 0.983240\n",
      "obj = -0.375225, rho = -0.983454\n",
      "nSV = 2464, nBSV = 2464\n",
      "Total nSV = 2464\n",
      "Accuracy = 51.6746% (324/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1218\n",
      "nu = 0.971269\n",
      "obj = -0.370711, rho = -0.986553\n",
      "nSV = 2434, nBSV = 2434\n",
      "Total nSV = 2434\n",
      "Accuracy = 49.2823% (309/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1221\n",
      "nu = 0.973663\n",
      "obj = -0.371602, rho = -0.985182\n",
      "nSV = 2440, nBSV = 2440\n",
      "Total nSV = 2440\n",
      "Accuracy = 49.7608% (312/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1222\n",
      "nu = 0.974461\n",
      "obj = -0.371900, rho = -0.985075\n",
      "nSV = 2442, nBSV = 2442\n",
      "Total nSV = 2442\n",
      "Accuracy = 49.9203% (313/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1219\n",
      "nu = 0.972865\n",
      "obj = -1.112252, rho = -0.957022\n",
      "nSV = 2438, nBSV = 2438\n",
      "Total nSV = 2438\n",
      "Accuracy = 49.6013% (311/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1244\n",
      "nu = 0.989625\n",
      "obj = -1.130386, rho = 0.938340\n",
      "nSV = 2480, nBSV = 2480\n",
      "Total nSV = 2480\n",
      "Accuracy = 52.9506% (332/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1236\n",
      "nu = 0.985634\n",
      "obj = -1.126212, rho = -0.944408\n",
      "nSV = 2470, nBSV = 2470\n",
      "Total nSV = 2470\n",
      "Accuracy = 52.1531% (327/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1244\n",
      "nu = 0.992019\n",
      "obj = -1.132866, rho = -0.929434\n",
      "nSV = 2486, nBSV = 2486\n",
      "Total nSV = 2486\n",
      "Accuracy = 53.429% (335/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1225\n",
      "nu = 0.976856\n",
      "obj = -1.116595, rho = -0.952892\n",
      "nSV = 2448, nBSV = 2448\n",
      "Total nSV = 2448\n",
      "Accuracy = 50.3987% (316/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1253\n",
      "nu = 0.999202\n",
      "obj = -3.396005, rho = -0.737305\n",
      "nSV = 2504, nBSV = 2504\n",
      "Total nSV = 2504\n",
      "Accuracy = 54.8644% (344/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1230\n",
      "nu = 0.981644\n",
      "obj = -3.346785, rho = -0.842650\n",
      "nSV = 2460, nBSV = 2460\n",
      "Total nSV = 2460\n",
      "Accuracy = 51.3557% (322/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1239\n",
      "nu = 0.988827\n",
      "obj = -3.368837, rho = -0.815405\n",
      "nSV = 2478, nBSV = 2478\n",
      "Total nSV = 2478\n",
      "Accuracy = 52.7911% (331/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1246\n",
      "nu = 0.993615\n",
      "obj = -3.380191, rho = 0.782209\n",
      "nSV = 2490, nBSV = 2490\n",
      "Total nSV = 2490\n",
      "Accuracy = 53.748% (337/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1213\n",
      "nu = 0.958500\n",
      "obj = -3.276442, rho = 0.899239\n",
      "nSV = 2402, nBSV = 2402\n",
      "Total nSV = 2402\n",
      "Accuracy = 46.7305% (293/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1223\n",
      "nu = 0.975259\n",
      "obj = -9.837064, rho = -0.590888\n",
      "nSV = 2444, nBSV = 2444\n",
      "Total nSV = 2444\n",
      "Accuracy = 50.0797% (314/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1227\n",
      "nu = 0.976856\n",
      "obj = -9.839143, rho = 0.564903\n",
      "nSV = 2448, nBSV = 2448\n",
      "Total nSV = 2448\n",
      "Accuracy = 50.3987% (316/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1236\n",
      "nu = 0.984836\n",
      "obj = -9.898145, rho = 0.512785\n",
      "nSV = 2468, nBSV = 2468\n",
      "Total nSV = 2468\n",
      "Accuracy = 51.9936% (326/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1224\n",
      "nu = 0.971269\n",
      "obj = -9.810322, rho = 0.626456\n",
      "nSV = 2434, nBSV = 2434\n",
      "Total nSV = 2434\n",
      "Accuracy = 49.2823% (309/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1234\n",
      "nu = 0.983240\n",
      "obj = -9.893574, rho = 0.523742\n",
      "nSV = 2464, nBSV = 2464\n",
      "Total nSV = 2464\n",
      "Accuracy = 51.6746% (324/627) (classification)\n",
      "Accuracy = 47.7011% (498/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1219\n",
      "nu = 0.972865\n",
      "obj = -28.147453, rho = -0.176693\n",
      "nSV = 2438, nBSV = 2438\n",
      "Total nSV = 2438\n",
      "Accuracy = 71.7703% (450/627) (classification)\n",
      "Accuracy = 56.3218% (588/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1242\n",
      "nu = 0.990423\n",
      "obj = -27.977978, rho = 0.757430\n",
      "nSV = 2482, nBSV = 2482\n",
      "Total nSV = 2482\n",
      "Accuracy = 57.0973% (358/627) (classification)\n",
      "Accuracy = 52.3946% (547/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1231\n",
      "nu = 0.982442\n",
      "obj = -28.251053, rho = 0.384421\n",
      "nSV = 2462, nBSV = 2462\n",
      "Total nSV = 2462\n",
      "Accuracy = 67.6236% (424/627) (classification)\n",
      "Accuracy = 52.682% (550/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1230\n",
      "nu = 0.981644\n",
      "obj = -28.186663, rho = 0.401794\n",
      "nSV = 2460, nBSV = 2460\n",
      "Total nSV = 2460\n",
      "Accuracy = 65.7097% (412/627) (classification)\n",
      "Accuracy = 52.5862% (549/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1245\n",
      "nu = 0.992817\n",
      "obj = -28.114005, rho = 0.829754\n",
      "nSV = 2488, nBSV = 2488\n",
      "Total nSV = 2488\n",
      "Accuracy = 57.2568% (359/627) (classification)\n",
      "Accuracy = 52.2989% (546/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1158\n",
      "nu = 0.915678\n",
      "obj = -75.558774, rho = 0.981752\n",
      "nSV = 2296, nBSV = 2294\n",
      "Total nSV = 2296\n",
      "Accuracy = 67.1451% (421/627) (classification)\n",
      "Accuracy = 52.3946% (547/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1146\n",
      "nu = 0.905136\n",
      "obj = -74.332994, rho = 0.984967\n",
      "nSV = 2270, nBSV = 2268\n",
      "Total nSV = 2270\n",
      "Accuracy = 63.6364% (399/627) (classification)\n",
      "Accuracy = 52.3946% (547/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1149\n",
      "nu = 0.909018\n",
      "obj = -75.007551, rho = -0.982355\n",
      "nSV = 2278, nBSV = 2278\n",
      "Total nSV = 2278\n",
      "Accuracy = 64.7528% (406/627) (classification)\n",
      "Accuracy = 52.3946% (547/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1157\n",
      "nu = 0.916058\n",
      "obj = -75.299056, rho = 0.978464\n",
      "nSV = 2296, nBSV = 2294\n",
      "Total nSV = 2296\n",
      "Accuracy = 66.9856% (420/627) (classification)\n",
      "Accuracy = 52.3946% (547/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1139\n",
      "nu = 0.902086\n",
      "obj = -74.395648, rho = 0.985294\n",
      "nSV = 2262, nBSV = 2260\n",
      "Total nSV = 2262\n",
      "Accuracy = 65.3907% (410/627) (classification)\n",
      "Accuracy = 52.3946% (547/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1041\n",
      "nu = 0.792889\n",
      "obj = -198.274750, rho = -0.993613\n",
      "nSV = 1988, nBSV = 1986\n",
      "Total nSV = 1988\n",
      "Accuracy = 71.1324% (446/627) (classification)\n",
      "Accuracy = 53.8314% (562/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1053\n",
      "nu = 0.784714\n",
      "obj = -196.128850, rho = 0.995814\n",
      "nSV = 1970, nBSV = 1964\n",
      "Total nSV = 1970\n",
      "Accuracy = 70.0159% (439/627) (classification)\n",
      "Accuracy = 53.8314% (562/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1024\n",
      "nu = 0.782123\n",
      "obj = -195.700173, rho = -0.995344\n",
      "nSV = 1960, nBSV = 1960\n",
      "Total nSV = 1960\n",
      "Accuracy = 68.74% (431/627) (classification)\n",
      "Accuracy = 53.8314% (562/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1044\n",
      "nu = 0.792498\n",
      "obj = -198.472980, rho = 0.993649\n",
      "nSV = 1986, nBSV = 1986\n",
      "Total nSV = 1986\n",
      "Accuracy = 72.4083% (454/627) (classification)\n",
      "Accuracy = 53.8314% (562/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1030\n",
      "nu = 0.785315\n",
      "obj = -196.896551, rho = 0.995737\n",
      "nSV = 1968, nBSV = 1968\n",
      "Total nSV = 1968\n",
      "Accuracy = 70.6539% (443/627) (classification)\n",
      "Accuracy = 53.8314% (562/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 942\n",
      "nu = 0.686687\n",
      "obj = -531.273652, rho = -0.997528\n",
      "nSV = 1726, nBSV = 1719\n",
      "Total nSV = 1726\n",
      "Accuracy = 74.0032% (464/627) (classification)\n",
      "Accuracy = 55.6513% (581/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 956\n",
      "nu = 0.693238\n",
      "obj = -537.215858, rho = 0.998668\n",
      "nSV = 1740, nBSV = 1734\n",
      "Total nSV = 1740\n",
      "Accuracy = 74.4817% (467/627) (classification)\n",
      "Accuracy = 55.0766% (575/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 933\n",
      "nu = 0.685502\n",
      "obj = -529.087969, rho = -0.997831\n",
      "nSV = 1721, nBSV = 1715\n",
      "Total nSV = 1721\n",
      "Accuracy = 72.8868% (457/627) (classification)\n",
      "Accuracy = 55.2682% (577/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 962\n",
      "nu = 0.681707\n",
      "obj = -528.666317, rho = 0.999534\n",
      "nSV = 1712, nBSV = 1706\n",
      "Total nSV = 1712\n",
      "Accuracy = 72.2488% (453/627) (classification)\n",
      "Accuracy = 55.0766% (575/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 950\n",
      "nu = 0.681446\n",
      "obj = -528.365152, rho = 0.998281\n",
      "nSV = 1710, nBSV = 1705\n",
      "Total nSV = 1710\n",
      "Accuracy = 73.2057% (459/627) (classification)\n",
      "Accuracy = 55.2682% (577/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 913\n",
      "nu = 0.627557\n",
      "obj = -1499.702360, rho = 1.002589\n",
      "nSV = 1578, nBSV = 1568\n",
      "Total nSV = 1578\n",
      "Accuracy = 75.5981% (474/627) (classification)\n",
      "Accuracy = 56.8008% (593/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 913\n",
      "nu = 0.614756\n",
      "obj = -1466.729902, rho = 1.002824\n",
      "nSV = 1545, nBSV = 1535\n",
      "Total nSV = 1545\n",
      "Accuracy = 71.7703% (450/627) (classification)\n",
      "Accuracy = 57.0881% (596/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 908\n",
      "nu = 0.611427\n",
      "obj = -1459.075791, rho = -1.007120\n",
      "nSV = 1536, nBSV = 1529\n",
      "Total nSV = 1536\n",
      "Accuracy = 71.7703% (450/627) (classification)\n",
      "Accuracy = 57.0881% (596/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 910\n",
      "nu = 0.616476\n",
      "obj = -1471.904718, rho = -1.007421\n",
      "nSV = 1548, nBSV = 1543\n",
      "Total nSV = 1548\n",
      "Accuracy = 74.1627% (465/627) (classification)\n",
      "Accuracy = 56.5134% (590/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 917\n",
      "nu = 0.624048\n",
      "obj = -1490.457538, rho = 1.003541\n",
      "nSV = 1568, nBSV = 1561\n",
      "Total nSV = 1568\n",
      "Accuracy = 74.8006% (469/627) (classification)\n",
      "Accuracy = 57.1839% (597/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 969\n",
      "nu = 0.599497\n",
      "obj = -4347.822232, rho = -1.013250\n",
      "nSV = 1509, nBSV = 1497\n",
      "Total nSV = 1509\n",
      "Accuracy = 76.555% (480/627) (classification)\n",
      "Accuracy = 56.6092% (591/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 914\n",
      "nu = 0.582054\n",
      "obj = -4228.718959, rho = -1.014338\n",
      "nSV = 1465, nBSV = 1452\n",
      "Total nSV = 1465\n",
      "Accuracy = 73.0463% (458/627) (classification)\n",
      "Accuracy = 56.8008% (593/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 911\n",
      "nu = 0.587371\n",
      "obj = -4269.468908, rho = -1.012870\n",
      "nSV = 1479, nBSV = 1469\n",
      "Total nSV = 1479\n",
      "Accuracy = 74.8006% (469/627) (classification)\n",
      "Accuracy = 57.6628% (602/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 904\n",
      "nu = 0.579448\n",
      "obj = -4210.916359, rho = 1.018127\n",
      "nSV = 1456, nBSV = 1445\n",
      "Total nSV = 1456\n",
      "Accuracy = 73.8437% (463/627) (classification)\n",
      "Accuracy = 56.2261% (587/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 904\n",
      "nu = 0.576176\n",
      "obj = -4191.897503, rho = -1.016908\n",
      "nSV = 1449, nBSV = 1439\n",
      "Total nSV = 1449\n",
      "Accuracy = 72.8868% (457/627) (classification)\n",
      "Accuracy = 56.0345% (585/1044) (classification)\n",
      "*\n",
      "optimization finished, #iter = 940\n",
      "nu = 0.552444\n",
      "obj = -12091.173520, rho = -1.019870\n",
      "nSV = 1393, nBSV = 1377\n",
      "Total nSV = 1393\n",
      "Accuracy = 71.4514% (448/627) (classification)\n",
      "Accuracy = 59.8659% (625/1044) (classification)\n",
      "*.\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1006\n",
      "nu = 0.569386\n",
      "obj = -12397.277220, rho = -1.021757\n",
      "nSV = 1437, nBSV = 1422\n",
      "Total nSV = 1437\n",
      "Accuracy = 75.4386% (473/627) (classification)\n",
      "Accuracy = 58.7165% (613/1044) (classification)\n",
      "*.\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1030\n",
      "nu = 0.585340\n",
      "obj = -12721.620468, rho = -1.022789\n",
      "nSV = 1478, nBSV = 1461\n",
      "Total nSV = 1478\n",
      "Accuracy = 79.1069% (496/627) (classification)\n",
      "Accuracy = 59.0996% (617/1044) (classification)\n",
      "*.\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1013\n",
      "nu = 0.569968\n",
      "obj = -12396.281211, rho = -1.027122\n",
      "nSV = 1437, nBSV = 1421\n",
      "Total nSV = 1437\n",
      "Accuracy = 76.0766% (477/627) (classification)\n",
      "Accuracy = 57.567% (601/1044) (classification)\n",
      "*.\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1031\n",
      "nu = 0.550762\n",
      "obj = -12000.338933, rho = 1.020709\n",
      "nSV = 1390, nBSV = 1373\n",
      "Total nSV = 1390\n",
      "Accuracy = 71.1324% (446/627) (classification)\n",
      "Accuracy = 57.8544% (604/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1273\n",
      "nu = 0.550970\n",
      "obj = -35705.643277, rho = -1.019667\n",
      "nSV = 1391, nBSV = 1371\n",
      "Total nSV = 1391\n",
      "Accuracy = 76.555% (480/627) (classification)\n",
      "Accuracy = 59.5785% (622/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1397\n",
      "nu = 0.543188\n",
      "obj = -35204.981868, rho = -1.023691\n",
      "nSV = 1374, nBSV = 1351\n",
      "Total nSV = 1374\n",
      "Accuracy = 75.4386% (473/627) (classification)\n",
      "Accuracy = 59.4828% (621/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "**.*\n",
      "optimization finished, #iter = 1300\n",
      "nu = 0.542951\n",
      "obj = -35265.805322, rho = 1.045950\n",
      "nSV = 1375, nBSV = 1352\n",
      "Total nSV = 1375\n",
      "Accuracy = 76.3955% (479/627) (classification)\n",
      "Accuracy = 63.41% (662/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1271\n",
      "nu = 0.542036\n",
      "obj = -35108.875020, rho = 1.033804\n",
      "nSV = 1375, nBSV = 1348\n",
      "Total nSV = 1375\n",
      "Accuracy = 76.3955% (479/627) (classification)\n",
      "Accuracy = 63.0268% (658/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1338\n",
      "nu = 0.557993\n",
      "obj = -36153.105433, rho = -1.022526\n",
      "nSV = 1412, nBSV = 1388\n",
      "Total nSV = 1412\n",
      "Accuracy = 80.0638% (502/627) (classification)\n",
      "Accuracy = 61.1111% (638/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1855\n",
      "nu = 0.512845\n",
      "obj = -100078.309838, rho = 1.057009\n",
      "nSV = 1302, nBSV = 1273\n",
      "Total nSV = 1302\n",
      "Accuracy = 77.512% (486/627) (classification)\n",
      "Accuracy = 64.272% (671/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1535\n",
      "nu = 0.523491\n",
      "obj = -102624.861966, rho = 1.010072\n",
      "nSV = 1321, nBSV = 1298\n",
      "Total nSV = 1321\n",
      "Accuracy = 78.6284% (493/627) (classification)\n",
      "Accuracy = 64.751% (676/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1936\n",
      "nu = 0.517674\n",
      "obj = -100822.346047, rho = -1.039297\n",
      "nSV = 1310, nBSV = 1284\n",
      "Total nSV = 1310\n",
      "Accuracy = 77.9904% (489/627) (classification)\n",
      "Accuracy = 66.092% (690/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1757\n",
      "nu = 0.504461\n",
      "obj = -98384.956228, rho = -0.999145\n",
      "nSV = 1277, nBSV = 1254\n",
      "Total nSV = 1277\n",
      "Accuracy = 74.8006% (469/627) (classification)\n",
      "Accuracy = 66.1877% (691/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 1528\n",
      "nu = 0.514394\n",
      "obj = -100226.865462, rho = -1.006267\n",
      "nSV = 1302, nBSV = 1274\n",
      "Total nSV = 1302\n",
      "Accuracy = 75.7576% (475/627) (classification)\n",
      "Accuracy = 65.5172% (684/1044) (classification)\n",
      "..*.*\n",
      "optimization finished, #iter = 3766\n",
      "nu = 0.495064\n",
      "obj = -291450.876532, rho = -1.104690\n",
      "nSV = 1254, nBSV = 1223\n",
      "Total nSV = 1254\n",
      "Accuracy = 78.3094% (491/627) (classification)\n",
      "Accuracy = 69.636% (727/1044) (classification)\n",
      "..\n",
      "WARNING: using -h 0 may be faster\n",
      "*.*\n",
      "optimization finished, #iter = 3245\n",
      "nu = 0.495094\n",
      "obj = -290888.428216, rho = 0.995194\n",
      "nSV = 1263, nBSV = 1223\n",
      "Total nSV = 1263\n",
      "Accuracy = 79.2663% (497/627) (classification)\n",
      "Accuracy = 74.3295% (776/1044) (classification)\n",
      ".\n",
      "WARNING: using -h 0 may be faster\n",
      "*.*\n",
      "optimization finished, #iter = 2773\n",
      "nu = 0.496366\n",
      "obj = -291198.562324, rho = -1.027860\n",
      "nSV = 1261, nBSV = 1227\n",
      "Total nSV = 1261\n",
      "Accuracy = 79.2663% (497/627) (classification)\n",
      "Accuracy = 73.8506% (771/1044) (classification)\n",
      "...*..*..*\n",
      "optimization finished, #iter = 6193\n",
      "nu = 0.483722\n",
      "obj = -284238.466084, rho = -1.025637\n",
      "nSV = 1232, nBSV = 1198\n",
      "Total nSV = 1232\n",
      "Accuracy = 76.874% (482/627) (classification)\n",
      "Accuracy = 74.3295% (776/1044) (classification)\n",
      "...*\n",
      "optimization finished, #iter = 3776\n",
      "nu = 0.491495\n",
      "obj = -290003.628606, rho = 1.087581\n",
      "nSV = 1248, nBSV = 1216\n",
      "Total nSV = 1248\n",
      "Accuracy = 77.9904% (489/627) (classification)\n",
      "Accuracy = 73.4674% (767/1044) (classification)\n",
      "....*.....*.*\n",
      "optimization finished, #iter = 9479\n",
      "nu = 0.482010\n",
      "obj = -858045.493497, rho = -1.115180\n",
      "nSV = 1236, nBSV = 1185\n",
      "Total nSV = 1236\n",
      "Accuracy = 81.8182% (513/627) (classification)\n",
      "Accuracy = 73.8506% (771/1044) (classification)\n",
      "...\n",
      "WARNING: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 3973\n",
      "nu = 0.480077\n",
      "obj = -854152.772066, rho = 1.049373\n",
      "nSV = 1229, nBSV = 1184\n",
      "Total nSV = 1229\n",
      "Accuracy = 80.2233% (503/627) (classification)\n",
      "Accuracy = 74.6169% (779/1044) (classification)\n",
      "....*..*\n",
      "optimization finished, #iter = 6193\n",
      "nu = 0.479058\n",
      "obj = -850997.834233, rho = -0.966899\n",
      "nSV = 1225, nBSV = 1182\n",
      "Total nSV = 1225\n",
      "Accuracy = 79.9043% (501/627) (classification)\n",
      "Accuracy = 73.9464% (772/1044) (classification)\n",
      "....\n",
      "WARNING: using -h 0 may be faster\n",
      "*.*\n",
      "optimization finished, #iter = 5247\n",
      "nu = 0.473388\n",
      "obj = -840697.700638, rho = -1.039092\n",
      "nSV = 1207, nBSV = 1169\n",
      "Total nSV = 1207\n",
      "Accuracy = 78.4689% (492/627) (classification)\n",
      "Accuracy = 73.8506% (771/1044) (classification)\n",
      "....*..*\n",
      "optimization finished, #iter = 6875\n",
      "nu = 0.469614\n",
      "obj = -833356.105157, rho = 1.061166\n",
      "nSV = 1206, nBSV = 1156\n",
      "Total nSV = 1206\n",
      "Accuracy = 78.3094% (491/627) (classification)\n",
      "Accuracy = 74.2337% (775/1044) (classification)\n",
      "..........*.........*.................*...*\n",
      "optimization finished, #iter = 38962\n",
      "nu = 0.458121\n",
      "obj = -2456994.274712, rho = 1.083032\n",
      "nSV = 1185, nBSV = 1122\n",
      "Total nSV = 1185\n",
      "Accuracy = 78.7879% (494/627) (classification)\n",
      "Accuracy = 74.2337% (775/1044) (classification)\n",
      "...................*.........*\n",
      "optimization finished, #iter = 28939\n",
      "nu = 0.455713\n",
      "obj = -2438960.431104, rho = -1.093555\n",
      "nSV = 1171, nBSV = 1114\n",
      "Total nSV = 1171\n",
      "Accuracy = 78.9474% (495/627) (classification)\n",
      "Accuracy = 73.9464% (772/1044) (classification)\n",
      ".......*........*\n",
      "optimization finished, #iter = 15692\n",
      "nu = 0.464466\n",
      "obj = -2491617.353593, rho = 1.010215\n",
      "nSV = 1198, nBSV = 1137\n",
      "Total nSV = 1198\n",
      "Accuracy = 81.1802% (509/627) (classification)\n",
      "Accuracy = 73.9464% (772/1044) (classification)\n",
      "...........*.....*...*\n",
      "optimization finished, #iter = 19155\n",
      "nu = 0.466165\n",
      "obj = -2504026.596112, rho = 1.068379\n",
      "nSV = 1204, nBSV = 1147\n",
      "Total nSV = 1204\n",
      "Accuracy = 81.1802% (509/627) (classification)\n",
      "Accuracy = 73.4674% (767/1044) (classification)\n",
      "........*..*\n",
      "optimization finished, #iter = 10927\n",
      "nu = 0.442767\n",
      "obj = -2374597.772282, rho = -1.003087\n",
      "nSV = 1137, nBSV = 1084\n",
      "Total nSV = 1137\n",
      "Accuracy = 77.6715% (487/627) (classification)\n",
      "Accuracy = 73.9464% (772/1044) (classification)\n",
      ".........................*.................*............*\n",
      "optimization finished, #iter = 53885\n",
      "nu = 0.444633\n",
      "obj = -7172296.310801, rho = -1.283779\n",
      "nSV = 1153, nBSV = 1079\n",
      "Total nSV = 1153\n",
      "Accuracy = 78.1499% (490/627) (classification)\n",
      "Accuracy = 71.2644% (744/1044) (classification)\n",
      ".................................*.......................*......................................*.*\n",
      "optimization finished, #iter = 93710\n",
      "nu = 0.446026\n",
      "obj = -7218522.383827, rho = -1.083941\n",
      "nSV = 1158, nBSV = 1086\n",
      "Total nSV = 1158\n",
      "Accuracy = 81.3397% (510/627) (classification)\n",
      "Accuracy = 72.4138% (756/1044) (classification)\n",
      "...........................*.....*\n",
      "optimization finished, #iter = 32424\n",
      "nu = 0.445162\n",
      "obj = -7184827.923420, rho = -1.143053\n",
      "nSV = 1154, nBSV = 1089\n",
      "Total nSV = 1154\n",
      "Accuracy = 79.4258% (498/627) (classification)\n",
      "Accuracy = 71.4559% (746/1044) (classification)\n",
      ".........................*..........*..................................*....................................................*...*\n",
      "optimization finished, #iter = 123567\n",
      "nu = 0.440029\n",
      "obj = -7098904.193173, rho = 0.799063\n",
      "nSV = 1147, nBSV = 1069\n",
      "Total nSV = 1147\n",
      "Accuracy = 78.3094% (491/627) (classification)\n",
      "Accuracy = 70.1149% (732/1044) (classification)\n",
      "................................*...................*\n",
      "optimization finished, #iter = 51479\n",
      "nu = 0.432122\n",
      "obj = -6974142.879380, rho = 1.182203\n",
      "nSV = 1122, nBSV = 1050\n",
      "Total nSV = 1122\n",
      "Accuracy = 76.7145% (481/627) (classification)\n",
      "Accuracy = 73.1801% (764/1044) (classification)\n",
      ".......................................................................*..............................................................*........................................................................*...........*\n",
      "optimization finished, #iter = 215892\n",
      "nu = 0.436752\n",
      "obj = -21123131.649746, rho = -1.061551\n",
      "nSV = 1139, nBSV = 1060\n",
      "Total nSV = 1139\n",
      "Accuracy = 80.3828% (504/627) (classification)\n",
      "Accuracy = 66.1877% (691/1044) (classification)\n",
      "...............................................................*..................................................................................................................*......................................................................................................................*.....................*\n",
      "optimization finished, #iter = 315719\n",
      "nu = 0.429962\n",
      "obj = -20864358.138717, rho = -0.806737\n",
      "nSV = 1130, nBSV = 1041\n",
      "Total nSV = 1130\n",
      "Accuracy = 78.4689% (492/627) (classification)\n",
      "Accuracy = 69.5402% (726/1044) (classification)\n",
      ".........................................................................................*.................................................................*\n",
      "optimization finished, #iter = 154185\n",
      "nu = 0.431894\n",
      "obj = -20886191.068142, rho = 1.132870\n",
      "nSV = 1134, nBSV = 1049\n",
      "Total nSV = 1134\n",
      "Accuracy = 77.512% (486/627) (classification)\n",
      "Accuracy = 70.3065% (734/1044) (classification)\n",
      "..................................................................*.....................................................................................................*......*\n",
      "optimization finished, #iter = 172053\n",
      "nu = 0.439612\n",
      "obj = -21259368.387286, rho = 1.055288\n",
      "nSV = 1145, nBSV = 1060\n",
      "Total nSV = 1145\n",
      "Accuracy = 79.5853% (499/627) (classification)\n",
      "Accuracy = 69.4444% (725/1044) (classification)\n",
      "...................................................................................................*.......................................................................................................................................*....................................................................*..............................................*\n",
      "optimization finished, #iter = 347581\n",
      "nu = 0.432775\n",
      "obj = -20956414.582188, rho = 0.887045\n",
      "nSV = 1136, nBSV = 1051\n",
      "Total nSV = 1136\n",
      "Accuracy = 79.1069% (496/627) (classification)\n",
      "Accuracy = 70.2107% (733/1044) (classification)\n"
     ]
    }
   ],
   "source": [
    "# nu-svm\n",
    "full_val_acc_res = []\n",
    "full_test_acc_res = []\n",
    "for d in range(1, 6):\n",
    "    val_acc_res = []\n",
    "    test_acc_res = []\n",
    "    for c in C:\n",
    "        test_acc_list = []\n",
    "        val_acc_list = []\n",
    "        for i in range(5):\n",
    "            train_x_scaled_, val_x_scaled, train_y_scaled_, val_y_scaled = train_test_split(train_x_scaled, train_y_scaled, test_size=0.2)\n",
    "            m = svm_train(train_y_scaled_, train_x_scaled_, f'-t 1 -c {c} -d {d}')\n",
    "            sv_len = m.get_sv_indices()\n",
    "            sv_counts_list += [len(sv_len)]\n",
    "            p_label, p_acc, p_val = svm_predict(val_y_scaled, val_x_scaled, m)\n",
    "            val_acc_list += [p_acc[0]]\n",
    "            p_label, p_acc, p_val = svm_predict(test_y_scaled, test_x_scaled, m)\n",
    "            test_acc_list += [p_acc[0]]\n",
    "        val_acc_res += [sum(val_acc_list)/5]\n",
    "        test_acc_res += [sum(test_acc_list)/5]\n",
    "    full_val_acc_res += [val_acc_res]\n",
    "    full_test_acc_res += [test_acc_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "a9ad0b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_val_acc_res = np.array(full_val_acc_res)\n",
    "full_test_acc_res = np.array(full_test_acc_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "2cb8f0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[54.06698565, 50.7814992 , 49.28229665, 50.01594896, 49.8245614 ,\n",
       "        71.99362041, 69.88835726, 73.23763955, 73.84370016, 74.83253589,\n",
       "        76.10845295, 78.97926635, 78.02232855, 79.01116427, 79.96810207,\n",
       "        79.74481659, 78.59649123, 79.68102073, 78.69218501, 80.28708134],\n",
       "       [51.45135566, 51.32376396, 51.83413078, 50.07974482, 51.1323764 ,\n",
       "        69.95215311, 72.44019139, 72.63157895, 73.11004785, 74.80063796,\n",
       "        73.74800638, 77.51196172, 78.59649123, 79.17065391, 79.36204147,\n",
       "        81.33971292, 81.18022329, 79.5215311 , 80.12759171, 80.09569378],\n",
       "       [51.51515152, 50.36682616, 51.86602871, 51.48325359, 51.51515152,\n",
       "        55.02392344, 66.98564593, 71.70653907, 73.52472089, 73.84370016,\n",
       "        74.38596491, 74.54545455, 78.5645933 , 79.13875598, 79.77671451,\n",
       "        79.87240829, 82.00956938, 80.19138756, 79.68102073, 80.86124402],\n",
       "       [51.77033493, 50.27113238, 51.10047847, 50.46251994, 48.64433812,\n",
       "        50.7814992 , 65.90111643, 69.2185008 , 73.01435407, 74.00318979,\n",
       "        73.33333333, 74.03508772, 76.36363636, 75.91706539, 78.46889952,\n",
       "        79.0430622 , 79.90430622, 79.55342903, 80.66985646, 79.33014354],\n",
       "       [51.06858054, 51.06858054, 50.1754386 , 51.70653907, 51.89792663,\n",
       "        50.68580542, 63.89154705, 65.58213716, 70.59011164, 73.36523126,\n",
       "        73.62041467, 74.22647528, 74.64114833, 76.96969697, 76.93779904,\n",
       "        78.34130781, 79.74481659, 79.55342903, 78.78787879, 79.01116427]])"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_val_acc_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "d342058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_c_index = full_val_acc_res.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "efd5815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selec best c accordning to best val acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "b92ed0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_test_acc = []\n",
    "select_val_acc = []\n",
    "for i in range(5):\n",
    "    select_test_acc += [full_test_acc_res[i, best_c_index[i]]]\n",
    "    select_val_acc += [full_val_acc_res[i, best_c_index[i]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "d872aee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 15, 16, 18, 16])"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_c_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "98d2c0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_c = []\n",
    "for i in range(5):\n",
    "    select_c += [C[best_c_index[i]]]\n",
    "d_list = [1, 2, 3, 4, 5]\n",
    "a = zip(select_c, d_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "525f307c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([80.28708134, 81.33971292, 82.00956938, 80.66985646, 79.74481659])"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "eb711924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x7f180903a580>,\n",
       "  <matplotlib.axis.XTick at 0x7f180903a520>,\n",
       "  <matplotlib.axis.XTick at 0x7f180903ad00>,\n",
       "  <matplotlib.axis.XTick at 0x7f1809071790>,\n",
       "  <matplotlib.axis.XTick at 0x7f1809078040>],\n",
       " [Text(0, 0, '(19683.0, 1)'),\n",
       "  Text(1, 0, '(243.0, 2)'),\n",
       "  Text(2, 0, '(729.0, 3)'),\n",
       "  Text(3, 0, '(6561.0, 4)'),\n",
       "  Text(4, 0, '(729.0, 5)')])"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot d vs d\n",
    "import matplotlib.pyplot as plt\n",
    "select_test_acc = np.array(select_test_acc)\n",
    "plt.plot(100 - select_test_acc)\n",
    "plt.title('Test Error vs d')\n",
    "plt.xlabel('d')\n",
    "plt.ylabel('test_error')\n",
    "plt.xticks([i for i in range(5)], a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "208892e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x7f18090174c0>,\n",
       "  <matplotlib.axis.XTick at 0x7f1809017520>,\n",
       "  <matplotlib.axis.XTick at 0x7f1809017eb0>,\n",
       "  <matplotlib.axis.XTick at 0x7f1808fc28b0>,\n",
       "  <matplotlib.axis.XTick at 0x7f1808fca040>],\n",
       " [Text(0, 0, '(1, 19683.0)'),\n",
       "  Text(1, 0, '(2, 243.0)'),\n",
       "  Text(2, 0, '(3, 729.0)'),\n",
       "  Text(3, 0, '(4, 6561.0)'),\n",
       "  Text(4, 0, '(5, 729.0)')])"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot d vs d\n",
    "import matplotlib.pyplot as plt\n",
    "select_val_acc = np.array(select_val_acc)\n",
    "plt.plot(100 - select_val_acc)\n",
    "# plt.legend(['d=1', 'd=2', 'd=3', 'd=4', 'd=5'])\n",
    "plt.title('Val Error vs d')\n",
    "plt.xlabel('d')\n",
    "plt.ylabel('val_error')\n",
    "a = zip(d_list, select_c)\n",
    "plt.xticks([i for i in range(5)], a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
